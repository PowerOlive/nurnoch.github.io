<!DOCTYPE html>
<html lang="zh">
<head>

        <title>决策树</title>
        <meta charset="utf-8" />
        <link href="http://www.wengweitao.com/feeds\all.atom.xml" type="application/atom+xml" rel="alternate" title="wwt's blog Full Atom Feed" />
        <link href="http://www.wengweitao.com/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="wwt's blog Full RSS Feed" />
        <link href="http://www.wengweitao.com/feeds/ji-qi-xue-xi.rss.xml" type="application/atom+xml" rel="alternate" title="wwt's blog Categories Atom Feed" />


        <!-- Mobile viewport optimized: j.mp/bplateviewport -->
        <meta name="viewport" content="width=device-width,initial-scale=1, maximum-scale=1">
        <meta name="google-site-verification" content="pEViDEEJCicDD__DfnOAQ7xKtjtxJhVDoGDAOahOWy0" />

        <meta name="sogou_site_verification" content="ia8yXdZS0p"/>

        <link rel="stylesheet" type="text/css" href="http://www.wengweitao.com/theme/gumby.css" />
        <link rel="stylesheet" type="text/css" href="http://www.wengweitao.com/theme/style.css" />
        <link rel="stylesheet" type="text/css" href="http://www.wengweitao.com/theme/pygment.css" />


        <script src="http://www.wengweitao.com/theme/js/libs/modernizr-2.6.2.min.js"></script>




</head>

<body id="index" class="home">


    <div class="container">

        <div class="row">

          <header id="banner" class="body">
                  <h1><a href="http://www.wengweitao.com/">wwt's blog <strong></strong></a></h1>
          </header><!-- /#banner -->

            <div id="navigation" class="navbar row">
              <a href="#" gumby-trigger="#navigation &gt; ul" class="toggle"><i class="icon-menu"></i></a>
             
              <ul class="columns">
                <li><a href="http://www.wengweitao.com/">首页</a></li>

                <li><a href="/categories.html">分类</a></li>
                <li><a href="/tags.html">标签</a></li>
                <li><a href="/archives.html">归档</a></li>
                <li><a href="/pages/about-me.html">关于我</a></li>

              </ul>
            </div>

<section id="content" class="body">

   <div class="row">
        <div class="eleven columns">


            <header>
              <h2 class="entry-title">
                <a href="http://www.wengweitao.com/jue-ce-shu.html" rel="bookmark"
                   title="Permalink to 决策树">决策树</a></h2>
           

            </header>
            <footer class="post-info">
              <abbr class="published" title="2014-07-29T10:00:00">
                Tue 29 July 2014
              </abbr>
              <address class="vcard author">By 
                <a class="url fn" href="http://www.wengweitao.com/author/wwt.html"> wwt</a>
              </address>
            </footer><!-- /.post-info -->
            <div class="entry-content">
              <blockquote>
<p>决策树（decision tree）是一种基本的分类与回归方法。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型可读性好，分类速度快。决策树的学习通常包含3个部分：特征选择、决策树生成和决策树的修剪。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。常用的算法有ID3、C4.5和CART。</p>
</blockquote>
<h2>决策树模型与学习</h2>
<h3>决策树模型</h3>
<p><strong>决策树</strong></p>
<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。<strong>内部结点</strong>表示一个特征或者属性，<strong>叶节点</strong>表示一个类。</p>
<p>用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归的对实例进行测试并分配，直至到达叶节点，最后将实例分配到叶节点的类中。</p>
<h3>决策树与if-then规则</h3>
<p>互斥且完备</p>
<h3>决策树与条件概率分布</h3>
<p>决策树还可以表示为<strong>给定特征条件下类的条件概率分布</strong>。各叶节点上的条件概率往往偏向某一个类，决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p>
<h3>决策树学习</h3>
<p>学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。</p>
<p>决策树模型可以看成是由训练数据集估计条件概率模型。</p>
<p>决策树的学习的策略是以损失函数为目标函数的最小化。决策树的损失函数通常是正则化的极大似然函数。<strong>学习的问题就变成了在损失函数意义下选择最优决策树的问题。</strong>因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树的学习算法通常采用启发式方法，近似求解这一优化问题。这样得到的决策树是次最优（sub-optimal）的。</p>
<p>决策树的学习算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。</p>
<p>（1）开始构建根结点，将所有的训练数据都放入根结点；</p>
<p>（2）选择一个最优特征，按照这一特征将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类；</p>
<p>（3）如果这些子集已经基本被正确分类，那么就把这些子集分到所对应的叶节点中去；</p>
<p>（4）如果还有子集未能基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割</p>
<p>（5）如此递归下去，直到全部基本正确分类，最后每一个子集都被分配到叶节点上，即都有了明确的分类，这就生成了一棵决策树。</p>
<p>以上生成的决策树对训练数据有很好的分类能力，但可能发生过拟合的情况。我们需要对生成的决策树进行自下而上的剪枝（去掉过于细分的叶结点，使其回退到父节点或者更高的结点，使树变得更简单），使其具有更好的泛化能力。</p>
<p>如果特征数量过多，可以在决策树学习开始的时候，对特征进行选择，留下对训练数据有足够充分分类能力的特征。</p>
<p>可以看出决策树的学习算法包含特征选择、决策树生成和决策树剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。</p>
<hr />
<h2>特征选择</h2>
<h3>特征选择问题</h3>
<p>特征的选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。<strong>通常特征选择的准则是信息增益或信息增益比</strong>。特征选择是决定用哪个特征来划分特征空间。如果一个特征具有较强的分类能力，那么我们就应该选择这个特征（或者说一个特征能使划分后的子集在当前条件下有最好的分类）。信息增益（information gain）就能够很好的表示这一准则。</p>
<h3>信息增益</h3>
<p>先给出熵和条件熵的定义。</p>
<p><strong>熵（entropy）</strong>表示随机变量不确定性的度量。假设X是一个取有限个值的离散变量，X的熵定义为：
</p>
<div class="math">$$H(X)=-\sum_{i=1}^{n}p_ilogp_i$$</div>
<p>
其中<span class="math">\(p_i\)</span>为X取每一个可能值的概率，上式中的对数以2为底或者以e为底，这时熵的单位分别称作比特（bit）或纳特（nat）。可以看出X的熵只依赖于X的分布，而与X的取值无关，所以可以将X的熵记为
</p>
<div class="math">$$H(p)=-\sum_{i=1}^{n}p_ilogp_i$$</div>
<p>
熵越大，随机变量的不确定性就越大。从定义可以验证：
</p>
<div class="math">$$0 \leq H(p) \leq logn$$</div>
<p>
即熵达到最大值（所有可能的事件等概率时不确定性最高）。</p>
<p><strong>条件熵H(Y|X)</strong>表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望
</p>
<div class="math">$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$</div>
<p>
这里<span class="math">\(p_i=P(X=x_i), i=1,2,...,n\)</span></p>
<p>当熵和条件熵中的概率由数据统计得到时，所对应的熵与条件熵分别称为<strong>经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）</strong>。</p>
<p><strong>信息增益（information gain）</strong>表示得知特征X的信息而使得类Y的信息不确定性的<strong>减少</strong>长度。</p>
<p>特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差：
</p>
<div class="math">$$g(D|A)=H(D)-H(D|A)$$</div>
<p>
一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。
信息增益大的特征具有更强的分类能力。</p>
<p>根据学习增益准则的特征选择方法：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p>
<p><strong>信息增益的算法</strong>：</p>
<p>输入：训练数据集D和特征A；</p>
<p>输出：特征A对训练数据集D的信息增益g(D,A)</p>
<p>（1）计算数据集D的经验熵
</p>
<div class="math">$$H(D)=-\sum_{k=1}^{K}\frac{C_k}{|D|}log_2\frac{C_k}{|D|}$$</div>
<p>
（2）计算特征A对数据集D的经验条件熵
</p>
<div class="math">$$H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$$</div>
<p>
（3）计算信息增益
</p>
<div class="math">$$g(D|A)=H(D)-H(D|A)$$</div>
<p><span class="math">\(|D|\)</span>表示样本容量，设有K个类<span class="math">\(C_k，|C_k|\)</span>属于类<span class="math">\(C_k\)</span>的样本数量。特征A将D划分为n个子集<span class="math">\(D_1,D_2,...,D_n，|D_i|\)</span>为<span class="math">\(D_i\)</span>的样本数。子集<span class="math">\(D_i\)</span>中属于类<span class="math">\(C_k\)</span>的样本的集合为<span class="math">\(D_{ik}\)</span>。</p>
<h3>信息增益比</h3>
<p>以信息增益作为划分训练数据集的特征，存在<strong>偏向于选择取值较多的特征</strong>的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。</p>
<p>特征A对训练数据集D的信息增益比<span class="math">\(g_R(D,A)\)</span>定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵<span class="math">\(H_A(D)\)</span>之比：
</p>
<div class="math">$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$</div>
<p>
其中<span class="math">\(H_A(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_2\frac{|D_{i}|}{|D|}\)</span>，n为特征A的取值个数，<span class="math">\(D_i\)</span>表示特征A将D分成的子集。</p>
<hr />
<h2>决策树的生成</h2>
<h3>ID3算法</h3>
<p>ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体的方法是：</p>
<p>（1）从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点</p>
<p>（2）对子节点递归的调用以上方法，构建决策树</p>
<p>（3）直到所有特征的信息增益均很小或没有特征选择为止。</p>
<p>ID3算法相当于用极大似然估计法进行概率模型的选择。</p>
<h3>C4.5的生成算法</h3>
<p>C4.5算法与ID3算法类似，在生成的过程中，用信息增益比来选择特征。</p>
<hr />
<h2>决策树的剪枝</h2>
<p>决策树生成算法产生的决策树，会出现过拟合的现象。因为在学习的过程中过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法就是考虑决策树的复杂度，对已生成的决策树进行简化。对已生成的决策树进行简化的过程称为<strong>剪枝（pruning）</strong>。</p>
<p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。决策树的损失函数可以定义为：
</p>
<div class="math">$$C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$</div>
<p>
其中，树T的叶节点数为|T|，叶节点t有<span class="math">\(|N_t|\)</span>个样本点，其中属于k类的数目为<span class="math">\(|N_{tk}|\)</span>个，<span class="math">\(H_t(T)\)</span>为叶节点t的经验熵
</p>
<div class="math">$$H_t(T)=-\sum_{k=1}^{K}\frac{|N_{tk}|}{|N_t|}log_2\frac{|N_{tk}|}{|N_t|}$$</div>
<p>
决策树的损失函数可以表示为：
</p>
<div class="math">$$C_\alpha(T)=C(T)+\alpha|T|$$</div>
<p>
这时C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型的复杂度，参数α控制二者直接的影响。较大的α促使选择较简答的模型，较小反之。</p>
<p>剪枝，就是当<span class="math">\(\alpha\)</span>确定时，选择损失函数最小的模型，即损失函数最小的子树。利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p>
<p><strong>树的剪枝算法</strong>：</p>
<p>输入：生成算法产生的整棵树T，参数<span class="math">\(\alpha\)</span></p>
<p>输出：修剪后的子树<span class="math">\(T_\alpha\)</span></p>
<p>（1）计算每个结点的经验熵</p>
<p>（2）递归地从叶节点向上回缩。设叶节点回到到其父节点之前与之后的整体树分别为<span class="math">\(T_B\)</span>和<span class="math">\(T_A\)</span>，如果其对应的损失函数有：
</p>
<div class="math">$$C_\alpha(T_A) \leq C_\alpha(T_B)$$</div>
<p>
则进行剪枝，即将父节点变为新的叶结点。</p>
<p>（3）返回（2）直至不能继续，得到损失函数最小的子树。</p>
<p>上式只需考虑两棵树的损失函数的差，其计算可以在局部进行，所以决策树的剪枝算法可以由一种动态规划算法实现。</p>
<hr />
<h2>CART算法</h2>
<p><strong>分类与回归树（classification and regression tree, CART）</strong>是应用广泛的决策树学习方法。既可以用于分类也可以用于回归。CART假设决策树是<strong>二叉树</strong>，内部结点的特征取值为“是”或“否”，左分支是取值为“是”的分支，右分支是取值为“否”的的分支。这样的决策树等于递归地二分每个特征，将输入空间划分为有限个单元，并在这些单元上确定预测的概率分布。</p>
<p>CART算法由以下两步组成：</p>
<p>（1）决策树的生成</p>
<p>（2）决策树的剪枝</p>
<h3>CART生成</h3>
<p>决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树<strong>用基尼指数（Gini index）最小化准则，进行特征选择</strong>，生成二叉树。</p>
<h4>回归树的生成</h4>
<p>遍历所有输入变量，找到最优的切分变量j和切分点（可以用平方误差来表示回归树对于训练数据的预测误差，使预测误差最小），构成一个对（j,s）。依次将输入空间划分为两个区域，接着对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的<strong>回归树通常称为最小二乘回归树（least squares regression tree）</strong>。</p>
<h6>分类树的生成</h6>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<p>在分类问题中，假设有K个类，样本点属于第k类的概率为<span class="math">\(p_k\)</span>，则概率分布的基尼指数定义为
</p>
<div class="math">$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$$</div>
<p>
对于给定的样本集合D，其基尼指数为
</p>
<div class="math">$$Gini(p)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2$$</div>
<p>
假设样本集合D根据特征A是否取某一可能值a被分割成两个部分<span class="math">\(D_1\)</span>和<span class="math">\(D_2\)</span>，则在特征A的条件下，集合D的基尼系数定义为
</p>
<div class="math">$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$</div>
<p>
<strong>基尼指数Gini(D)表示集合D的不确定性</strong>，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性也就越大（与熵类似）。</p>
<p><strong>CART生成算法：</strong></p>
<p>输入：训练数据集D，停止计算的条件（如结点中样本个数小于预定阈值，或样本集的基尼指数小于预定阈值）</p>
<p>输出：CART决策树</p>
<p>从根结点开始，递归地对每个结点进行以下操作：</p>
<p>（1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，对每一个特征A，对其可能的每一个取值a，根据样本点对A=a的测试为“是”或“否”将D分割成<span class="math">\(D_1\)</span>和<span class="math">\(D_2\)</span>两部分，计算A=a时的基尼指数。</p>
<p>（2）在所有的特征A以及它们所有可能的切分点a中，<strong>选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点</strong>。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</p>
<h3>CART剪枝</h3>
<p>CART剪枝算法由两步组成：</p>
<p><strong>（1）剪枝，形成一个子树序列</strong></p>
<p>从生成算法产生的决策树<span class="math">\(T_0\)</span>低端开始不断剪枝，直到<span class="math">\(T_0\)</span>的根结点，形成一个子树序列<span class="math">\({T_0,T_1,...,T_n}\)</span>。</p>
<p>在剪枝的过程中，计算子树的损失函数：
</p>
<div class="math">$$C_\alpha(T)=C(T)+\alpha|T|$$</div>
<p>
其中，T为任意子树，C(T)为对训练数据的预测误差（如基尼指数），|T|为子树的叶结点个数。</p>
<p>从整体树<span class="math">\(T_0\)</span>开始剪枝，对<span class="math">\(T_0\)</span>的任意内部结点t，以t为单结点树的损失函数是（叶结点个数为0）
</p>
<div class="math">$$C_\alpha(t)=C(t)+\alpha$$</div>
<p>
以t为根结点的子树<span class="math">\(T_t\)</span>的损失函数是
</p>
<div class="math">$$C_\alpha(T_t)=C(T_t)+\alpha|T_t|$$</div>
<p>
当<span class="math">\(\alpha=0及\alpha\)</span>充分小时，有不等式
</p>
<div class="math">$$C_\alpha(T_t) \leq C_\alpha(t)$$</div>
<p>
当<span class="math">\(\alpha\)</span>增大时，上面<span class="math">\(C_\alpha(T_t) = C_\alpha(t)\)</span>，当<span class="math">\(\alpha\)</span>再增大时，上面的不等式就反向了。只要<span class="math">\(\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}\)</span>，<span class="math">\(T_t与t\)</span>有相同的损失函数值，而t的结点少，因此t比<span class="math">\(T_t\)</span>更可取，对<span class="math">\(T_t\)</span>进行剪枝。</p>
<p>为此对<span class="math">\(T_0\)</span>中每一内部结点t，计算
</p>
<div class="math">$$g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}$$</div>
<p>
上式的分子可以表示误差的增加，分母部分表示剪枝后树叶数量的减小。它表示剪枝后整体损失函数减少的程度。在<span class="math">\(T_0\)</span>中减去g(t)最小的<span class="math">\(T_t\)</span>，将得到子树作为<span class="math">\(T_1\)</span>，同时将最小的g(t)设为<span class="math">\(\alpha_1\)</span>。<span class="math">\(T_1\)</span>为区间<span class="math">\([\alpha_1,\alpha_2)\)</span>的最优子树。</p>
<p>如此剪枝下去，直至得到根结点。在这一过程中，不断增加<span class="math">\(\alpha\)</span>的值，产生新的区间。</p>
<p>（1）对于所有的子树t，我们想用合适的叶子节点来代替t，然后计算增加的误差E与t的叶子节点的比值（即g(t)）。我们选择比值最小的那个子树t，用合适的叶子节点代替之。</p>
<p>（2）重复迭代以上步骤，每次都替换掉一棵子树。我们会得到从完全增长的树<span class="math">\(T_0\)</span>到只有根节点一个决策结点的树<span class="math">\(T_n\)</span>的一系列决策树：<span class="math">\(T_0,T_1,...,T_n\)</span>。然后我们用独立的验证集(我们可以从可用数据集中抽取三分之一作为验证集，剩下的三分之二作为训练集)来验证各个决策树的分类准确性。选取准确性最高的决策树为最终的CART决策树。</p>
<p><strong>（2）在剪枝得到的子树序列<span class="math">\(T_0,T_1,...,T_n\)</span>中通过交叉验证选取最优子树<span class="math">\(T_\alpha\)</span></strong></p>
<p>测试子树序列<span class="math">\(T_0,T_1,...,T_n\)</span>中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中每棵子树<span class="math">\(T_0,T_1,...,T_n\)</span>都对应于一个参数<span class="math">\(\alpha_0,\alpha_1,...,\alpha_n\)</span>。所以当最优子树<span class="math">\(T_k\)</span>确定时，对应的<span class="math">\(\alpha_k\)</span>也确定了，即得到最优决策树<span class="math">\(T_\alpha\)</span>。</p>
<p>如果一个结点是叶节点那么其g(t)为无穷大，如果不是叶节点那么按照以上的方法计算g(t)。每次从当前的树中选择一个g(t)最小的（因为该结点增加的误差率是最小的），然后删去该结点或结点集合，得到一棵新的树，然后递归上面的过程。</p>
<p><strong>CART剪枝算法</strong>：</p>
<p>输入：CART算法生成的决策树<span class="math">\(T_0\)</span></p>
<p>输出：最优决策树<span class="math">\(T_\alpha\)</span></p>
<p>（1）设k=0, T=<span class="math">\(T_0\)</span></p>
<p>（2）$\alpha = +\infty $</p>
<p>（3）自下而上对各内部结点计算<span class="math">\(C(T_t)\)</span>，计算出g(t)，并且将各结点计算出的最小的g(t)保存在<span class="math">\(\alpha\)</span>中</p>
<p>（4）自上而下访问内部结点，如果g(t)=<span class="math">\(\alpha\)</span>，剪去g(t)最小的T，并对叶节点t以多数表决法决定其类，得到树T</p>
<p>（5）k=k+1,在<span class="math">\(\alpha_k=\alpha, T_k=T\)</span></p>
<p>（6）T不是由根结点单独一个结点构成的树，就<strong>返回（4）（（2）？）</strong></p>
<p>（7）采用交叉验证在剪枝得到的子树序列<span class="math">\(T_0,T_1,...,T_n\)</span>中选取最优子树</p>
<hr />
<h2>Reference</h2>
<p><a href="http://book.douban.com/subject/10590856/">统计学习方法</a>第五章</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div><!-- /.entry-content -->
            <div class="comments">
              <h3>Comments</h3>
              <div id="disqus_thread"></div>
              <script type="text/javascript">
                var disqus_identifier = "jue-ce-shu.html";
                (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = 'http://wengwt.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
              </script>
            </div>


        </div><!-- /.eleven.columns -->

<div class="three columns">

<!--
<h4>Pages</h4>

 <ul>
      <li><a href="/categories.html">分类</a></li>
      <li><a href="/tags.html">标签</a></li>
      <li><a href="/archives.html">归档</a></li>
      <li><a href="/pages/about-me.html">关于我</a></li>
  </ul>
-->

<aside id="sidebar"><!--Google站内搜索开始-->
        <div class="search">
                <!--<h2>站内搜索</h2>-->
                <form method=get action="http://www.google.com/search">
                <input type=text name=q>
                <input type=submit name=btnG value="搜索">
                <input type=hidden name=ie value=UTF-8>
                <input type=hidden name=oe value=UTF-8>
                <input type=hidden name=hl value=zh-CN>
                <input type=hidden name=domains value="http://www.wengweitao.com">
                <input type=hidden name=sitesearch value="http://www.wengweitao.com">
                </form>
        </div>
<!--Google站内搜索结束-->

<h4>分类</h4>
<ul>
		<li><a href="http://www.wengweitao.com/category/bian-cheng-yu-yan.html">编程语言</a></li>
		<li><a href="http://www.wengweitao.com/category/du-shu-bi-ji.html">读书笔记</a></li>
		<li><a href="http://www.wengweitao.com/category/gong-ju.html">工具</a></li>
		<li><a href="http://www.wengweitao.com/category/ji-qi-xue-xi.html">机器学习</a></li>
		<li><a href="http://www.wengweitao.com/category/python.html">Python</a></li>
		<li><a href="http://www.wengweitao.com/category/sheng-huo.html">生活</a></li>
		<li><a href="http://www.wengweitao.com/category/suan-fa-yu-shu-ju-jie-gou.html">算法与数据结构</a></li>
</ul>

<!--
<h4>标签</h4>
<ul class="tagcloud">
        <li class="tag-2"><a href="http://www.wengweitao.com/tag/jiao-cheng.html">教程</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/go.html">Go</a></li>
        <li class="tag-1"><a href="http://www.wengweitao.com/tag/leetcode.html">leetcode</a></li>
        <li class="tag-3"><a href="http://www.wengweitao.com/tag/ji-ben-gai-nian.html">基本概念</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/wan.html">玩</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/xiao-yuan.html">校园</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/gong-ju.html">工具</a></li>
        <li class="tag-1"><a href="http://www.wengweitao.com/tag/python.html">Python</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/suan-fa-fen-xi.html">算法分析</a></li>
        <li class="tag-1"><a href="http://www.wengweitao.com/tag/mian-shi-ti.html">面试题</a></li>
        <li class="tag-1"><a href="http://www.wengweitao.com/tag/pai-xu-suan-fa-ji-chu.html">排序 算法 基础</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/ce-shi.html">测试</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/bian-cheng-zhu-ji.html">编程珠玑</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/jiao-cheng-linux.html">教程、Linux</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/pai-xu-suan-fa-ji-chu-zong-jie.html">排序 算法 基础 总结</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/ji-chu-gai-nian.html">基础概念</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/vim.html">Vim</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/er-cha-shu.html">二叉树</a></li>
        <li class="tag-1"><a href="http://www.wengweitao.com/tag/du-shu-bi-ji.html">读书笔记</a></li>
        <li class="tag-4"><a href="http://www.wengweitao.com/tag/tmux.html">tmux</a></li>
</ul>
-->


<nav class="widget">
  <h4>社交</h4>
  <ul>
    <li><a href="http://weibo.com/u/2678027854">新浪微博</a></li>
    <li><a href="http://www.douban.com/people/wengwt/">豆瓣</a></li>
    <li><a href="http://www.zhihu.com/people/vita-49">知乎</a></li>
    <li><a href="https://github.com/nurnoch">Github</a></li>
    <li><a href="https://www.v2ex.com/member/wwttc">V2EX</a></li>
  </ul>
</nav>

<nav class="widget">
  <h4>友情链接</h4>
  <ul>
    <li><a href="http://netlab.pkusz.edu.cn/">互联网信息工程研发中心</a></li>
    <li><a href="http://www.houcj.net/">houcj's blog</a></li>
    <li><a href="http://www.rudy-yuan.net/">rundy-yuan's blog</a></li>
    <li><a href="http://zwyang.me/blog/">zwyanswer's blog</a></li>
  </ul>
</nav>


</div> </div><!-- /.row -->


</section>

       </div><!-- /.row -->
    </div><!-- /.container -->


       <div class="container.nopad bg">

    
        <footer id="credits" class="row">
          <div class="seven columns left-center">

                   <address id="about" class="vcard body">
                    Proudly powered by  <a href="http://getpelican.com/">Pelican</a> and Theme by Gum © 2015 wwt       
                       <!-- cnzz -->
                    <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1255354158'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s4.cnzz.com/z_stat.php%3Fid%3D1255354158%26show%3Dpic2' type='text/javascript'%3E%3C/script%3E"));</script>  
                    </address>
                   
          </div>


          <div class="seven columns">
            <div class="row">
              <ul class="socbtns">





              </ul>
            </div>
          </div>
        </footer>

    </div>



<script type="text/javascript">
    var disqus_shortname = 'wengwt';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
  <script src="http://www.wengweitao.com/theme/js/libs/jquery-1.9.1.min.js"></script>
  <script src="http://www.wengweitao.com/theme/js/libs/gumby.min.js"></script>
  <script src="http://www.wengweitao.com/theme/js/plugins.js"></script>
</body>
</html>