<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>wwt's blog</title><link href="http://www.wengweitao.com/" rel="alternate"></link><link href="http://www.wengweitao.com/feeds/ji-qi-xue-xi.rss.xml" rel="self"></link><id>http://www.wengweitao.com/</id><updated>2014-07-26T16:18:00+08:00</updated><entry><title>梯度下降法</title><link href="http://www.wengweitao.com/ti-du-xia-jiang-fa.html" rel="alternate"></link><updated>2014-07-26T16:18:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-26:ti-du-xia-jiang-fa.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;梯度下降法（Gradient Descent）&lt;/strong&gt;是一种常见的最优化算法，用于求解函数的最大值或者最小值。&lt;/p&gt;
&lt;h2&gt;梯度的概念&lt;/h2&gt;
&lt;p&gt;一个函数&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;对它的一个变量&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的梯度定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial J(\theta))}{\partial \theta} = \lim_{\delta \theta\rightarrow 0}\frac{J(\theta + \delta \theta)-J(\theta ))}{\delta \theta}$$&lt;/div&gt;
&lt;p&gt;
某一点上的梯度指向标量场增长最快的方向，梯度的长度就是最大的变化率。&lt;/p&gt;
&lt;h2&gt;梯度下降&lt;/h2&gt;
&lt;p&gt;在高数中，我们求解一个函数的最小值时，最常用的方法就是求出它的导数为0的那个点，进而判断这个点是否能够取最小值。但是，在实际很多情况，我们很难求解出使函数的导数为0的方程，这个时候就可以使用梯度下降。我们知道对于一个函数沿着梯度的那个方向是下降是最快的。例如为了选取一个&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;使&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;最小，我们可以先随机选择&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;一个初始值，然后不断的修改&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;以减小&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;，直到&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的值不再改变。对于梯度下降法，可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{j} = \theta_j - \alpha \frac{\partial J(\theta))}{\partial \theta_j} $$&lt;/div&gt;
&lt;p&gt;
即不断地向梯度的那个方向（减小最快的方向）更新&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，最终使得&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;最小。其中&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;称为学习速率（learning rate），取值太小会导致迭代过慢，取值太大可能错过最值点。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;举一个具体的例子，假如你在一座山的山顶准备下山，往哪一个方向走下山最快呢？下山最快的方向是最陡的那个方向，每一步你都应该朝最陡的那个方向走，直到到达山底，学习速率就表示你每一步迈的步伐有多大。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;为什么从函数的梯度方向下降可以得到函数的最小值&lt;/h2&gt;
&lt;p&gt;梯度下降法，基于这样的观察：如果实值函数F(x)在点a 处可微且有定义，那么函数 F(x)在a点沿着梯度相反的方向&lt;span class="math"&gt;\(-\bigtriangledown F(a)\)&lt;/span&gt;下降最快。&lt;/p&gt;
&lt;p&gt;因而，如果
&lt;/p&gt;
&lt;div class="math"&gt;$$b=a-\alpha\bigtriangledown F(a)$$&lt;/div&gt;
&lt;p&gt;
那么&lt;span class="math"&gt;\(F(b) \leq F(a)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;考虑如下序列&lt;span class="math"&gt;\(x_0,x_1,x_2,...\)&lt;/span&gt;使得
&lt;/p&gt;
&lt;div class="math"&gt;$$x_{n+1}=x_n-\alpha\bigtriangledown F(x_n)$$&lt;/div&gt;
&lt;p&gt;
因此可以得到：
&lt;span class="math"&gt;\(F(x_0) \geq F(x_1) \geq F(x_2) \geq ...\)&lt;/span&gt;
如果顺利的话序列最终可以收敛到期望的极值。&lt;/p&gt;
&lt;p&gt;&lt;img alt="梯度下降描述" src="http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Gradient_descent.png/350px-Gradient_descent.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：梯度下降得到的结果可能是局部最优值。如果&lt;span class="math"&gt;\(F(x)\)&lt;/span&gt;是凸函数，则可以保证梯度下降得到的是全局最优值。&lt;/p&gt;
&lt;h2&gt;批梯度下降法&lt;/h2&gt;
&lt;p&gt;批梯度下降法（batch gradient descent）的算法描述如下：&lt;/p&gt;
&lt;p&gt;对每一个j重复以下过程直到收敛 {
    &lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{j} = \theta_j - \alpha \sum_{i=1}^{m}\frac{\partial J(\theta , z))}{\partial \theta_j} $$&lt;/div&gt;
&lt;p&gt;
}&lt;/p&gt;
&lt;p&gt;其中假设训练样本数有m个，每个样本用&lt;span class="math"&gt;\(z_i\)&lt;/span&gt;可以表示为&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;。可以看出，使用批梯度下降法每一次更新&lt;span class="math"&gt;\(\theta_j\)&lt;/span&gt;都需要遍历训练样本中的所有样本。&lt;/p&gt;
&lt;h2&gt;随机梯度下降法&lt;/h2&gt;
&lt;p&gt;除了批梯度下降法之外，还有一种算法称为——&lt;strong&gt;随机梯度下降法（stochastic gradient descent，SGD）&lt;/strong&gt;。算法描述如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Loop {
for i=1 to m, {
&lt;div class="math"&gt;$$\theta_{j} = \theta_j - \alpha\frac{\partial J(\theta , z_i))}{\partial \theta_j} $$&lt;/div&gt;
}
}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;每一次更新只使用了一个训练样本，但更新了m次。&lt;/p&gt;
&lt;p&gt;批梯度下降法每更新一步都需要遍历所有的样本数据，如果样本数m很大的话，就会非常的耗时；而对于随机梯度下降，每一遇到一个样本数据，就可以马上更新。通常情况下，使用随机梯度下降法的速度会比批梯度下降法快很多。因此，当样本数很大的时候，我们通常都选择使用随机梯度下降法。&lt;/p&gt;
&lt;h2&gt;关于梯度下降法中学习速率的取值问题&lt;/h2&gt;
&lt;p&gt;前面提到&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;称为学习速率（learning rate），取值太小会导致迭代过慢，取值太大可能错过最值点。所以对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;还是非常重要的。最常见的对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;的选择策略为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不变策略：&lt;span class="math"&gt;\(\alpha_k=\alpha_0\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;1/k策略：&lt;span class="math"&gt;\(\alpha_k=\alpha_0\frac{\tau}{\tau + k}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体的取值方法，还是需要根据实际的数据进行实验得出。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;http://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&lt;/li&gt;
&lt;li&gt;http://www.iro.umontreal.ca/~bengioy/ift6266/H12/html/gradient_en.html&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="基本概念"></category></entry><entry><title>统计学习方法概论</title><link href="http://www.wengweitao.com/tong-ji-xue-xi-fang-fa-gai-lun.html" rel="alternate"></link><updated>2014-07-25T20:12:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-25:tong-ji-xue-xi-fang-fa-gai-lun.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文主要介绍统计学习方法的一些基本概念。首先叙述统计学习的定义、研究对象与方法；然后叙述什么是监督学习；接着提出统计学习方法的三要素：模型、测量和算法；然后又介绍了模型选择的方法，包括：正则化与交叉验证；也介绍了学习方法的泛化能力；接着介绍了监督学习中的两种模型：生成模型和判别模型；最后介绍了监督学习方法的应用：分类问题、标注问题与回归问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;统计学习&lt;/h2&gt;
&lt;h3&gt;统计学习的特点&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;统计学习（statistical learning）&lt;/strong&gt;是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究对象：数据&lt;/li&gt;
&lt;li&gt;目的：对数据进行预测和分析&lt;/li&gt;
&lt;li&gt;以方法为中心，统计学习方法构建模型并运用模型进行预测与分析&lt;/li&gt;
&lt;li&gt;是概率论、统计学等多个学科的交叉&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;统计学习的对象&lt;/h3&gt;
&lt;p&gt;统计学习的对象是数据（data），并且假设同类数据具有一定的统计规律性，这是统计学习的前提。只有具有一定的统计规律性才能使用概率论的方法进行描述。&lt;/p&gt;
&lt;h3&gt;统计学习的目的&lt;/h3&gt;
&lt;p&gt;考虑学习什么样的模型和如何学习模型，以使模型能够对数据进行准确的预测与分析，同时也要尽可能考虑学习的效率。&lt;/p&gt;
&lt;h3&gt;统计学习的方法&lt;/h3&gt;
&lt;p&gt;统计学习由：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习（supervised learning）&lt;/li&gt;
&lt;li&gt;非监督学习（unsupervised learning）&lt;/li&gt;
&lt;li&gt;半监督学习（semi-supervised learning）&lt;/li&gt;
&lt;li&gt;强化学习（reinforcement learning）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;等几种学习方法组成。这里主要讨论监督学习。这种情况下，统计学习方法可以概括如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;从给定的、有限的、用于学习的&lt;strong&gt;训练数据（traning data）&lt;/strong&gt;集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为&lt;strong&gt;假设空间（hypoth space）&lt;/strong&gt;；应用某个&lt;strong&gt;评价准则（evaluation criterion）&lt;/strong&gt;，从假设空间中选取一个&lt;strong&gt;最优的模型&lt;/strong&gt;，使得它对已知训练数据及未知&lt;strong&gt;测试数据（test data）&lt;/strong&gt;在给定的评价准则下有最优的预测；最优模型的选取由算法实现；利用最优模型对新数据进行预测或分析。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这样统计学习方法包括：模型的假设空间、模型选择的准则以及模型学习的算法。&lt;strong&gt;这也就是统计学习方法的三要素：模型、策略、算法。&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;监督学习&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;监督学习（supervised learning）&lt;/strong&gt;的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。
分类和回归都属于监督学习：必须知道预测什么，即目标变量的分类信息。&lt;/p&gt;
&lt;p&gt;而与此相对的是&lt;strong&gt;无监督学习（unsupervised learning）&lt;/strong&gt;，数据没有类别信息，也不会给定目标值。&lt;/p&gt;
&lt;h3&gt;基本概念&lt;/h3&gt;
&lt;h4&gt;输入空间、特征空间与输出空间&lt;/h4&gt;
&lt;p&gt;将输入与输出所有可能取值的集合分别称为&lt;strong&gt;输入空间（input space）&lt;/strong&gt;与&lt;strong&gt;输出空间（output space）&lt;/strong&gt;。
每个具体的输入是一个&lt;strong&gt;实例（instance）&lt;/strong&gt;，通常由&lt;strong&gt;特征向量（feature vector）&lt;/strong&gt;表示。这时，所有特征向量存在的空间称为&lt;strong&gt;特征空间（feature space）&lt;/strong&gt;。特征空间的每一维对应一个特征。&lt;/p&gt;
&lt;p&gt;输入变量X和输出变量Y有不同的类型，根据不同的类型，对预测任务给予不同的名称：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;回归问题&lt;/strong&gt;：输入变量与输出变量均为连续变量的预测（房间面积预测房价）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分类问题&lt;/strong&gt;：输出变量为有限个离散变量的预测问题（新闻分类）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标注问题&lt;/strong&gt;：输入变量与输出变量均为变量序列的预测问题（词性标注）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;联合概率分布&lt;/h4&gt;
&lt;p&gt;假定我们知道Y的一些情况，包括它和X一起出现的概率，在数学上称作&lt;em&gt;联合概率分布(Joint Probabilily)&lt;/em&gt;。输入变量X和输出变量Y遵循联合概率分布P(X,Y)。P(X,Y)表示&lt;strong&gt;分布函数&lt;/strong&gt;，或&lt;strong&gt;分布密度函数&lt;/strong&gt;。X和Y具有联合概率分布的假设是监督学习关于数据的基本假设。&lt;/p&gt;
&lt;h4&gt;假设空间&lt;/h4&gt;
&lt;p&gt;由输入到输出的映射是由模型来表示的，学习的目的就是为了找到最好的一个模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是&lt;strong&gt;假设空间（hypoth space）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;监督学习的模型可以是概率模型也可以是&lt;strong&gt;非概率模型&lt;/strong&gt;，由条件概率分布 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 或决策函数（decision function） &lt;span class="math"&gt;\(Y=f(x)\)&lt;/span&gt; 表示。&lt;/p&gt;
&lt;h3&gt;问题的形式化&lt;/h3&gt;
&lt;p&gt;监督学习分为学习和预测两个过程。学习的过程需要训练数据集，而训练数据集往往是人工给出的，所以称为监督学习。&lt;/p&gt;
&lt;p&gt;如果一个模型有很好的预测能力，训练样本输出&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;和模型输出&lt;span class="math"&gt;\(f(x_i)\)&lt;/span&gt;之间的差就应该足够小。&lt;/p&gt;
&lt;h2&gt;统计学习3要素&lt;/h2&gt;
&lt;p&gt;方法 = 模型 + 策略 + 算法&lt;/p&gt;
&lt;h3&gt;模型&lt;/h3&gt;
&lt;p&gt;模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。
假设空间通常是由一个&lt;strong&gt;参数向量&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;决定&lt;/strong&gt;的族。&lt;/p&gt;
&lt;h3&gt;策略&lt;/h3&gt;
&lt;p&gt;有了模型的假设空间，接着需要考虑按照什么样的准则学习或者选择最优的模型。
需要引入损失函数与风险函数的概念。&lt;/p&gt;
&lt;h4&gt;损失函数和风险函数&lt;/h4&gt;
&lt;p&gt;用&lt;strong&gt;损失函数（loss function）或代价函数（cost function）&lt;/strong&gt;来度量预测错误的程度，记作&lt;span class="math"&gt;\(L(Y,f(X))\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;常用的损失函数有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;0-1损失函数
&lt;div class="math"&gt;$$L(Y, f(X))=\left\{\begin{matrix}
0, Y \neq f(X)\\ 
1, Y =  f(X)
\end{matrix}\right.$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;平方损失函数（quadratic loss function）
&lt;div class="math"&gt;$$L(Y, f(X))=(Y - f(X))^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;绝对损失函数
&lt;div class="math"&gt;$$L(Y, f(X))=|Y - f(X)|$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对数损失函数或者对数似然损失函数
&lt;div class="math"&gt;$$L(Y, P(Y|X))=-log(P(Y|X))$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;输出(X,Y)是随机变量，遵循联合概率分布，所以&lt;strong&gt;损失函数的期望&lt;/strong&gt;是：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{exp}(f)=E_p[L(Y, f(X))]=\int L(y,f(x))P(x,y)dxdy$$&lt;/div&gt;
&lt;p&gt;这是模型&lt;span class="math"&gt;\(f(X)\)&lt;/span&gt;在联合概率分布P(X, Y)的平均意义下的损失，称为&lt;strong&gt;风险函数（risk function）或期望损失（expected loss）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;学习的目标是选择期望风险最小的模型，由于联合分布P(X,Y)是未知的（已知那么可以直接同P(X)求出P(Y|X)了），所以风险函数无法直接计算。&lt;/p&gt;
&lt;p&gt;模型f(X)关于一个给定训练数据集的平均损失称为&lt;strong&gt;经验风险（empirical risk）或者经验损失（empirical loss）&lt;/strong&gt;，记作：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{emp}(f)=\frac{1}{N}\sum L(y,f(x))$$&lt;/div&gt;
&lt;p&gt;
根据大数定律，当N趋于无穷时，经验风险趋于期望风险。所以很自然的想到用经验风险估计期望风险。但是实际训练样本数目有限，用经验风险评估期望风险常常并不理想，要对经验风险进行一定的矫正，这就关系到监督学习中的两个基本策略：经验风险最小化和结构化风险最小化。&lt;/p&gt;
&lt;h4&gt;经验风险最小化和结构化风险最小化&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;经验风险最小化（empirical risk minimization， ERM）&lt;/strong&gt;的策略认为，经验风险最小的模型是最优的模型。
当样本容量足够大时，经营风险最小化能保证有很好的学习效果，比如极大似然估计就是一个例子。&lt;/p&gt;
&lt;p&gt;当样本容量很小时，经营风险最小化可能产生&lt;strong&gt;过拟合（over-fitting）&lt;/strong&gt;的现象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结构风险最小化（structural risk minimization， SRM）&lt;/strong&gt;是为了防止过拟合而提出的策略，&lt;strong&gt;结构风险最小化等价于正则化（regularization）&lt;/strong&gt;。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。结构风险的定义是：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{srm}(f)=\frac{1}{N}\sum L(y_i,f(x_i))+\lambda J(f)$$&lt;/div&gt;
&lt;p&gt;
J(f)为模型的复杂度，模型f越复杂，复杂度J(f)就越大；反之越小。也就是说复杂度表示了对复杂模型的惩罚。$\lambda \geq 0 $用以权衡经验风险和模型复杂度。&lt;/p&gt;
&lt;p&gt;贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation, MAP）就是结构化风险最小化的例子。&lt;/p&gt;
&lt;p&gt;这样，&lt;strong&gt;监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;算法&lt;/h3&gt;
&lt;p&gt;最后需要考虑用什么样的计算方法求解最优模型。&lt;/p&gt;
&lt;p&gt;如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;模型的假设空间（模型）、模型选择的准则（策略）以及模型学习的算法（算法），这3个确定了，统计学习的方法也就确定了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2&gt;模型评估与模型选择&lt;/h2&gt;
&lt;h3&gt;训练误差与测试误差&lt;/h3&gt;
&lt;p&gt;前者是基于训练数据集的平均损失，后者是根据测试数据集的平均损失。测试误差反映了学习方法对未知的测试数据集的预测能力，越小就代表预测能力越强。通常将学习方法对未知数据的预测能力称为&lt;strong&gt;泛化能力（generalization ability）&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;过拟合与模型选择&lt;/h3&gt;
&lt;p&gt;对于有多个模型，如果一味追求提高对训练数据的预测能力，所选模型的复杂度往往会比真模型更高，这种现象就称为&lt;strong&gt;过拟合（over-fitting）&lt;/strong&gt;。过拟合是指学习时选择的模型包含的参数过多，以致于出现这一模型对已知数据预测很好，但对未知数据预测得很差的现象。&lt;/p&gt;
&lt;p&gt;模型选择时，不仅要考虑对已知数据的预测能力，还要考虑对未知数据的预测能力。当训练数据拟合效果较好，模型也比较简单，是一个较好的选择。&lt;/p&gt;
&lt;p&gt;在多项式函数拟合中可以看到，随着多项式次数（模型复杂度）的增加，训练误差会减小，直至趋向于0，但测试误差却并不如此，会先减小，达到最小值，然后增大。
下面介绍两种常用的&lt;strong&gt;模型选择方法&lt;/strong&gt;：正则化与交叉验证。&lt;/p&gt;
&lt;h2&gt;正则化与交叉验证&lt;/h2&gt;
&lt;h3&gt;正则化&lt;/h3&gt;
&lt;p&gt;模型选择的典型方法是正则化（regularization），正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化的作用就是选择经验风险与模型复杂度同时较小的模型。&lt;/p&gt;
&lt;p&gt;正则化符合奥卡姆剃刀（Occam's razor）原理：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;若无必要，勿增实体。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在所有可能的选择汇总，能够很好的解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度看，正则化对应于模型的先验概率。复杂的模型有较小的先验概率，简单的模型有较大的先验概率。&lt;/p&gt;
&lt;h3&gt;交叉验证&lt;/h3&gt;
&lt;p&gt;如果样本充足，可以将数据集分为3个部分：训练集、验证集和测试集。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。
但是，在实际中，数据样本是不充足的，为了选择好的模型，可以采用交叉验证（cross validation）的方法。交叉验证的基本思想是重复地使用数据，将给定的数据集进行切分，将切分的数据集组合为训练集与测试集，反复进行训练、测试以及模型选择。&lt;/p&gt;
&lt;p&gt;1.简单交叉验证&lt;/p&gt;
&lt;p&gt;随机地将数据集分为两部分，一部分为训练集，一部分为测试集（如70%和30%的比例切分）。选出测试误差最小的模型。&lt;/p&gt;
&lt;p&gt;2.S折交叉验证&lt;/p&gt;
&lt;p&gt;应用最多的是S折交叉验证（S-fold cross validation）。随机地将数据集切分成S个互不相交的大小相同的子集；然后利用S-1个自己训练模型，利用余下的一个子集测试模型；重复这一过程S次，每次选择不同的一份子集作为测试集；最后选出S次评测中平均测试误差最小的模型。&lt;/p&gt;
&lt;p&gt;3.留一交叉验证&lt;/p&gt;
&lt;p&gt;S折交叉验证当S=N的特例，往往在数据缺乏的情况下使用。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;泛化能力&lt;/h2&gt;
&lt;h3&gt;泛化误差&lt;/h3&gt;
&lt;p&gt;泛化能力是指该方法学习到的模型对未知数据的预测能力，通常是通过测试误差来评价学习方法的泛化能力。
如果学到的模型是f，那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{exp}(f)=E_p[L(Y, f(X))]=\int L(y,f(x))P(x,y)dxdy$$&lt;/div&gt;
&lt;p&gt;
事实上，泛化误差就是所学习到的模型的期望风险。&lt;/p&gt;
&lt;h3&gt;泛化误差的上界&lt;/h3&gt;
&lt;p&gt;学习方法的泛化能力往往都是通过研究泛化误差的上界进行的。泛化误差上界通常具有如下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它是样本容量的函数，当样本容易增加，泛化上界趋向于0；&lt;/li&gt;
&lt;li&gt;它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于二类分类问题，当假设空间是有限个，对任意一个函数f，至少以概率&lt;span class="math"&gt;\(1-\sigma\)&lt;/span&gt;以下不等式成立：
&lt;/p&gt;
&lt;div class="math"&gt;$$R(f) \leq R’(f) + \epsilon (d, N, \sigma)$$&lt;/div&gt;
&lt;p&gt;
左端是泛化误差，右端即为泛化误差的上界。在泛化误差的上界中，第1项是训练误差，训练误差越小，泛化误差越小。第2项是N的单调递减函数，当N趋于无穷时趋于0；同时也是&lt;span class="math"&gt;\(\sqrt{logd}\)&lt;/span&gt;阶的函数（d是函数个数），假设空间中函数个数越多，其值越大。
可以证明以上不等式，需要用到Hoeffding不等式。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;生成模型与判别模型&lt;/h2&gt;
&lt;p&gt;监督学习方法可以分为&lt;strong&gt;生成方法（generative approach）和判别方法（discriminative approach）&lt;/strong&gt;。所学到的模型分别称为生成模型和判别模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生成方法&lt;/strong&gt;由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$&lt;/div&gt;
&lt;p&gt;
这样的方法称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。
典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;判别方法&lt;/strong&gt;由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。
典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生成方法的特点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以还原出P(X,Y)，而判别模型不能&lt;/li&gt;
&lt;li&gt;学习收敛速度更快，即样本容量增加，学到的模型能够更快收敛于真实模型&lt;/li&gt;
&lt;li&gt;当存在隐变量时，仍然可以用生成学习方法学习，此时判别方法就不能用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;判别方法的特点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接学习的是P(Y|X)或决策函数f(X)，直接面对预测，学习的效率更高&lt;/li&gt;
&lt;li&gt;由于是直接学习P(Y|X)或决策函数f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2&gt;分类问题&lt;/h2&gt;
&lt;p&gt;当&lt;strong&gt;输出变量Y取有限个离散值&lt;/strong&gt;时，预测问题便成为分类问题。&lt;/p&gt;
&lt;p&gt;评价分类器性能的指标一般是分类&lt;strong&gt;准确率（accuracy）&lt;/strong&gt;，其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。&lt;/p&gt;
&lt;p&gt;对于二类分类问题常用的评价指标是&lt;strong&gt;精确率（precision）&lt;/strong&gt;与&lt;strong&gt;召回率（recall）&lt;/strong&gt;。通常以关注的类为正类，其他类为负类。预测的4种情况出现的总数分别记作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP：将正类预测为正类数&lt;/li&gt;
&lt;li&gt;FN：将正类预测为负类数&lt;/li&gt;
&lt;li&gt;FP：将负类预测为正类数&lt;/li&gt;
&lt;li&gt;TN：将负类预测为负类数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;精确率定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$P=\frac{TP}{TP+FP}$$&lt;/div&gt;
&lt;p&gt;
即预测结果为正类中的确是正类的概率。&lt;/p&gt;
&lt;p&gt;召回率定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$R=\frac{TP}{TP+FN}$$&lt;/div&gt;
&lt;p&gt;
即预测结果中与正类的概率占数据集中总的正类的概率。&lt;/p&gt;
&lt;p&gt;此外，还有&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;值，是精确率和召回率的调和均值，即：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$&lt;/div&gt;
&lt;p&gt;
精确率和召回率都高时，&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;值也会高。&lt;/p&gt;
&lt;p&gt;许多统计学习方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络等。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;标注问题&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;标注（taggging）&lt;/strong&gt;也是一个监督学习问题，可以认为是分类问题的一个推广，标注问题是对更复杂的结构预测问题的简单形式。标注问题的输入是一个观察序列，输出是一个标记序列或状态序列。注意，可能的标记个数是有限的，但其组合缩成的标记序列的个数是依序列长度呈指数级增长。&lt;/p&gt;
&lt;p&gt;标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。&lt;/p&gt;
&lt;p&gt;标注问题在信息抽取、自然语言处理等领域被广泛应用，如自然语言处理中的词性标注。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;回归问题&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;回归（regression）&lt;/strong&gt;是监督学习的另一个重要问题。回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系。回归模型是表示从输入变量到输出变量之间的映射函数。回归问题的学习等价于函数拟合。&lt;/p&gt;
&lt;p&gt;回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间的关系的类型，分为线性回归和非线性回归。&lt;/p&gt;
&lt;p&gt;回归学习最常用的损失函数时平方损失函数，在此情况下，回归问题可以用著名的最小二乘法（least square）求解。&lt;/p&gt;
&lt;p&gt;回归问题可以用于预测股票，如将影响股票价格的各种因素作为自变量（输入特征），而将股价作为因变量（输出的值），将过去的数据作为训练数据，就可以学习一个回归模型，对未来的股价进行预测。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第一章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry></feed>