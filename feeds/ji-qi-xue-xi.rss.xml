<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>wwt's blog</title><link href="http://www.wengweitao.com/" rel="alternate"></link><link href="http://www.wengweitao.com/feeds/ji-qi-xue-xi.rss.xml" rel="self"></link><id>http://www.wengweitao.com/</id><updated>2014-08-10T19:09:00+08:00</updated><entry><title>EM算法</title><link href="http://www.wengweitao.com/emsuan-fa.html" rel="alternate"></link><updated>2014-08-10T19:09:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-10:emsuan-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;EM算法是一种迭代算法，用于含有隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代分为两步：E步，求期望（Expectation）；M步，求极大（Maximization）。所以这一算法称为期望极大算法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;EM算法的引入&lt;/h2&gt;
&lt;p&gt;如果概率模型中仅含有观测变量（observable variable），那么给定数据，可以直接使用极大似然估计法或贝叶斯估计法估计模型参数。但是，如果概率模型中不仅含有观测变量还含有隐变量，那么就不能简单的使用那些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。&lt;/p&gt;
&lt;h3&gt;EM算法&lt;/h3&gt;
&lt;p&gt;输入：观测变量数据Y，隐变量数据Z，联合分布&lt;span class="math"&gt;\(P(Y,Z|\theta)\)&lt;/span&gt;，条件分布&lt;span class="math"&gt;\(P(Z|Y,\theta)\)&lt;/span&gt;;&lt;/p&gt;
&lt;p&gt;输出：模型参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）选择参数的初值&lt;span class="math"&gt;\(\theta^{(0)}\)&lt;/span&gt;，开始迭代；&lt;/p&gt;
&lt;p&gt;（2）E步：记&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;为第i次迭代参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的估计值，在第i+1次迭代的E步，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta, \theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]=\sum_ZlogP(Y,Z|\theta)P(Z|Y,\theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;（3）M步：求使&lt;span class="math"&gt;\(Q(\theta, \theta^{(i)})\)&lt;/span&gt;极大化的&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，确定第i+1次迭代的参数估计值&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta^{(i+1)}=argmax_\theta Q(\theta, \theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;（4）重复第（2）步与第（3）步，直到收敛&lt;/p&gt;
&lt;p&gt;其中&lt;span class="math"&gt;\(Q(\theta, \theta^{(i)})\)&lt;/span&gt;是算法的核心，称为&lt;strong&gt;Q函数&lt;/strong&gt;。其中第一个&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;表示要极大化的参数，第二个变元表示参数的当前估计值，每次迭代实际在求Q函数及其极大（M步中求极大，并完成一次迭代更新，可以证明每次迭代使似然函数增大或者达到局部极值）。&lt;/p&gt;
&lt;p&gt;完全数据的对数似然函数&lt;span class="math"&gt;\(logP(Y,Z|\theta)\)&lt;/span&gt;关于在给定观测数据Y和当前参数&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;下对未观测数据Z的条件概率分布&lt;span class="math"&gt;\(P(Y,Z|\theta^{(i)})\)&lt;/span&gt;的期望称为Q函数
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta, \theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]$$&lt;/div&gt;
&lt;p&gt;步骤（1）中，初值可以任意选择，但是EM算法对初值是敏感的；&lt;/p&gt;
&lt;p&gt;步骤（4）中，迭代的终止条件，一般是对较小的正数&lt;span class="math"&gt;\(\varepsilon_1, \varepsilon_2\)&lt;/span&gt;，若满足
&lt;/p&gt;
&lt;div class="math"&gt;$$||\theta^{(i+1)}-\theta^{(i)}||&amp;lt;\varepsilon_1$$&lt;/div&gt;
&lt;p&gt;
或
&lt;/p&gt;
&lt;div class="math"&gt;$$||Q(\theta^{(i+1)}, \theta^{(i)})-Q(\theta^{(i)}, \theta^{(i)})||&amp;lt;\varepsilon_2$$&lt;/div&gt;
&lt;p&gt;
则停止迭代。&lt;/p&gt;
&lt;p&gt;举一个三硬币模型。假设有3枚硬币，分别记为A,B,C。每次都先抛A硬币，若为正面，则抛B硬币，反面则抛C硬币，此时的结果不能观测到，记此结果为Z；然后抛B或C硬币，若出现正面记为1，反面记为0，此时的结果作为观测变量记为Y。求此模型的参数，即三枚硬币正面分别出现的概率a,b和c。&lt;/p&gt;
&lt;p&gt;Y表示观测随机变量的数据，Z表示隐随机变量的数据。Y和Z连在一起称为完全数据（complete data），观测数据Y又称为不完全数据。假设给定观测数据Y，其概率分布是&lt;span class="math"&gt;\(P(Y|\theta)\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;就是要求的模型参数。不完全数据Y的似然函数是&lt;span class="math"&gt;\(P(Y|\theta)\)&lt;/span&gt;，对数似然函数是&lt;span class="math"&gt;\(L(\theta)=logP(Y|\theta)\)&lt;/span&gt;；Y和Z的联合概率分布是&lt;span class="math"&gt;\(P(Y,Z|\theta)\)&lt;/span&gt;，那么完全数据的对数似然函数是&lt;span class="math"&gt;\(logP(Y，Z|\theta)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;EM算法通过迭代求&lt;span class="math"&gt;\(L(\theta)=logP(Y|\theta)\)&lt;/span&gt;的极大似然估计（**观测数据数据每次迭代包含两步：E步，求期望；M步，求极大化。&lt;/p&gt;
&lt;h3&gt;EM算法的导出&lt;/h3&gt;
&lt;p&gt;EM算法为什么能近似实现对观测数据的极大似然估计呢？下面通过近似求解观测数据的对数似然函数的极大化问题来导出EM算法。&lt;/p&gt;
&lt;p&gt;面对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据）Y关于参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的对数似然函数，即极大化
&lt;/p&gt;
&lt;div class="math"&gt;$$L(\theta)=logP(Y|\theta)=log\sum_ZP(Y,Z)|\theta)=log(\sum_ZP(Y|Z,\theta)P(Z|\theta))$$&lt;/div&gt;
&lt;p&gt;
上面的式子含有未观测 并且含有和的对数，所以很难直接极大化。&lt;/p&gt;
&lt;p&gt;假设第i次迭代后&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的估计值为&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;，我们希望新的估计值&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;能使&lt;span class="math"&gt;\(L(\theta)\)&lt;/span&gt;的值更大，并逐步达到极大值。考虑两者的差：
&lt;/p&gt;
&lt;div class="math"&gt;$$L(\theta)-L(\theta^{(i)})=log(\sum_ZP(Y|Z,\theta)P(Z|\theta))-logP(Y|\theta^{(i)})$$&lt;/div&gt;
&lt;p&gt; 
利用Jensen不等式&lt;sup id="fnref:Jesen"&gt;&lt;a class="footnote-ref" href="#fn:Jesen" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;,可以得到其下界：
&lt;/p&gt;
&lt;div class="math"&gt;$$L(\theta)-L(\theta^{(i)}) \geq \sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})P(Y|\theta^{(i)})}$$&lt;/div&gt;
&lt;p&gt;
令
&lt;/p&gt;
&lt;div class="math"&gt;$$B(\theta,\theta^{(1)})=L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Y|Z,\theta^{(i)})P(Y|\theta^{(i)})}$$&lt;/div&gt;
&lt;p&gt;
则&lt;span class="math"&gt;\(B(\theta,\theta^{(1)})\)&lt;/span&gt;是&lt;span class="math"&gt;\(L(\theta)\)&lt;/span&gt;的一个下界，而且&lt;span class="math"&gt;\(L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})\)&lt;/span&gt;。所以，任意可以使&lt;span class="math"&gt;\(B(\theta,\theta^{(1)})\)&lt;/span&gt;增大的&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，也可以使&lt;span class="math"&gt;\(L(\theta)\)&lt;/span&gt;增大。选择&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;使得
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta^{(i+1)}=argmax_\theta B(\theta, \theta^{(i+1)})$$&lt;/div&gt;
&lt;p&gt;
然后可以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta^{(i+1)}=argmax_\theta Q(\theta, \theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;
等价于EM算法的一次迭代，即求Q函数及其极大化。EM算法是通过不断求解其下界的极大化逼近求解对数似然函数极大化的算法。&lt;/p&gt;
&lt;p&gt;但是，EM算法不能保证找到全局最优值。&lt;/p&gt;
&lt;h3&gt;EM算法在非监督学习中的应用&lt;/h3&gt;
&lt;p&gt;有时训练数据只有输入而没有输出，从这样的数据学习模型称为非监督学习问题。EM算法可以用于非监督学习，生成模型由联合概率分布P(X,Y)（相当于P(Y)）表示，可以认为非监督学习训练数据是联合概率分布产生的数据。X为观测数据，Y为非观测数据。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;EM算法的收敛性&lt;/h2&gt;
&lt;p&gt;EM算法提供一种近似计算含有隐含变量概率模型的极大似然估计的方法。EM算法的最大优点是简单性和普适性。EM算法得到的估计序列是否收敛？&lt;/p&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(P(Y|\theta)\)&lt;/span&gt;为观测数据的似然函数，&lt;span class="math"&gt;\(\theta^{(i)}(i=1,2,...)\)&lt;/span&gt;为EM算法得到的参数估计序列，&lt;span class="math"&gt;\(P(Y|\theta^{(i)})(i=1,2,...)\)&lt;/span&gt;为对应的似然函数序列，则&lt;span class="math"&gt;\(P(Y|\theta^{i})\)&lt;/span&gt;是单调递增的，即
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y|\theta^{(i+1)}) \geq P(Y|\theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(L(\theta)=logP(Y|\theta)\)&lt;/span&gt;为观测数据的对数似然函数，&lt;span class="math"&gt;\(\theta^{(i)}(i=1,2,...)\)&lt;/span&gt;为EM算法得到的参数估计序列，&lt;span class="math"&gt;\(L(\theta^{(i)})(i=1,2,...)\)&lt;/span&gt;为为对应的对数似然函数序列。&lt;/p&gt;
&lt;p&gt;（1）如果&lt;span class="math"&gt;\(P(Y|\theta)\)&lt;/span&gt;有上界，则&lt;span class="math"&gt;\(L(\theta^{(i)})=logP(Y|\theta^{(i)})\)&lt;/span&gt;收敛到某一直&lt;span class="math"&gt;\(L^*\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（2）&lt;span class="math"&gt;\(Q(\theta, \theta')与L(\theta)\)&lt;/span&gt;满足一定条件下，由EM算法得到的参数估计序列&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;的收敛值&lt;span class="math"&gt;\(\theta^{*}\)&lt;/span&gt;是&lt;span class="math"&gt;\(L(\theta)\)&lt;/span&gt;的稳定点。&lt;/p&gt;
&lt;p&gt;稳定点，不能保证收敛到极大值点。所以初值的选择变得非常重要，常用的办法是选取几个不同的初值进行迭代，然后对得到的各个初值加以比较，从中选择最好的。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;EM算法在高斯混合模型学习中的应用&lt;/h2&gt;
&lt;p&gt;EM算法的一个重要应用是高斯混合模型的参数估计。&lt;/p&gt;
&lt;h3&gt;高斯混合模型&lt;/h3&gt;
&lt;p&gt;高斯混合模型是指具有如下形式的概率分布模型：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(y|\theta)=\sum_{k=1}^{K}\alpha_k\phi(y|\theta_k)$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(\alpha_k \geq 0\)&lt;/span&gt;是系数（&lt;span class="math"&gt;\(\sum_{k=1}^{K}\alpha_k = 1\)&lt;/span&gt;）；&lt;span class="math"&gt;\(\phi(y|\theta_k)\)&lt;/span&gt;是高斯分布密度，&lt;span class="math"&gt;\(\phi(y|\theta_k)=(\mu_k, \sigma_k^2)\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})$$&lt;/div&gt;
&lt;p&gt;
称为第k个模型。&lt;/p&gt;
&lt;h3&gt;高斯混合模型参数估计的EM算法&lt;/h3&gt;
&lt;p&gt;我们用EM算法估计高斯混合模型的参数&lt;span class="math"&gt;\(\theta=(\alpha_1,\alpha_2,...,\alpha_k;\theta_1,\theta_2,...,\theta_k)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）明确隐变量，写出完全数据的对数似然函数&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;第k个模型&lt;span class="math"&gt;\(\phi(y|\theta_k)\)&lt;/span&gt;生成的观测数据&lt;span class="math"&gt;\(y_j\)&lt;/span&gt;是已知的；但是观测数据来自于哪个模型是未知的，定义隐变量&lt;span class="math"&gt;\(\gamma_{jk}\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$\gamma_{jk}=\left\{\begin{matrix}
1, &amp;amp; 第j个观测来自第k个分模型\\ 
0, &amp;amp; 否则
\end{matrix}\right.$$&lt;/div&gt;
&lt;p&gt;
这样完全数据就是：
&lt;/p&gt;
&lt;div class="math"&gt;$$(y_j,\gamma_{j1},\gamma_{j2},...,\gamma_{jK}), j=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
于是，可以写出完全数据的似然函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(y,\gamma|\theta)=\prod_{j=1}^{N}P(y_j,\gamma_{j1},\gamma_{j2},...,\gamma_{jK}|\theta)=...=\prod_{k=1}^{K}\alpha_k^{n_k}\prod_{j=1}^{N}[\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y_j-\mu_k)^2}{2\sigma_k^2})]^{\gamma_{jk}}$$&lt;/div&gt;
&lt;p&gt;
其中，&lt;span class="math"&gt;\(n_k=\sum_{j=1}^{N}\gamma_{jk}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;那么，完全数据的对数似然函数就为：
&lt;/p&gt;
&lt;div class="math"&gt;$$logP(y,\gamma|\theta)=\sum_{k=1}^{K}n_klog\alpha_k+\sum_{j=1}^{N}\gamma_{jk}[log\frac{1}{\sqrt{2\pi}\sigma_k}-\frac{(y_j-\mu_k)^2}{2\sigma_k^2})]$$&lt;/div&gt;
&lt;p&gt;（2）EM算法的E步：确定Q函数
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta, \theta^{(i)})=E[logP(y,\gamma|\theta)|y,\theta^{(i)}]$$&lt;/div&gt;
&lt;p&gt;（3）确定EM算法的M步&lt;/p&gt;
&lt;p&gt;求使&lt;span class="math"&gt;\(Q(\theta, \theta^{(i)})\)&lt;/span&gt;极大化的&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，确定第i+1次迭代的参数估计值&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（4）重复直到收敛&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;EM算法的推广&lt;/h2&gt;
&lt;p&gt;EM算法还可以解释&lt;strong&gt;为F函数的极大-极大算法&lt;/strong&gt;，基于这个解释有若干变形与推广，如&lt;strong&gt;广义期望极大（GEM）算法&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;F函数的极大-极大算法&lt;/h3&gt;
&lt;p&gt;假设隐变量数据Z的概率分布为&lt;span class="math"&gt;\(\widetilde{P}(Z)\)&lt;/span&gt;，定义分布&lt;span class="math"&gt;\(\widetilde{P}\)&lt;/span&gt;与参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的函数&lt;span class="math"&gt;\(F(\widetilde{P}, \theta)\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$F(\widetilde{P}, \theta)=E_{\widetilde{P}}[logP(Y,Z|\theta)]+H(\widetilde{P})$$&lt;/div&gt;
&lt;p&gt;
称为&lt;strong&gt;FF函数&lt;/strong&gt;。式中&lt;span class="math"&gt;\(H(\widetilde{P})\)&lt;/span&gt;是分布&lt;span class="math"&gt;\(\widetilde{P}(Z)\)&lt;/span&gt;的熵。&lt;span class="math"&gt;\(F(\widetilde{P}, \theta)\)&lt;/span&gt;具有如下的性质：&lt;/p&gt;
&lt;p&gt;（1）对于固定的&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，存在唯一的分布布&lt;span class="math"&gt;\(\widetilde{P}(\theta)\)&lt;/span&gt;极大化&lt;span class="math"&gt;\(F(\widetilde{P}, \theta)\)&lt;/span&gt;，这时&lt;span class="math"&gt;\(\widetilde{P}(\theta)\)&lt;/span&gt;由下式给出：
&lt;/p&gt;
&lt;div class="math"&gt;$$\widetilde{P}_\theta(Z)=P(Z|Y,\theta)$$&lt;/div&gt;
&lt;p&gt;
并且&lt;span class="math"&gt;\(\widetilde{P}(\theta)\)&lt;/span&gt;随&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;连续变化。&lt;/p&gt;
&lt;p&gt;（2）若&lt;span class="math"&gt;\(\widetilde{P}_\theta(Z) = P(Z|Y,\theta)\)&lt;/span&gt;，则
&lt;/p&gt;
&lt;div class="math"&gt;$$F(\widetilde{P}, \theta)=logP(Y|\theta)$$&lt;/div&gt;
&lt;p&gt;
于是，可以得到EM算法用F函数的极大-极大的解释。&lt;/p&gt;
&lt;p&gt;（3）如果&lt;span class="math"&gt;\(F(\widetilde{P}, \theta)\)&lt;/span&gt;在&lt;span class="math"&gt;\(\widetilde{P}^*\)&lt;/span&gt;和&lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;有局部极大值，那么&lt;span class="math"&gt;\(L(\theta)\)&lt;/span&gt;也在&lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;有局部极大值。 类似的，如果&lt;span class="math"&gt;\(F(\widetilde{P}, \theta)\)&lt;/span&gt;在&lt;span class="math"&gt;\(\widetilde{P}^*\)&lt;/span&gt;和&lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;有全局极大值，那么&lt;span class="math"&gt;\(L(\theta)\)&lt;/span&gt;也在&lt;span class="math"&gt;\(\theta^*\)&lt;/span&gt;有全局极大值。&lt;/p&gt;
&lt;p&gt;（4）&lt;strong&gt;EM算法的一次迭代可由F函数的极大-极大算法实现。&lt;/strong&gt;
&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;为第i次迭代参数的估计，&lt;span class="math"&gt;\(\widetilde{P}^{(i)}\)&lt;/span&gt;为第i次迭代函数&lt;span class="math"&gt;\(\widetilde{P}\)&lt;/span&gt;的估计。在第i+1次迭代的两步为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对固定的&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;，求&lt;span class="math"&gt;\(\widetilde{P}^{(i+1)}\)&lt;/span&gt;使&lt;span class="math"&gt;\(F(\widetilde{P}, \theta^{(i)})\)&lt;/span&gt;极大化；&lt;/li&gt;
&lt;li&gt;对固定的&lt;span class="math"&gt;\(\widetilde{P}^{(i+1)}\)&lt;/span&gt;，求&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;使&lt;span class="math"&gt;\(F(\widetilde{P}^{(i+1), \theta})\)&lt;/span&gt;极大化；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过以上两步完成了EM算法的一次迭代。因此，EM算法与F函数的极大-极大算法得到的参数估计序列是一致的。&lt;/p&gt;
&lt;p&gt;于是就有了EM算法的推广。&lt;/p&gt;
&lt;h3&gt;GEM算法&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GEM算法1：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：观测数据，F函数&lt;/p&gt;
&lt;p&gt;输出：模型参数&lt;/p&gt;
&lt;p&gt;（1） 初始化参数&lt;span class="math"&gt;\(\theta^{(0)}\)&lt;/span&gt;，开始迭代&lt;/p&gt;
&lt;p&gt;（2）第i+1次迭代，第1步：记&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;为参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的估计值，&lt;span class="math"&gt;\(\widetilde{P}^{(i)}\)&lt;/span&gt;为&lt;span class="math"&gt;\(\widetilde{P}\)&lt;/span&gt;的估计。求&lt;span class="math"&gt;\(\widetilde{P}^{(i+1)}\)&lt;/span&gt;使&lt;span class="math"&gt;\(F(\widetilde{P}, \theta^{(i)})\)&lt;/span&gt;极大化&lt;/p&gt;
&lt;p&gt;（3）第2步：求&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;使&lt;span class="math"&gt;\(F(\widetilde{P}^{(i+1), \theta})\)&lt;/span&gt;极大化&lt;/p&gt;
&lt;p&gt;（4）重复（2）和（3），直到收敛。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GEM算法2：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：观测数据，Q函数&lt;/p&gt;
&lt;p&gt;输出：模型参数&lt;/p&gt;
&lt;p&gt;（1）初始化参数&lt;span class="math"&gt;\(\theta^{(0)}\)&lt;/span&gt;，开始迭代&lt;/p&gt;
&lt;p&gt;（2）第i+1次迭代，第1步：记&lt;span class="math"&gt;\(\theta^{(i)}\)&lt;/span&gt;为参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的估计值，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta, \theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]=\sum_ZlogP(Y,Z|\theta)P(Z|y,\theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;（3）第2步：求&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;使
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta^{(i+1)},\theta^{(i)}) \geq Q(\theta^{(i)}, \theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;（4）重复（2）和（3），直到收敛。&lt;/p&gt;
&lt;p&gt;当参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的维数为d时，可以采用一种特殊的GEM法， 它将EM算法的M步分解为d次条件极大化， 每次只改变参数向量的一个分量，其余分量不改变。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GEM算法3：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：观测数据，Q函数&lt;/p&gt;
&lt;p&gt;输出：模型参数&lt;/p&gt;
&lt;p&gt;（1）初始化参数&lt;span class="math"&gt;\(\theta^{(0)}\)&lt;/span&gt;，开始迭代&lt;/p&gt;
&lt;p&gt;（2）第i+1次迭代，第1步：记&lt;span class="math"&gt;\(\theta^{(i)}=(\theta_1^{(i)},\theta_2^{(i)},...,\theta_d^{(i)})\)&lt;/span&gt;为参数&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的估计值，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta, \theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y,\theta^{(i)}]=\sum_ZlogP(Y,Z|\theta)P(Z|y,\theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;（3）第2步：进行d次条件极大化：&lt;/p&gt;
&lt;p&gt;首先，在&lt;span class="math"&gt;\(\theta_2^{(i)},...,\theta_k^{(i)}\)&lt;/span&gt;保持不变的条件下求使&lt;span class="math"&gt;\(Q(\theta, \theta^{(i)})\)&lt;/span&gt;达到极大的&lt;span class="math"&gt;\(\theta_1^{(i+1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然后，在&lt;span class="math"&gt;\(\theta_1^{(i)},\theta_3^{(i)},...,\theta_k^{(i)}\)&lt;/span&gt;保持不变的条件下求使&lt;span class="math"&gt;\(Q(\theta, \theta^{(i)})\)&lt;/span&gt;达到极大的&lt;span class="math"&gt;\(\theta_2^{(i+1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;如此继续，经过d次条件极大化，得到求&lt;span class="math"&gt;\(\theta^{(i+1)}\)&lt;/span&gt;使
&lt;/p&gt;
&lt;div class="math"&gt;$$Q(\theta^{(i+1)},\theta^{(i)}) \geq Q(\theta^{(i)}, \theta^{(i)})$$&lt;/div&gt;
&lt;p&gt;（4）重复（2）和（3），直到收敛。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第七章&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:Jesen"&gt;
&lt;p&gt;&lt;span class="math"&gt;\(log\sum_j\lambda_jy_j \geq \sum_j\\lambda_jlogy_j\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(\lambda_j \geq 0, \sum_j\lambda_j=1\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:Jesen" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>提升树</title><link href="http://www.wengweitao.com/ti-sheng-shu.html" rel="alternate"></link><updated>2014-08-07T22:27:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-07:ti-sheng-shu.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;提升树模型&lt;/h2&gt;
&lt;p&gt;以决策树为基函数的提升方法称为提升树（boosting tree）。一个基本分类器x &amp;lt; v或者x &amp;gt; v，可以看作是由一个根结点直接连接两个叶节点的简单决策树，即所谓的决策树桩（decision stump）。提升树模型可以表示为决策树的加法模型：
&lt;/p&gt;
&lt;div class="math"&gt;$$f_M(x)=\sum_{m=1}^{M}T(x;\varTheta_m)$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(T(x;\varTheta_m)\)&lt;/span&gt;表示决策树；&lt;span class="math"&gt;\(\varTheta_m\)&lt;/span&gt;为决策树的参数；M为树的个数。&lt;/p&gt;
&lt;h2&gt;提升树算法&lt;/h2&gt;
&lt;p&gt;提升树算法采用前向分步算法。首先，确定原始提升树&lt;span class="math"&gt;\(f_0(x)=0\)&lt;/span&gt;，第m步的模型是
&lt;/p&gt;
&lt;div class="math"&gt;$$f_m(x)=f_{m-1}(x)+T(x;\varTheta_m)$$&lt;/div&gt;
&lt;p&gt;
其中，&lt;span class="math"&gt;\(f_{m-1}(x)\)&lt;/span&gt;为当前模型，通过经验风险最小化确定下一棵决策树的参数&lt;span class="math"&gt;\(\varTheta_m\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\widehat{\varTheta_m}=argmin_{\varTheta_m}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+T(x_i;\varTheta_m))$$&lt;/div&gt;
&lt;p&gt;针对不同问题的提升树算法，其主要区别是使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题你，以及用一般损失函数的一般决策问题。&lt;/p&gt;
&lt;p&gt;对于&lt;strong&gt;回归问题的提升树&lt;/strong&gt;。已知一个训练数据集T，如果将输入空间&lt;span class="math"&gt;\(\chi\)&lt;/span&gt;划分为J个互不相交的区域&lt;span class="math"&gt;\(R_1, R_2,...,R_j\)&lt;/span&gt;，并且在每个区域上确定输出的常量&lt;span class="math"&gt;\(c_j\)&lt;/span&gt;，那么树可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$T(x;\varTheta)=\sum_{j=1}^{J}c_jI(x \in R_j)$$&lt;/div&gt;
&lt;p&gt;
J是回归树的复杂度即叶节点的个数。&lt;/p&gt;
&lt;p&gt;回归问题提升树使用以下前向分步算法：
&lt;/p&gt;
&lt;div class="math"&gt;$$f_0(x)=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$f_m(x)=f_{m-1}(x)+T(x;\varTheta_m)$$&lt;/div&gt;
&lt;div class="math"&gt;$$f_M(x)=\sum_{m=1}^{M}T(x;\varTheta_m)$$&lt;/div&gt;
&lt;p&gt;在前向分步算法的第m步，给定当前模型&lt;span class="math"&gt;\(f_{m-1}(x)\)&lt;/span&gt;，需求解
&lt;/p&gt;
&lt;div class="math"&gt;$$\widehat{\varTheta_m}=argmin_{\varTheta_m}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+T(x_i;\varTheta_m))$$&lt;/div&gt;
&lt;p&gt;
得到&lt;span class="math"&gt;\(\widehat{\varTheta_m}\)&lt;/span&gt;，即第m棵树的参数。&lt;/p&gt;
&lt;p&gt;当采用平方损失误差函数时：
&lt;/p&gt;
&lt;div class="math"&gt;$$L(y,f(x)) = (f(x) - y)^2$$&lt;/div&gt;
&lt;p&gt;
其损失变为：
&lt;/p&gt;
&lt;div class="math"&gt;$$L(y,f_{m-1}(x)+T(x;\varTheta_m)) = [y - f_{m-1}(x)-T(x;\varTheta_m)]^2 = [r - T(x;\varTheta_m)]^2$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(r=y - f_{m-1}(x)\)&lt;/span&gt;，是当前模型拟合数据的&lt;strong&gt;残差（residual）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以对于回归问题的提升树算法，只需简单地拟合当前模型的残差。这样的算法是相当简单的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;回归问题的提升树算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：训练数据集T&lt;/p&gt;
&lt;p&gt;输出：提升树&lt;span class="math"&gt;\(f_M(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）初始化&lt;span class="math"&gt;\(f_0(x)=0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（2）对m=1,2,...,M&lt;/p&gt;
&lt;p&gt;（a）计算残差
&lt;/p&gt;
&lt;div class="math"&gt;$$r_{mi}=y_i - f_{m-1}(x_i),　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;（b）拟合残差&lt;span class="math"&gt;\(r_{mi}\)&lt;/span&gt;学习一棵回归树，得到&lt;span class="math"&gt;\(T(x;\varTheta_m)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（c）更新&lt;span class="math"&gt;\(f_m(x)=f_{m-1}(x)+T(x;\varTheta_m)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（3）得到回归问题提升树
&lt;/p&gt;
&lt;div class="math"&gt;$$f_M(x)=\sum_{m=1}^{M}T(x;\varTheta_m)$$&lt;/div&gt;
&lt;h2&gt;梯度提升&lt;/h2&gt;
&lt;p&gt;当损失函数是平方损失和指数损失函数时，每一步的优化使很简单的。但对一般的损失函数而言，往往每一步的优化就没有那么容易了。针对这一问题，提出了&lt;strong&gt;梯度提升（gradient boosting）&lt;/strong&gt;算法。这是利用最速下降法的近似算法，其关键是利用损失函数的负梯度在当前模型下的值：
&lt;/p&gt;
&lt;div class="math"&gt;$$-[\frac{\partial L(y,f(x_i)}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$$&lt;/div&gt;
&lt;p&gt;
作为回归问题提升树算法中的残差近似值，拟合一棵回归树。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;梯度提升算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：训练数据集T；损失函数L(y,f(x))&lt;/p&gt;
&lt;p&gt;输出：提升树&lt;span class="math"&gt;\(\widehat{f}(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）初始化
&lt;/p&gt;
&lt;div class="math"&gt;$$f_0(x)=argmin_c\sum_{i=1}^NL(y_i,c)$$&lt;/div&gt;
&lt;p&gt;
估计使损失函数极小化的常数值，它是只有一个根结点的树。&lt;/p&gt;
&lt;p&gt;（2）对m=1,2,...,M&lt;/p&gt;
&lt;p&gt;（a）对i=1,2,...,N，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$r_{mi}=-[\frac{\partial L(y_i,f(x_i)}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$$&lt;/div&gt;
&lt;p&gt;（b）对&lt;span class="math"&gt;\(r_{mi}\)&lt;/span&gt;拟合一棵回归树，得到第m棵树的叶节点区域&lt;span class="math"&gt;\(R_{mj}\)&lt;/span&gt;, j=1,2,...,J，即估计回归树叶节点区域&lt;/p&gt;
&lt;p&gt;（c）对j=1,2,...,J，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$c_{mj}=argmin_c\sum_{x_i \in R_{mj}}L(y_i, f_{m-1}(x_i)+c)$$&lt;/div&gt;
&lt;p&gt;
估计叶节点区域的值，并且使损失函数最小化&lt;/p&gt;
&lt;p&gt;（d）更新回归树&lt;span class="math"&gt;\(f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{mj}I(x \in R_{mj}）\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（3）得到回归树最终的模型
&lt;/p&gt;
&lt;div class="math"&gt;$$\widehat{f}(x)=f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x \in R_{mj}）$$&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第八章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>AdaBoost算法</title><link href="http://www.wengweitao.com/adaboostsuan-fa.html" rel="alternate"></link><updated>2014-08-06T19:56:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-06:adaboostsuan-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;提升（boosting）方法是一种常用的统计学习方法。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;提升方法AdaBoost算法&lt;/h2&gt;
&lt;h3&gt;提升方法的基本思路&lt;/h3&gt;
&lt;p&gt;提升方法基于这一思想：对于一个复杂的任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家的单独判断好。&lt;/p&gt;
&lt;p&gt;在概率近似正确（probably approximately correct， PAC）学习框架中，一个概念（或者类），如果存在一个多项式学习算法能够学习它，并且正确率很高，那么就称这个概念是&lt;strong&gt;强可学习的&lt;/strong&gt;；如果存在一个多项式学习算法能够学习它，并且正确率仅比随机猜测略好，那么就称这个概念是&lt;strong&gt;弱可学习的&lt;/strong&gt;。可以证明强可学习与弱可学习是等价的。&lt;/p&gt;
&lt;p&gt;这样一来，我们在学习中，如果发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”。弱学习算法通常要比强学习算法容易得多，那么如何具体实施提升？关于提升的方法最具代表性的是AdaBoost算法。&lt;/p&gt;
&lt;p&gt;对于分类问题而言，一个粗糙的分类规则要比精确的分类规则容易的多。&lt;strong&gt;提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器构成一个强分类器。&lt;/strong&gt;大多数提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。&lt;/p&gt;
&lt;p&gt;这样，对提升方法来说，&lt;strong&gt;有两个问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每一轮如何改变训练数据的权值或概率分布&lt;/li&gt;
&lt;li&gt;如何将弱分类器组合成一个强分类器。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于第一个问题，AdaBoost通过提高前一轮弱分类器错误分类样本的权值，而降低被正确分类样本的权值，这样那些没有被正确分类的数据，在后一轮就会受到弱分类器更大的关注。对于第二个问题，AdaBoost采取加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。&lt;/p&gt;
&lt;h3&gt;AdaBoost算法&lt;/h3&gt;
&lt;p&gt;输入：训练数据集T；弱学习算法&lt;/p&gt;
&lt;p&gt;输出：最终分类器G(x)&lt;/p&gt;
&lt;p&gt;（1）初始化训练数据的权值分布
&lt;/p&gt;
&lt;div class="math"&gt;$$D_1=(w_{11},...,w_{1i},...,w_{1N}), 　w_{1i}=\frac{1}{N}, 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
其中N为样本数据的总数，D的下标为m代表的是第几轮，假设训练数据集具有均匀的权值分布。&lt;/p&gt;
&lt;p&gt;（2）在每一轮对m=1,2,...,M依次执行下列操作：&lt;/p&gt;
&lt;p&gt;（a）使用具有权值分布&lt;span class="math"&gt;\(D_m\)&lt;/span&gt;的训练数据集进行学习，得到基本分类器
&lt;/p&gt;
&lt;div class="math"&gt;$$G_m(x):\chi \rightarrow \{-1, +1\}$$&lt;/div&gt;
&lt;p&gt;（b）计算&lt;span class="math"&gt;\(G_m(x)\)&lt;/span&gt;在训练数据集上的分类误差率
&lt;/p&gt;
&lt;div class="math"&gt;$$e_m=P(G_m(x_i) \neq y_i)=\sum_{i=1}^{N}w_{mi}I(G_m(x_i) \neq y_i)$$&lt;/div&gt;
&lt;p&gt;（c）计算&lt;span class="math"&gt;\(G_m(x)\)&lt;/span&gt;的系数
&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$$&lt;/div&gt;
&lt;p&gt;
这里的对数为自然对数。&lt;span class="math"&gt;\(\alpha_m\)&lt;/span&gt;随着&lt;span class="math"&gt;\(e_m\)&lt;/span&gt;的减小而增大。&lt;/p&gt;
&lt;p&gt;（d）更新训练数据集的权值分布
&lt;/p&gt;
&lt;div class="math"&gt;$$D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N}) $$&lt;/div&gt;
&lt;div class="math"&gt;$$w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)), 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
即：
&lt;/p&gt;
&lt;div class="math"&gt;$$w_{mm+1,i}=\left\{\begin{matrix}
\frac{w_{mi}}{Z_m}e^{-\alpha_m}, &amp;amp; G_m(x_i)=y_i\\ 
\frac{w_{mi}}{Z_m}e^{\alpha_m} &amp;amp; G_m(x_i) \neq y_i
\end{matrix}\right.$$&lt;/div&gt;
&lt;p&gt;
可见被基本分类器误分类样本的权值得以扩大，被正确分类的权值缩小。&lt;/p&gt;
&lt;p&gt;这里&lt;span class="math"&gt;\(Z_m\)&lt;/span&gt;是规范化因子，使概率总和为1
&lt;/p&gt;
&lt;div class="math"&gt;$$Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))$$&lt;/div&gt;
&lt;p&gt;（3）构建基本分类器的线性组合
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\sum_{m=1}^M\alpha_mG_m(x)$$&lt;/div&gt;
&lt;p&gt;
得到最终分类器
&lt;/p&gt;
&lt;div class="math"&gt;$$G(x)=sign(f(x))=sign(\sum_{m=1}^M\alpha_mG_m(x))$$&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;AdaBoost算法的训练误差分析&lt;/h2&gt;
&lt;p&gt;AdaBoost算法最基本的性质是它能在学习过程中不断减小训练误差，即在训练数据集上的分类误差率。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AdaBoost的训练误差界：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AdaBoost算法最终分类器的训练误差界为
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{N}\sum_{i=1}^{N}I(G(x_i) \neq y_i) \leq \frac{1}{N}\sum_iexp(-y_if(x_i))= \prod_mZ_m$$&lt;/div&gt;
&lt;p&gt;说明，可以再每轮选取适当的&lt;span class="math"&gt;\(G_m\)&lt;/span&gt;使得&lt;span class="math"&gt;\(Z_m\)&lt;/span&gt;最小，从而使得训练误差下降最快。对于二类分类问题，有如下结果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;二类分类问题AdaBoost的训练误差界：&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$\prod_{m=1}^{M}Z_m=\prod_{m=1}^{M}[2\sqrt{e_m(1-e_m)}]=\prod_{m=1}^{M}\sqrt{(1-4\gamma_m^2)} \leq exp(-2\sum_{m=1}^{M}\gamma_m^2)　其中\gamma_m=\frac{1}{2}-e_m$$&lt;/div&gt;
&lt;p&gt;如果存在&lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;，对所有m有&lt;span class="math"&gt;\(r_m \geq \gamma\)&lt;/span&gt;，则
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{N}\sum_{i=1}^{N}I(G(x_i) \neq y_i) \leq exp(-2M\gamma^2)$$&lt;/div&gt;
&lt;p&gt;这表明在此条件下&lt;strong&gt;AdaBoost的训练误差是以指数速率下降的&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;AdaBoost算法的解释&lt;/h2&gt;
&lt;p&gt;AdaBoost算法另外一种解释是，AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。&lt;/p&gt;
&lt;h3&gt;前向分步算法&lt;/h3&gt;
&lt;p&gt;考虑加法模型（additive model）
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=\sum_{m=1}^{M}\beta_mb(x;\gamma_m)$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(b(x;\gamma_m)\)&lt;/span&gt;是基函数，&lt;span class="math"&gt;\(\gamma_m\)&lt;/span&gt;是基函数的参数，&lt;span class="math"&gt;\(\beta_m\)&lt;/span&gt;是基函数的系数。&lt;/p&gt;
&lt;p&gt;在给定训练数据及损失函数L(y,f(x))的条件下，学习加法模型f(x)成为经典风险极小化即损失函数极小化问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\beta_m, \gamma_m}\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i;\gamma_m))$$&lt;/div&gt;
&lt;p&gt;
前向分步算法（forward stagewise algorithm）求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体的，每步只需优化如下损失函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\beta, \gamma}\sum_{i=1}^{N}L(y_i,\beta b(x_i;\gamma))$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;前向分步算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：训练数据集T；损失函数L(y,f(x))；基函数集&lt;span class="math"&gt;\({b(x;\gamma)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;输出：加法模型f(x)&lt;/p&gt;
&lt;p&gt;（1）初始化&lt;span class="math"&gt;\(f_0(x)=0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（2）对m=1,2,...,M&lt;/p&gt;
&lt;p&gt;（a）极小化损失函数
&lt;/p&gt;
&lt;div class="math"&gt;$$(\beta_m, \gamma_m)=argmin_{\beta, \gamma}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))$$&lt;/div&gt;
&lt;p&gt;
 得到参数&lt;span class="math"&gt;\(\beta_m, \gamma_m\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（b）更新
&lt;/p&gt;
&lt;div class="math"&gt;$$f_m(x)=f_{m-1}(x)+\beta b(x;\gamma_m)$$&lt;/div&gt;
&lt;p&gt;（3）得到加法模型
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=f_M(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m)$$&lt;/div&gt;
&lt;p&gt;这样优化问题就化为逐次求解各个&lt;span class="math"&gt;\(\beta_m, \gamma_m\)&lt;/span&gt;的优化问题。&lt;/p&gt;
&lt;h3&gt;前向分步算法与AdaBoost&lt;/h3&gt;
&lt;p&gt;AdaBoost算法是前向分步加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第八章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>支持向量机——SMO（序列最小最优化算法）</title><link href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-smoxu-lie-zui-xiao-zui-you-hua-suan-fa.html" rel="alternate"></link><updated>2014-08-05T10:02:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-05:zhi-chi-xiang-liang-ji-smoxu-lie-zui-xiao-zui-you-hua-suan-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;SVM的学习算法可以归结为凸二次规划问题。这样的凸二次规划问题你具有全局最优解，并且许多最优化算法可以用来求解，但是当训练样本容量很大，这些算法往往变得非常低效，以致无法使用。所以本文介绍了如何高效地实现支持向量机学习。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们将用序列最小最优化（sequential minimal optimization, SMO）算法，求解
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha}\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jK(x_i,x_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. 　\sum_{i=1}^{N}\alpha_iy_i = 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$0 \leq \alpha_i \leq C 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
在这个问题中，变量是拉格朗日乘子，一个变量&lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;对应一个样本点&lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt;；变量的总数等于样本容量N。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SMO算法是一种启发式算法，其基本思路是：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果所有变量的解都满足此最优化问题的KKT条件，那么这个最优化问题的解就得到了（因为KKT条件是该最优化问题的充分必要条件）。否则选择两个变量，固定其他的变量，针对这两个问题构建一个二次规划问题。这个二次规划问题的解应该更接近二次规划问题的解（因为这会使得原始二次规划问题的目标函数值更小）。而且，这时子问题可以通过解析方法求解，这样就大大提高了整个算法的计算速度。&lt;/p&gt;
&lt;p&gt;每一次，子问题都有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而得到原问题的最终解。&lt;/p&gt;
&lt;p&gt;注意，每个子问题有两个变量，而不能是1个变量，因为当选择一个变量时，由于约束条件，其他变量的值就固定了该变量的值也就固定了。所以子问题中同时更新两个变量。&lt;/p&gt;
&lt;p&gt;SMO算法包括两个部分：&lt;/p&gt;
&lt;p&gt;（1）求解两个变量二次规划问题的解析方法&lt;/p&gt;
&lt;p&gt;（2）选择变量的启发式方法&lt;/p&gt;
&lt;h2&gt;两个变量二次规划问题的求解方法&lt;/h2&gt;
&lt;p&gt;不失一般性，假设选择的两个变量是&lt;span class="math"&gt;\(\alpha_1, \alpha_2\)&lt;/span&gt;，其他变量&lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;固定。那么SMO的最优化问题的子问题可以写成
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha1, \alpha_2}W(\alpha_1, \alpha_2)=\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2-(\alpha_1+\alpha_2)+y_1y_2K_{12}\alpha_1\alpha_2+ y_1\alpha_1\sum_{i=3}^{N}\alpha_iy_iK_{i1}+y_2\alpha_2\sum_{i=3}^{N}\alpha_iy_iK_{i2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. 　\alpha_1y_1 + \alpha_2y_2= -\sum_{i=3}^{N}\alpha_iy_i = \zeta $$&lt;/div&gt;
&lt;div class="math"&gt;$$0 \leq \alpha_i \leq C 　i=1,2$$&lt;/div&gt;
&lt;p&gt;
以上的目标函数式省略了不含&lt;span class="math"&gt;\(\alpha_1,\alpha_2\)&lt;/span&gt;的常数项。&lt;/p&gt;
&lt;p&gt;首先，我们分析下约束条件，然后求此约束条件下的极小。&lt;/p&gt;
&lt;p&gt;约束条件使得目标函数在一条平行于长度为C的正方形的对角线的线段上的最优值。这使得两个变量的最优化问题实质上是单变量的最优化问题，不妨考虑&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;的最优化问题。&lt;/p&gt;
&lt;p&gt;假设初始可行解为&lt;span class="math"&gt;\(\alpha_1^{old},\alpha_2^{old}\)&lt;/span&gt;，最优解为&lt;span class="math"&gt;\(\alpha_1^{new},\alpha_2^{new}\)&lt;/span&gt;。并假设在沿着约束方向未经剪辑&lt;sup id="fnref:unclipped"&gt;&lt;a class="footnote-ref" href="#fn:unclipped" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;时&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;的最优解为&lt;span class="math"&gt;\(\alpha_2^{new, unc}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;由于&lt;span class="math"&gt;\(\alpha_2^{new}\)&lt;/span&gt;需要满足不等式约束，所以最优值&lt;span class="math"&gt;\(\alpha_2^{new}\)&lt;/span&gt;的取值范围必须满足条件：
&lt;/p&gt;
&lt;div class="math"&gt;$$L \leq \alpha_2^{new} \leq H$$&lt;/div&gt;
&lt;p&gt;
其中L与H分别是&lt;span class="math"&gt;\(\alpha_2^{new}\)&lt;/span&gt;所在的对角线段端点的界，如下图所示。最优解必须在正方形内且在与对角线平行的线上。&lt;/p&gt;
&lt;p&gt;&lt;img alt="smo_alpha" src="./imgs/smo_alpha.png" /&gt;&lt;/p&gt;
&lt;p&gt;当&lt;span class="math"&gt;\(y_1=y_2\)&lt;/span&gt;时，它们可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha_1 + \alpha_2= k$$&lt;/div&gt;
&lt;p&gt;
此时有：
&lt;/p&gt;
&lt;div class="math"&gt;$$L=max(0, \alpha_1^{old}+\alpha_2^{old}-C), H=min(C, \alpha_1^{old}+\alpha_2^{old})$$&lt;/div&gt;
&lt;p&gt;
计算过程：&lt;/p&gt;
&lt;p&gt;如图所示，先考虑L的取值。当直线为①情况时，L为0；当直线为②情况时，此时&lt;span class="math"&gt;\(\alpha_1'=C\)&lt;/span&gt;，代入&lt;span class="math"&gt;\(\alpha_1' + \alpha_2' = k\)&lt;/span&gt; 得到&lt;span class="math"&gt;\(\alpha_2'= L = k - \alpha_1' = k - C = \alpha_1 + \alpha_2 - C\)&lt;/span&gt;。所以&lt;span class="math"&gt;\(L=max(0, \alpha_1^{old}+\alpha_2^{old}-C)\)&lt;/span&gt;。然后考虑H的取值。当直线为①情况时，此时&lt;span class="math"&gt;\(\alpha_1'=0\)&lt;/span&gt;，代入&lt;span class="math"&gt;\(\alpha_1' + \alpha_2' = k\)&lt;/span&gt; 得到&lt;span class="math"&gt;\(\alpha_2'= H = k - \alpha_1' = k  = \alpha_1 + \alpha_2\)&lt;/span&gt;；当直线为②情况时，H为C。所以&lt;span class="math"&gt;\(H=min(C, \alpha_1^{old}+\alpha_2^{old})\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;当&lt;span class="math"&gt;\(y_1 \neq y_2\)&lt;/span&gt;时，它们可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha_1 - \alpha_2= k$$&lt;/div&gt;
&lt;p&gt;
此时有：
&lt;/p&gt;
&lt;div class="math"&gt;$$L=max(0, \alpha_2^{old}-\alpha_1^{old}-C), H=min(C, C+\alpha_2^{old}-\alpha_1^{old})$$&lt;/div&gt;
&lt;p&gt;为了方便，记
&lt;/p&gt;
&lt;div class="math"&gt;$$g(x)=\sum_{i=1}^{N}\alpha_iy_iKK(x_i,x)+b$$&lt;/div&gt;
&lt;p&gt;
令
&lt;/p&gt;
&lt;div class="math"&gt;$$E_i=g(x_i)-y_i=(\sum_{i=1}^{N}\alpha_iy_iKK(x_i,x)+b)-y_i,　i=1,2$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(E_i\)&lt;/span&gt;是函数g(x)对输入&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;的预测值与真实输出&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;之差。&lt;/p&gt;
&lt;p&gt;下面，先求沿着约束方向  未经剪辑时&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;的最优解&lt;span class="math"&gt;\(\alpha_2^{new, unc}\)&lt;/span&gt;；然后再求剪辑后&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;的解&lt;span class="math"&gt;\(\alpha_2^{new}\)&lt;/span&gt;。可以证明&lt;sup id="fnref:calculate"&gt;&lt;a class="footnote-ref" href="#fn:calculate" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha_2^{new, unc}=\alpha_2^{old}+\frac{y_2(E_1 - E_2)}{\eta}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha_2^{new}=\left\{\begin{matrix}
H,　\alpha_2^{new, unc}&amp;gt;H\\ 
\alpha_2^{new, unc},　0 \leq \alpha_2^{new, unc}\leq H\\ 
L,　\alpha_2^{new, unc}　&amp;lt; L
\end{matrix}\right.$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha_1^{new}=\alpha_1^{old}+y_1y_2(\alpha_2^{old}-\alpha_2^{new})$$&lt;/div&gt;
&lt;p&gt;
其中，
&lt;/p&gt;
&lt;div class="math"&gt;$$\eta=K_{11}+K_{22}-2K_{12}=||\phi(x_1)-\phi(x_2)||$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;是输入空间到特征空间的映射。&lt;/p&gt;
&lt;p&gt;这样就得到了最优化问题的解&lt;span class="math"&gt;\((\alpha_1^{new},\alpha_2^{new})\)&lt;/span&gt;。&lt;/p&gt;
&lt;h2&gt;变量的选择方法&lt;/h2&gt;
&lt;p&gt;SMO算法在每个子问题中选择两个变量变化，其中至少一个变量是违反KKT条件的。&lt;/p&gt;
&lt;p&gt;（1）第一个变量的选择&lt;/p&gt;
&lt;p&gt;SMO称选择第1个变量的过程为外层循环。外层循环是在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量。因此，需要检验训练样本点&lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt;是否满足KKT条件。具体的，在&lt;a href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-xian-xing-zhi-chi-xiang-liang-ji.html"&gt;线性支持向量机学习算法&lt;/a&gt;中根据KKT条件有：&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown_w L(w^*, b^*, \xi^*, \alpha^*, \mu^*)=w^*-\sum_{i=1}^{N}\alpha_i^*y_ix_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigtriangledown_b L(w^*, b^*, \xi^*, \alpha^*, \mu^*)=-\sum_{i=1}^{N}\alpha_i^*y_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigtriangledown_\xi L(w^*, b^*, \xi^*, \alpha^*, \mu^*)=C-\alpha_i^*-\mu^*=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha^*(y_i(w_i^*.x_i + b^*)-1+\xi_i^*) = 0 $$&lt;/div&gt;
&lt;div class="math"&gt;$$u_i^*\xi_i^* = 0 $$&lt;/div&gt;
&lt;div class="math"&gt;$$y_i(w_i^*.x_i + b^*)-1+\xi_i^* \geq 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\xi_i^* \geq 0 $$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha_i^* \geq 0 $$&lt;/div&gt;
&lt;div class="math"&gt;$$\mu_i^* \geq 0 $$&lt;/div&gt;
&lt;p&gt;
当&lt;span class="math"&gt;\(\alpha_i=0\)&lt;/span&gt;时，&lt;span class="math"&gt;\(\mu_i=C\)&lt;/span&gt;，那么松弛变量&lt;span class="math"&gt;\(\xi_i=0\)&lt;/span&gt;，得到&lt;span class="math"&gt;\(y_ig(x_i) \geq 1\)&lt;/span&gt;；&lt;/p&gt;
&lt;p&gt;当&lt;span class="math"&gt;\(0 &amp;lt; \alpha_i &amp;lt; C\)&lt;/span&gt;时，&lt;span class="math"&gt;\(\mu_i &amp;gt; 0\)&lt;/span&gt;，那么松弛变量&lt;span class="math"&gt;\(\xi_i=0\)&lt;/span&gt;，得到&lt;span class="math"&gt;\(y_ig(x_i) = 1\)&lt;/span&gt;；&lt;/p&gt;
&lt;p&gt;当&lt;span class="math"&gt;\(\alpha_i=C\)&lt;/span&gt;时，&lt;span class="math"&gt;\(\mu_i=0\)&lt;/span&gt;，那么松弛变量&lt;span class="math"&gt;\(\xi_i &amp;gt; 0\)&lt;/span&gt;，得到&lt;span class="math"&gt;\(y_ig(x_i) = 1 - \xi_i\)&lt;/span&gt;即&lt;span class="math"&gt;\(y_ig(x_i) \leq 1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在检验过程中，外层循环首先遍历所有满足条件&lt;span class="math"&gt;\(0&amp;lt;\alpha_i &amp;lt; C\)&lt;/span&gt;的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件。如果都满足KKT条件，那么 再遍历整个数据集，检验它们是否满足KKT条件。&lt;/p&gt;
&lt;p&gt;（2）第2个变量的选择&lt;/p&gt;
&lt;p&gt;SMO称选择第2个变量的过程为内层循环。假设在外层循环中已经找到第1个变量&lt;span class="math"&gt;\(\alpha_1\)&lt;/span&gt;，现在找第2个变量。第2个变量的选择标准是希望能使&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;有足够大的变化。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\alpha_2^{news}\)&lt;/span&gt;是依赖于&lt;span class="math"&gt;\(|E_1-E_2|\)&lt;/span&gt;的。一种简单的做法是选择&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;，使其对应的&lt;span class="math"&gt;\(|E_1-E_2|\)&lt;/span&gt;最大，因为&lt;span class="math"&gt;\(\alpha_1\)&lt;/span&gt;已经确定，&lt;span class="math"&gt;\(E_1\)&lt;/span&gt;也确定了。为了节省时间，把所有的&lt;span class="math"&gt;\(E_i\)&lt;/span&gt;保存在一个列表中。&lt;/p&gt;
&lt;p&gt;如果上面的方法还是不能是目标函数由足够的下降，那么采用启发式规则继续选&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;（3）计算阈值b和差值&lt;span class="math"&gt;\(E_i\)&lt;/span&gt;
每次完成两个变量的优化后，都要重新计算E，于是就还得重新计算阈值b。&lt;/p&gt;
&lt;p&gt;当&lt;span class="math"&gt;\(0&amp;lt;\alpha_1^{new}&amp;lt; C\)&lt;/span&gt;时，由KKT条件&lt;span class="math"&gt;\(y_ig(x_i) = 1\)&lt;/span&gt;及&lt;span class="math"&gt;\(y_i^2=1\)&lt;/span&gt;，有
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^{N}\alpha_iy_iK_{i1}+b=y_1$$&lt;/div&gt;
&lt;p&gt;
于是
&lt;/p&gt;
&lt;div class="math"&gt;$$b_1^{new}=y_1-\sum_{i=3}^N\alpha_iy_iK_{i1}-\alpha_1^{new}y_1K_{11}-\alpha_2^{new}y_2K_{21}$$&lt;/div&gt;
&lt;div class="math"&gt;$$E_1=g(x_1)-y_1=\sum_{i=3}^N\alpha_iy_iK_{i1}+\alpha_1^{old}y_1K_{11}+\alpha_2^{old}y_2K_{21} + b^{old}-y_1$$&lt;/div&gt;
&lt;p&gt;
于是可得
&lt;/p&gt;
&lt;div class="math"&gt;$$b_1^{new}=-E_1-(\alpha_1^{new}-\alpha_1^{old})y_1K_{11}-(\alpha_2^{new}-\alpha_2^{old})y_2K_{21} + b^{old}$$&lt;/div&gt;
&lt;p&gt;同样如果&lt;span class="math"&gt;\(0&amp;lt;\alpha_2^{new}&amp;lt; C\)&lt;/span&gt;时，可得
&lt;/p&gt;
&lt;div class="math"&gt;$$b_2^{new}=-E_2-(\alpha_1^{new}-\alpha_1^{old})y_1K_{12}-(\alpha_2^{new}-\alpha_2^{old})y_2K_{22} + b^{old}$$&lt;/div&gt;
&lt;p&gt;如果&lt;span class="math"&gt;\(0&amp;lt;\alpha_1^{new}&amp;lt; C\)&lt;/span&gt;且&lt;span class="math"&gt;\(0&amp;lt;\alpha_2^{new}&amp;lt; C\)&lt;/span&gt;时，&lt;span class="math"&gt;\(b_1^{new}=b_2^{new}\)&lt;/span&gt;；&lt;/p&gt;
&lt;p&gt;如果&lt;span class="math"&gt;\(\alpha_1^{new},\alpha_2^{new}\)&lt;/span&gt;是0或者C，那么&lt;span class="math"&gt;\(b_1^{new}, b_2^{new}\)&lt;/span&gt;以及它们之间的数都是符合KKT条件的阈值，这时选择它们的中点作为&lt;span class="math"&gt;\(b^{new}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;每次完成两个变量的优化之后，更新对应的&lt;span class="math"&gt;\(E_i\)&lt;/span&gt;值，并将它们保存在列表中，&lt;span class="math"&gt;\(E_i\)&lt;/span&gt;值的更新要用到&lt;span class="math"&gt;\(b^{new}\)&lt;/span&gt;值，以及所有支持向量对应的&lt;span class="math"&gt;\(\alpha_j\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$E_i^{new}=(\sum_{S}\alpha_jy_jKK(x_i,x_j)+b^{new})-y_i$$&lt;/div&gt;
&lt;p&gt;
 其中，S是所有支持向量&lt;span class="math"&gt;\(x_j\)&lt;/span&gt;的集合。&lt;/p&gt;
&lt;h2&gt;SMO算法&lt;/h2&gt;
&lt;p&gt;输入：训练数据集T，精度&lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;输出：近似解&lt;span class="math"&gt;\(\widehat{\alpha}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）取初值&lt;span class="math"&gt;\(\alpha^{(0)}=0\)&lt;/span&gt;，令k=0&lt;/p&gt;
&lt;p&gt;（2）选取优化变量&lt;span class="math"&gt;\(\alpha_1^{(k)},\alpha_2^{(k)}\)&lt;/span&gt;，解析求解两个变量的最优化问题，求得最优解&lt;span class="math"&gt;\(\alpha_1^{(k+1)},\alpha_2^{(k+1)}\)&lt;/span&gt;，更新&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;为&lt;span class="math"&gt;\(\alpha^{(k+1)}\)&lt;/span&gt;；&lt;/p&gt;
&lt;p&gt;（3）若在精度&lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;范围内满足停机条件
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^N\alpha_iy_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$0 \leq \alpha_i \leq C 　i=1,2,...,N$$&lt;/div&gt;
&lt;div class="math"&gt;$$y_i.g(x_i)=\left\{\begin{matrix}
\geq 1, &amp;amp; {x_i|\alpha_1=0}\\ 
=1, &amp;amp; {x_i|0&amp;lt;\alpha_i&amp;lt;C}\\ 
\leq 1, &amp;amp; {x_i|\alpha_i=C}
\end{matrix}\right.$$&lt;/div&gt;
&lt;p&gt;则转（4）；否则令k=k+1，转（2）&lt;/p&gt;
&lt;p&gt;（4）取&lt;span class="math"&gt;\(\widehat{\alpha}=\alpha^{(k+1)}\)&lt;/span&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第七章&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:unclipped"&gt;
&lt;p&gt;未经剪辑是指未考虑&lt;span class="math"&gt;\(0 \leq \alpha_i \leq C\)&lt;/span&gt;的约束条件时的值&amp;#160;&lt;a class="footnote-backref" href="#fnref:unclipped" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:calculate"&gt;
&lt;p&gt;求解过程如下：
记
&lt;div class="math"&gt;$$v_i = \sum_{j=3}^{N}\alpha_jy_jK(x_i,x_j)= g(x_i)- \sum_{j=1}^{2}\alpha_jy_jK(x_i,x_j)-b,　　i=1,2$$&lt;/div&gt;
目标函数变为：
&lt;div class="math"&gt;$$W(\alpha_1, \alpha_2)=\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2-(\alpha_1+\alpha_2)+y_1y_2K_{12}\alpha_1\alpha_2+y_1\alpha_1v_1+y_2\alpha_2v_2$$&lt;/div&gt;
可以将&lt;span class="math"&gt;\(\alpha_1\)&lt;/span&gt;表示为:
&lt;div class="math"&gt;$$\alpha_1=(\zeta - y_2\alpha_2)y_1$$&lt;/div&gt;
代入目标函数，得到只含有&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;的式子，对&lt;span class="math"&gt;\(\alpha_2\)&lt;/span&gt;求导数，令其为0，并将&lt;span class="math"&gt;\(\alpha_1^{old} y_1 + \alpha_2^{old}y_2=  \zeta\)&lt;/span&gt;就可得到。 &amp;#160;&lt;a class="footnote-backref" href="#fnref:calculate" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>支持向量机——非线性支持向量机</title><link href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-fei-xian-xing-zhi-chi-xiang-liang-ji.html" rel="alternate"></link><updated>2014-08-04T19:39:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-04:zhi-chi-xiang-liang-ji-fei-xian-xing-zhi-chi-xiang-liang-ji.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;如果分类问题是非线性的，那么就要用到非线性支持向量机。非线性支持向量机主要特点就是利用核技巧。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;核技巧&lt;/h2&gt;
&lt;h3&gt;非线性分类问题&lt;/h3&gt;
&lt;p&gt;利用非线性模型才能很好地进行分类的问题，就是非线性分类问题。如果能用&lt;span class="math"&gt;\(R^n\)&lt;/span&gt;中的一个超平面将正负例正确分开，则称这个问题为非线性可分问题。如下的例子，无法用直线（线性模型）将正负实例正确分开，但可以用一条椭圆（非线性模型）将它们正确分开。&lt;/p&gt;
&lt;p&gt;&lt;img alt="非线性可分例子" src="./imgs/nolinear.png" /&gt;&lt;/p&gt;
&lt;p&gt;通常进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。例如，上图中将椭圆变换为右图中的直线，将非线性分类问题变换为线性分类问题。&lt;/p&gt;
&lt;p&gt;核技巧应用到支持向量机，其基本思想就是通过一个非线性变换将输入空间（欧氏空间&lt;span class="math"&gt;\(R^n\)&lt;/span&gt;或离散集合）对应于一个特征空间（希尔伯特空间&lt;sup id="fnref:Hilbert"&gt;&lt;a class="footnote-ref" href="#fn:Hilbert" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;H），使得在输入空间中的超平面模型对应于特征空间中的超平面模型（支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。&lt;/p&gt;
&lt;h3&gt;核函数的定义&lt;/h3&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(\chi\)&lt;/span&gt;是输入空间（欧氏空间&lt;span class="math"&gt;\(R^n\)&lt;/span&gt;的子集或离散集合），&lt;span class="math"&gt;\(H\)&lt;/span&gt;为特征空间（希尔伯特空间），如果存在一个从&lt;span class="math"&gt;\(\chi\)&lt;/span&gt;到&lt;span class="math"&gt;\(H\)&lt;/span&gt;的映射：
&lt;/p&gt;
&lt;div class="math"&gt;$$\phi(x): \chi \rightarrow H$$&lt;/div&gt;
&lt;p&gt;
使得所有&lt;span class="math"&gt;\(x,z \in \chi\)&lt;/span&gt;，函数K(x,z)满足条件：
&lt;/p&gt;
&lt;div class="math"&gt;$$K(x,z)=\phi(x).\phi(z)$$&lt;/div&gt;
&lt;p&gt;
则称&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;为&lt;strong&gt;核函数&lt;/strong&gt;，&lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;为映射函数。&lt;/p&gt;
&lt;p&gt;核技巧的想法是，在学习与预测中只定义核函数&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;，而不是显式地定义映射函数&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;。通常，直接计算&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;比较容易，而通过&lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(\phi(z)\)&lt;/span&gt;计算&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;不容易。对于给定的核&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;，特征空间和映射函数的取法并不唯一。&lt;/p&gt;
&lt;h3&gt;核技巧在SVM中的应用&lt;/h3&gt;
&lt;p&gt;我们注意到在&lt;a href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-xian-xing-zhi-chi-xiang-liang-ji.html"&gt;线性支持向量机&lt;/a&gt;的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha}L(w, b,  \xi, \alpha, \mu)=\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;p&gt;
因此，上式的目标函数中的内积&lt;span class="math"&gt;\(x_ix_j\)&lt;/span&gt;可以用核函数&lt;span class="math"&gt;\(K(x_i,x_j)=\phi(x).\phi(j)\)&lt;/span&gt;来代替，这时对偶问题的目标函数就变为：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha}L(w, b,  \xi, \alpha, \mu)=\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jK(x_ix_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;p&gt;
 同样，分类决策函数中的内积也可以用核函数代替：
 &lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(\sum_{i=1}^{N}\alpha_i^*y_iK(x_ix_j)+b^*)$$&lt;/div&gt;
&lt;p&gt;也就是说，在核函数&lt;span class="math"&gt;\(K(x_i,x_j)\)&lt;/span&gt;给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行得，不需要显示地定义特征空间和映射函数。这样的技巧就称为&lt;strong&gt;核技巧&lt;/strong&gt;。它是巧妙的利用线性分类学习方法与核函数解决非线性问题的技术。&lt;/p&gt;
&lt;p&gt;在实际应用中，选择核函数往往依赖于领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。&lt;/p&gt;
&lt;h2&gt;正定核&lt;/h2&gt;
&lt;p&gt;&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;满足什么条件才能称为核函数呢？&lt;/p&gt;
&lt;p&gt;通常，所说的核函数就是&lt;strong&gt;正定核函数（positive definite kernel function）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;依据函数&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;，构成一个希尔伯特空间，其步骤是：&lt;/p&gt;
&lt;h3&gt;（1）定义映射&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;，构成向量空间S&lt;/h3&gt;
&lt;p&gt;先定义映射
&lt;/p&gt;
&lt;div class="math"&gt;$$\phi :x \rightarrow K(.,x)$$&lt;/div&gt;
&lt;p&gt;
根据这一映射，定义线性组合
&lt;/p&gt;
&lt;div class="math"&gt;$$f(.)=\sum_{i=1}^{m}\alpha_iK(.,x_i)$$&lt;/div&gt;
&lt;p&gt;
考虑由线性组合为元素的集合S。&lt;/p&gt;
&lt;h3&gt;（2）在S上定义内积，使其成为内积空间&lt;/h3&gt;
&lt;p&gt;在S上定义一个运算&lt;span class="math"&gt;\(*\)&lt;/span&gt;，证明运算&lt;span class="math"&gt;\(*\)&lt;/span&gt;是S的内积，赋予内积的向量空间为内积空间。因此，S是一个内积空间&lt;sup id="fnref:内积空间"&gt;&lt;a class="footnote-ref" href="#fn:内积空间" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3&gt;（3）将内积空间S完备化为希尔伯特空间&lt;/h3&gt;
&lt;p&gt;这一希尔伯特空间称为再生核希尔伯特空间，这是由于核K具有再生性，即满足
&lt;/p&gt;
&lt;div class="math"&gt;$$K(.,x).f=f(x)$$&lt;/div&gt;
&lt;p&gt;
及
&lt;/p&gt;
&lt;div class="math"&gt;$$K(.,x).K(.,z)=K(x,z)$$&lt;/div&gt;
&lt;p&gt;
称为再生核。&lt;/p&gt;
&lt;h3&gt;（4）正定核的充要条件&lt;/h3&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(K: \chi \times \chi \rightarrow R\)&lt;/span&gt;是对称函数，则&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;为正定核的充要条件是对任意&lt;span class="math"&gt;\(x_i \in \chi, i=1,2,...,m\)&lt;/span&gt;，对&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;对应的Gram矩阵&lt;sup id="fnref:gram"&gt;&lt;a class="footnote-ref" href="#fn:gram" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$K=[K(x_i,x_j)]_{m \times m}$$&lt;/div&gt;
&lt;p&gt;
是半正定矩阵&lt;sup id="fnref:正定矩阵"&gt;&lt;a class="footnote-ref" href="#fn:正定矩阵" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;所以如果一个函数K是核函数，那么其对应的核矩阵是&lt;strong&gt;对称&lt;/strong&gt;的&lt;strong&gt;半正定矩阵&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;证明：&lt;/strong&gt;
必要性：
由于&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;为&lt;span class="math"&gt;\(\chi \times \chi\)&lt;/span&gt;上的正定核，所以存在从&lt;span class="math"&gt;\(\chi\)&lt;/span&gt;到希尔伯特空间的映射&lt;span class="math"&gt;\(\phi\)&lt;/span&gt;，使得
&lt;/p&gt;
&lt;div class="math"&gt;$$K(x,z)=\phi(x)\phi(z)$$&lt;/div&gt;
&lt;p&gt;
于是，对于任一&lt;span class="math"&gt;\(x_1,x_2,...,x_m\)&lt;/span&gt;，构造K(x,z)关于&lt;span class="math"&gt;\(x_1,x_2,...,x_m\)&lt;/span&gt;的Gram矩阵
&lt;/p&gt;
&lt;div class="math"&gt;$$[K_{ij}]_{m \times n}=[K(x_i,x_j)]_{m \times n}$$&lt;/div&gt;
&lt;p&gt;
对于任意的向量z，有
&lt;/p&gt;
&lt;div class="math"&gt;$$z^TKz=\sum_i \sum_j z_iK_{ij}z_j = \sum_i \sum_j z_i \phi(x_i)^T\phi(x_j)z_j = ... = \sum_k(\sum_iz_i\phi_i(x_i))^2 \geq 0$$&lt;/div&gt;
&lt;p&gt;
所以K(x,z)是半正定矩阵。&lt;/p&gt;
&lt;p&gt;充分性：
K(x,z)对任意&lt;span class="math"&gt;\(x_i \in \chi, i=1,2,...,m\)&lt;/span&gt;，关于&lt;span class="math"&gt;\(x_1,x_2,...,x_m\)&lt;/span&gt;的Gram矩阵是半正定的。对于给定的K(x,z)可以构造从&lt;span class="math"&gt;\(\chi\)&lt;/span&gt;到某个希尔伯特空间的映射
&lt;/p&gt;
&lt;div class="math"&gt;$$\phi :x \rightarrow K(.,x)$$&lt;/div&gt;
&lt;p&gt;
可得
&lt;/p&gt;
&lt;div class="math"&gt;$$K(.,x).f=f(x)$$&lt;/div&gt;
&lt;p&gt;
及
&lt;/p&gt;
&lt;div class="math"&gt;$$K(.,x).K(.,z)=K(x,z)$$&lt;/div&gt;
&lt;p&gt;
即得
&lt;/p&gt;
&lt;div class="math"&gt;$$K(x,z)=\phi(x)\phi(z)$$&lt;/div&gt;
&lt;p&gt;
表面K(x,z)是&lt;span class="math"&gt;\(\chi \times \chi\)&lt;/span&gt;上的核函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核函数的另一定义：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;是定义在&lt;span class="math"&gt;\(\chi \times \chi\)&lt;/span&gt; 上的对称函数，对任意&lt;span class="math"&gt;\(x_i \in \chi, i=1,2,...,m\)&lt;/span&gt;，&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;对应的Gram矩阵：
&lt;/p&gt;
&lt;div class="math"&gt;$$K=[K(x_i,x_j)]_{m \times m}$$&lt;/div&gt;
&lt;p&gt;
是半正定矩阵，则称&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;是正定核。&lt;/p&gt;
&lt;p&gt;对于一个具体函数&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;检验它是否是正定核函数不容易，因为要求对任意的输入集&lt;span class="math"&gt;\(x_i \in \chi, i=1,2,...,m\)&lt;/span&gt;，验证&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;对应的Gram矩阵是否为半定的。在实际问题中往往使用已有的核函数。&lt;/p&gt;
&lt;h3&gt;常用核函数&lt;/h3&gt;
&lt;p&gt;（1）多项式核函数
&lt;/p&gt;
&lt;div class="math"&gt;$$K(x,z)=(x.z+1)^p$$&lt;/div&gt;
&lt;p&gt;
分类决策函数成为：
 &lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(\sum_{i=1}^{N}\alpha_i^*y_i(x_i.z+1)^p+b^*)$$&lt;/div&gt;
&lt;p&gt;（2）高斯核函数
&lt;/p&gt;
&lt;div class="math"&gt;$$K(x,z)=exp(-\frac{||x-z||^2}{2\delta^2 })$$&lt;/div&gt;
&lt;p&gt;
对应的支持向量机是高斯径向基函数（radial basis function）分类器。
分类决策函数成为：
 &lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(\sum_{i=1}^{N}\alpha_i^*y_iexp(-\frac{||x-z||^2}{2\delta^2 })+b^*)$$&lt;/div&gt;
&lt;h3&gt;非线性支持向量机分类机&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;非线性支持向量机的学习算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：训练数据集T&lt;/p&gt;
&lt;p&gt;输出：分类决策函数&lt;/p&gt;
&lt;p&gt;（1）选取适当的核函数&lt;span class="math"&gt;\(K(x,z)\)&lt;/span&gt;和适当的参数C，构造并求解约束最优化问题
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha}\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jK(x_i,x_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. 　\sum_{i=1}^{N}\alpha_iy_i = 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$0 \leq \alpha_i \leq C 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
求得最优解&lt;span class="math"&gt;\(\alpha^* = (\alpha_1^* ... \alpha_n^*)^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（2）选择一个&lt;span class="math"&gt;\(0 &amp;lt; \alpha_j^* &amp;lt; C\)&lt;/span&gt;，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$b^* = y_j - \sum_{i=1}^{N}\alpha_i^*y_iK(x_i,x_j)$$&lt;/div&gt;
&lt;p&gt;（3）构造分类决策函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(\sum_{i=1}^{N}\alpha_i^*y_iK(x.x_i)+b^*)$$&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第七章&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:Hilbert"&gt;
&lt;p&gt;&lt;a href="http://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4"&gt;希尔伯特空间&lt;/a&gt;又叫完备的内积空间，是有限维欧几里得空间的一个推广，使之不局限于实的情形和有限的维数，但又不失完备性。在一个复数向量空间H上的给定的内积&amp;lt;.,.&amp;gt;可以按照如下的方式导出一个范数（norm）
&lt;img alt="hilbert" src="http://upload.wikimedia.org/math/b/8/8/b88c416b87ffb50ed8de63d046ffd015.png" /&gt;
此空间称为是一个希尔伯特空间&amp;#160;&lt;a class="footnote-backref" href="#fnref:Hilbert" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:内积空间"&gt;
&lt;p&gt;&lt;a href="http://zh.wikipedia.org/wiki/%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4"&gt;内积空间&lt;/a&gt;是增添了一个额外的结构的向量空间。这个额外的结构叫做内积或标量积。&amp;#160;&lt;a class="footnote-backref" href="#fnref:内积空间" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:gram"&gt;
&lt;p&gt;是指内积空间中的一组向量之间的内积构成的矩阵
&lt;img alt="gram" src="http://science.scileaf.com/library/math/8/6/86f4eefe06b11fd7d0a4aa7e77e686bb.png" /&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:gram" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:正定矩阵"&gt;
&lt;p&gt;一个n×n的实对称矩阵M是正定的，当且仅当对于所有的非零实系数向量z，都有&lt;span class="math"&gt;\(z^TMz &amp;gt; 0\)&lt;/span&gt;。
M是半正定矩阵当且仅当对所有不为零&lt;span class="math"&gt;\(的x \in \mathbb{R}^n\)&lt;/span&gt;（或
&lt;span class="math"&gt;\(x \in \mathbb{C}^n\)&lt;/span&gt;），都有：&lt;span class="math"&gt;\(x^{*} M x \geq 0\)&lt;/span&gt;
若 M 为半正定阵，可以写作&lt;span class="math"&gt;\(M \geq 0\)&lt;/span&gt; 。如果 M 是正定阵，可以写作 &lt;span class="math"&gt;\(M &amp;gt; 0\)&lt;/span&gt; 。&amp;#160;&lt;a class="footnote-backref" href="#fnref:正定矩阵" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>支持向量机——线性支持向量机</title><link href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-xian-xing-zhi-chi-xiang-liang-ji.html" rel="alternate"></link><updated>2014-08-03T14:54:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-03:zhi-chi-xiang-liang-ji-xian-xing-zhi-chi-xiang-liang-ji.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;对于理想情况下线性可分的问题，可以使用上文介绍的线性可分支持向量机（硬间隔最大化）完美解决。但是，实际情况中，训练数据往往是线性不可分的，即在样本中存在噪声或特异点。此时，可以使用本文中介绍的更一般的学习算法——线性支持向量机（软间隔最大化）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;线性支持向量机&lt;/h2&gt;
&lt;p&gt;怎样才能把&lt;a href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-xian-xing-ke-fen-zhi-chi-xiang-liang-ji.html"&gt;上文中介绍的线性可分支持向量机&lt;/a&gt;扩展到线性不可分的数据集中呢？这就需要改变硬间隔最大化，使其成为软间隔最大化。&lt;/p&gt;
&lt;p&gt;通常情况下，数据集中存在一些特异点（outlier），将这些特异点去除后，剩下大部分样本点组成的集合是线性可分的。那些线性不可分的样本点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;意味着不能满足函数间隔大于等于1的约束条件（即&lt;span class="math"&gt;\(y_i(w.x_i + b)-1 \geq 0\)&lt;/span&gt;）。为了解决这个问题，可以为每个样本点引入一个松弛变量&lt;span class="math"&gt;\(\xi_i \geq 0\)&lt;/span&gt;，使函数间隔加上松弛变量后大于等于1.同时对每个松弛变量&lt;span class="math"&gt;\(\xi_i\)&lt;/span&gt;，支付一个代价&lt;span class="math"&gt;\(\xi_i\)&lt;/span&gt;。这样线性不可分的线性支持向量机的学习问题变成如下的凸二次规划问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{w,b,\xi} \frac{1}{2}{||w||^2}+C\sum_{i=1}^{N}\xi_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$ s.t. 　(1)y_i(w.x_i+b) \geq 1 - \xi_i　　(2)\xi_i \geq 0 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;这里，&lt;span class="math"&gt;\(C&amp;gt;0\)&lt;/span&gt;称为惩罚参数，一般由应用问题决定，C值越大对误分类的惩罚越大。所以，这里的最小化目标包含两层含义：①间隔间隔尽量大 ②误分类个数尽量小。&lt;/p&gt;
&lt;p&gt;这样可以和训练数据集可分时一样来考虑训练数据集线性不可分时的线性可支持向量机学习问题。相应于硬间隔最大化，它称为&lt;strong&gt;软间隔最大化&lt;/strong&gt;，并称这样的模型为&lt;strong&gt;线性支持向量机&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;学习的对偶算法&lt;/h2&gt;
&lt;p&gt;（1）构建拉格朗日函数
&lt;/p&gt;
&lt;div class="math"&gt;$$L(w,b,\xi,\alpha,\mu)=\frac{1}{2}{||w||^2} +C\sum_{i=1}^{N}\xi_i- \sum_{i=1}^{N}\alpha_i(y_i(w.x_i+b)-1+\xi_i) - \sum_{i=1}^{N}\mu_i\xi_i$$&lt;/div&gt;
&lt;p&gt;
对每一个约束引进拉格朗日乘子&lt;span class="math"&gt;\(\alpha_i \geq 0, \mu_i \geq 0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;（2）根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题:
&lt;/p&gt;
&lt;div class="math"&gt;$$原始问题：min_{w,b,\xi}max_{\alpha_i \geq 0, \mu_i \geq 0}L(w, b, \xi, \alpha, \mu)$$&lt;/div&gt;
&lt;div class="math"&gt;$$对偶问题：max_{\alpha_i \geq 0, \mu_i \geq 0}min_{w,b,\xi}L(w, b,  \xi, \alpha, \mu)$$&lt;/div&gt;
&lt;p&gt;（3）求解对偶问题的解，需要先求&lt;span class="math"&gt;\(L(w, b,  \xi, \alpha, \mu)\)&lt;/span&gt;对&lt;span class="math"&gt;\(w,b,\xi\)&lt;/span&gt;的极小，再求对&lt;span class="math"&gt;\(\alpha, \mu\)&lt;/span&gt;的极大&lt;/p&gt;
&lt;p&gt;（4）先求&lt;span class="math"&gt;\(min_{w,b,\xi}L(w, b,  \xi, \alpha, \mu)\)&lt;/span&gt;
将拉格朗日函数分别对&lt;span class="math"&gt;\(w,b,\xi\)&lt;/span&gt;求偏导数并令偏导数等于0. 将求得的结果代入拉格朗日函数中可以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{w,b,\xi}L(w, b,  \xi, \alpha, \mu)=-\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)+ \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;p&gt;（5）求&lt;span class="math"&gt;\(min_{w,b,\xi}L(w, b,  \xi, \alpha, \mu)\)&lt;/span&gt;对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;的极大，即是对偶问题
&lt;/p&gt;
&lt;div class="math"&gt;$$max_{\alpha}L(w, b,  \xi, \alpha, \mu)=-\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)+ \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. 　\sum_{i=1}^{N}\alpha_iy_i = 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$　　C-\alpha_i-\mu_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha_i \geq 0 $$&lt;/div&gt;
&lt;div class="math"&gt;$$\mu_i \geq 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;转化下可以得到对偶问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha}L(w, b,  \xi, \alpha, \mu)=\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. 　\sum_{i=1}^{N}\alpha_iy_i = 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$0 \leq \alpha_i \leq C 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;得到&lt;strong&gt;线性支持向量机的学习算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：训练数据集T&lt;/p&gt;
&lt;p&gt;输出：分离超平面和分类决策函数&lt;/p&gt;
&lt;p&gt;（1）构造并求解约束最优化问题
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{\alpha}L(w, b,  \xi, \alpha, \mu)=\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. 　\sum_{i=1}^{N}\alpha_iy_i = 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$0 \leq \alpha_i \leq C 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
求得最优解&lt;span class="math"&gt;\(\alpha^* = (\alpha_1^* ... \alpha_n^*)^T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（2）计算
&lt;/p&gt;
&lt;div class="math"&gt;$$w^* = \sum_{i=1}^{N}\alpha_i^*y_ix_i$$&lt;/div&gt;
&lt;p&gt;
并选择一个&lt;span class="math"&gt;\(0 &amp;lt; \alpha_j^* &amp;lt; C\)&lt;/span&gt;，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$b^* = y_j - \sum_{i=1}^{N}\alpha_i^*y_i(x_ix_j)$$&lt;/div&gt;
&lt;p&gt;（3）求得分离超平面
&lt;/p&gt;
&lt;div class="math"&gt;$$w^*.x+b^*=0$$&lt;/div&gt;
&lt;p&gt;
分类决策函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(w^*.x+b^*)$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(w^*与b^*\)&lt;/span&gt;是由&lt;a href="http://book.douban.com/subject/10590856/"&gt;KKT条件&lt;/a&gt;得到的。即：&lt;/p&gt;
&lt;p&gt;原始问题是凸二次规划问题，解满足KKT条件，可得：
&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown_w L(w^*, b^*, \xi^*, \alpha^*, \mu^*)=w^*-\sum_{i=1}^{N}\alpha_i^*y_ix_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigtriangledown_b L(w^*, b^*, \xi^*, \alpha^*, \mu^*)=-\sum_{i=1}^{N}\alpha_i^*y_i=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigtriangledown_\xi L(w^*, b^*, \xi^*, \alpha^*, \mu^*)=C-\alpha_i^*-\mu^*=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha^*(y_i(w_i^*.x_i + b^*)-1+\xi_i^*) = 0 
$$&lt;/div&gt;
&lt;p&gt;u_i^&lt;em&gt;\xi_i^&lt;/em&gt; = 0 &lt;/p&gt;
&lt;div class="math"&gt;$$
$$&lt;/div&gt;
&lt;p&gt;y_i(w_i^&lt;em&gt;.x_i + b^&lt;/em&gt;)-1+\xi_i^&lt;em&gt; \geq 0&lt;div class="math"&gt;$$
$$&lt;/div&gt;\xi_i^&lt;/em&gt; \geq 0 &lt;/p&gt;
&lt;div class="math"&gt;$$
$$&lt;/div&gt;
&lt;p&gt;\alpha_i^&lt;em&gt; \geq 0 &lt;div class="math"&gt;$$
$$&lt;/div&gt;\mu_i^&lt;/em&gt; \geq 0 $$&lt;/p&gt;
&lt;p&gt;w的解是唯一的，但是b的解不唯一。对于任一适合条件&lt;span class="math"&gt;\(0 &amp;lt; \alpha_j^* &amp;lt; C\)&lt;/span&gt;，都可以求出&lt;span class="math"&gt;\(b^*\)&lt;/span&gt;，所以实际计算时可以取在所有符合条件的样本点上的平均值。&lt;/p&gt;
&lt;h2&gt;支持向量&lt;/h2&gt;
&lt;p&gt;我们可以将&lt;span class="math"&gt;\(\alpha_i^* &amp;gt; 0\)&lt;/span&gt;的样本点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;的实例&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;称为&lt;strong&gt;支持向量&lt;/strong&gt;（软间隔的支持向量）。但是，这时的支持向量要比上文中介绍的复杂。&lt;/p&gt;
&lt;p&gt;实例&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;到间隔边界的距离为&lt;span class="math"&gt;\(\frac{\xi_i}{||w||}\)&lt;/span&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(\alpha_i^* &amp;lt; C\)&lt;/span&gt;，则&lt;span class="math"&gt;\(\xi_i = 0\)&lt;/span&gt;&lt;sup id="fnref:关系"&gt;&lt;a class="footnote-ref" href="#fn:关系" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;，支持向量&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;恰好落在间隔边界上；&lt;/li&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(\alpha_i^* = C\)&lt;/span&gt;，&lt;span class="math"&gt;\(0&amp;lt; \xi_i &amp;lt; 1\)&lt;/span&gt;，支持向量&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;落在间隔边界与超平面之间，分类正确；&lt;/li&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(\alpha_i^* &amp;gt; C\)&lt;/span&gt;，&lt;span class="math"&gt;\(\xi_i =1\)&lt;/span&gt;，支持向量&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;恰好在超平面上；&lt;/li&gt;
&lt;li&gt;若 &lt;span class="math"&gt;\(\alpha_i^* &amp;gt; C\)&lt;/span&gt;，&lt;span class="math"&gt;\(\xi_i &amp;gt;1\)&lt;/span&gt;，支持向量&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;位于分离超平面误分一侧。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;合页损失函数&lt;/h2&gt;
&lt;p&gt;线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{i=1}^{N}[1-y_i(w.x_i+b)]_++\lambda ||w||^2$$&lt;/div&gt;
&lt;p&gt;
目标函数中的第一项是经验损失或经验风险，函数
&lt;/p&gt;
&lt;div class="math"&gt;$$L(y(w.x+b))=[1-y_i(w.x_i+b)]_+$$&lt;/div&gt;
&lt;p&gt;
称为合页损失函数（hinge loss function）。下标"+"表示以下取正值的函数
&lt;/p&gt;
&lt;div class="math"&gt;$$[z]_+=\left\{\begin{matrix}
z, z &amp;gt; 0\\ 
0, z \leq 0
\end{matrix}\right.$$&lt;/div&gt;
&lt;p&gt;
表示样本点被正确分类（z &amp;gt; 0），损失是0；错误分类损失为&lt;span class="math"&gt;\(1−y_i(w.x_i+b)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;合页损失函数的图形，以函数间隔&lt;span class="math"&gt;\(y(w.x+b)\)&lt;/span&gt;为横轴，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。与感知机的损失函数相比，相当于右移1长度，所以合页损失函数对学习要求更高，不仅要分类正确，而且确信度足够高时损失才是0（大于1的部分）。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第七章&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:关系"&gt;
&lt;p&gt;&lt;span class="math"&gt;\(C-\alpha_i-\mu_i=0\)&lt;/span&gt;并且&lt;span class="math"&gt;\(\mu_i^* \xi_i^*=0（从KTT条件可得）\)&lt;/span&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:关系" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>支持向量机——线性可分支持向量机</title><link href="http://www.wengweitao.com/zhi-chi-xiang-liang-ji-xian-xing-ke-fen-zhi-chi-xiang-liang-ji.html" rel="alternate"></link><updated>2014-08-02T20:32:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-02:zhi-chi-xiang-liang-ji-xian-xing-ke-fen-zhi-chi-xiang-liang-ji.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;支持向量机（support vector machine， SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使之有别与感知机。支持向量机还包括核技巧，这使它成为实质上的非线性分类器。SVM的学习策略是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的学习算法是求解凸二次规划的最优算法。SVM学习方法包含构建由简至烦的模型：
- 线性可分支持向量机：当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性分类器
- 线性支持向量机：当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），学习一个线性分类器，也称为软间隔支持向量机
- 非线性支持向量机：当训练数据线性不可分时，通过使用核技巧（kernel trick）即软间隔最大化，学习非线性支持向量机。
核函数（kernel function）表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。这样的方法称为核技巧。核方法是比支持向量机更为一般的机器学习方法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;线性可分支持向量机与硬间隔最大化&lt;/h2&gt;
&lt;h3&gt;线性可分支持向量机&lt;/h3&gt;
&lt;p&gt;支持向量机中，输入都由输入空间转换到特征空间将输入映射为特征向量，支持向量机的学习是在特征空间中进行的。&lt;/p&gt;
&lt;p&gt;假设训练数据是线性可分的，学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程&lt;span class="math"&gt;\(w.x+b=0\)&lt;/span&gt;，它由法向量w和截距b决定，可用(w,b)来表示。分离超平面将特征空间划分为两部分，一部分为正类，一部分为负类。法向量执行的一侧为正类，另一侧为负类。&lt;/p&gt;
&lt;p&gt;一般地，当训练数据集线性可分时，存在无穷个超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。&lt;/p&gt;
&lt;p&gt;给定线性可分训练数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为
&lt;/p&gt;
&lt;div class="math"&gt;$$w^*.x+b^*=0$$&lt;/div&gt;
&lt;p&gt;
以及相应的分类决策函数
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(w^*.x+b^*)$$&lt;/div&gt;
&lt;p&gt;
称为&lt;strong&gt;线性可分支持向量机&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线。&lt;/p&gt;
&lt;h3&gt;函数间隔和几何间隔&lt;/h3&gt;
&lt;p&gt;一个点距离超平面的远近可以表示分类预测的确信程度（越远就越可信）。在超平面程&lt;span class="math"&gt;\(w.x+b=0\)&lt;/span&gt;确定的情况下，程&lt;span class="math"&gt;\(|w.x+b|\)&lt;/span&gt;能够相对地表示点x距离超平面的远近。而&lt;span class="math"&gt;\(w.x+b\)&lt;/span&gt;的符号与类标记y的符号是否一致能够表示分类是否正确。所以可以用&lt;span class="math"&gt;\(y(w.x+b)\)&lt;/span&gt;来表示分类的正确性及确信度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;函数间隔：&lt;/strong&gt; 定义超平面(w,b)关于样本点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;的函数间隔为
&lt;/p&gt;
&lt;div class="math"&gt;$$\widehat{\gamma_i}=y_i(w.x_i+b)  $$&lt;/div&gt;
&lt;p&gt;
定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;的函数间隔之最小值。&lt;/p&gt;
&lt;p&gt;但是，对于函数间隔只要成比例地改变w和b，例如变为2w和2b，超平面并没有改变，但是函数间隔却成为原来的2倍。可以对分离超平面的法向量w加某些约束，如规范化，||w||=1，使得间隔是确定的。这时函数间隔成为几何间隔（geometric margin）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;几何间隔：&lt;/strong&gt; 定义超平面(w,b)关于样本点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;的几何间隔为
&lt;/p&gt;
&lt;div class="math"&gt;$$\gamma_i=y_i(\frac{w}{||w||}.x_i+\frac{b}{||w||})  $$&lt;/div&gt;
&lt;p&gt;
定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;的几何间隔之最小值。&lt;/p&gt;
&lt;p&gt;如果||w||=1，函数间隔和几何间隔相等。如果超平面参数w和b成比例地改变，函数间隔也按比例改变，而几何间隔不变。&lt;/p&gt;
&lt;h3&gt;间隔最大化&lt;/h3&gt;
&lt;p&gt;支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。这里的集合间隔最大的分离超平面是唯一的，这里的间隔最大化又称为&lt;strong&gt;硬间隔最大化&lt;/strong&gt;。间隔最大化最直观的解释就是，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。&lt;/p&gt;
&lt;h4&gt;1.最大间隔分离超平面&lt;/h4&gt;
&lt;p&gt;最大间隔分离超平面这个问题可以表示为下面的约束最优化问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$max_{w,b} \gamma$$&lt;/div&gt;
&lt;div class="math"&gt;$$ s.t. 　y_i(\frac{w}{||w||}.x_i+\frac{b}{||w||}) \geq \gamma 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
约束条件表示为超平面(w,b)关于每个训练样本点的几何间隔至少是&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;。考虑几何间隔和函数间隔的关系式：
&lt;/p&gt;
&lt;div class="math"&gt;$$\gamma_i = \frac{\widehat{\gamma_i}}{||w||}$$&lt;/div&gt;
&lt;p&gt;
可以将这个问题改写为：
&lt;/p&gt;
&lt;div class="math"&gt;$$max_{w,b} \frac{\widehat{\gamma_i}}{||w||}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ s.t. 　y_i(w.x_i+b) \geq \gamma 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;函数间隔&lt;/strong&gt;的取值&lt;span class="math"&gt;\(\widehat{\gamma_i}\)&lt;/span&gt;的取值并不影响最优化问题的解。可以将w和b成比例的改变，对上面最优化问题的不等式约束没有影响。也就是说，它产生一个等价的最优化问题。这样，就可以取&lt;span class="math"&gt;\(\widehat{\gamma_i}=1\)&lt;/span&gt;。将&lt;span class="math"&gt;\(\widehat{\gamma_i}=1\)&lt;/span&gt;代入上面的最优化问题，&lt;strong&gt;可以得到线性可分支持向量机学习的最优化问题&lt;/strong&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{w,b} \frac{1}{2}{||w||^2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ s.t. 　y_i(w.x_i+b) - 1\geq 0 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
这是一个凸二次规划&lt;sup id="fnref:convex"&gt;&lt;a class="footnote-ref" href="#fn:convex" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;（convex quadratic programming）问题。
如果解出了约束最优化问题的&lt;span class="math"&gt;\(w^*, b^*\)&lt;/span&gt;那么就可以得到最大间隔分离超平面&lt;span class="math"&gt;\(w^*.x + b^*=0\)&lt;/span&gt;及分类决策函数，也就是线性可分支持向量机模型。&lt;/p&gt;
&lt;h4&gt;最大间隔分类超平面的存在唯一性&lt;/h4&gt;
&lt;p&gt;线性可分训练数据集的最大间隔分离超平面是&lt;strong&gt;存在且唯一的&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;支持向量和间隔边界&lt;/h4&gt;
&lt;p&gt;训练数据集的样本点中与分离超平面距离最近的样本点的实例称为&lt;strong&gt;支持向量（support vector）&lt;/strong&gt;。支持向量是使约束条件等号成立的点，即
&lt;/p&gt;
&lt;div class="math"&gt;$$y_i(w.x_i+b) - 1= 0 $$&lt;/div&gt;
&lt;p&gt;
对&lt;span class="math"&gt;\(y_i=+1\)&lt;/span&gt;的正例点，支持向量在超平面：
&lt;/p&gt;
&lt;div class="math"&gt;$$H_1: w.x_i+b = 1 $$&lt;/div&gt;
&lt;p&gt;
上，对&lt;span class="math"&gt;\(y_i=-1\)&lt;/span&gt;的负例点，支持向量在超平面：
&lt;/p&gt;
&lt;div class="math"&gt;$$H_2: w.x_i+b = -1 $$&lt;/div&gt;
&lt;p&gt;
上。&lt;span class="math"&gt;\(H_1\)&lt;/span&gt;与&lt;span class="math"&gt;\(H_2\)&lt;/span&gt;上的点就是支持向量。&lt;span class="math"&gt;\(H_1\)&lt;/span&gt;与&lt;span class="math"&gt;\(H_2\)&lt;/span&gt;是平行的，它们之间的距离称为&lt;strong&gt;间隔（margin）&lt;/strong&gt;。间隔依赖于超平面的法向量w，等于&lt;span class="math"&gt;\(\frac{2}{||w||}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在决定分离超平面时只有支持向量其作用，而其他实例点并不起作用，所以将这种分类模型称为支持向量机。支持向量的个数一般很少，所以支持向量机由很少的“重要”训练样本确定。&lt;/p&gt;
&lt;h3&gt;学习的对偶算法&lt;/h3&gt;
&lt;p&gt;为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性。通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。这样做的优点，一是对偶问题往往更容易求解；而是自然引入核函数，进而推广到非线性分类问题。&lt;/p&gt;
&lt;p&gt;线性可分支持向量机学习的最优化问题为：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{w,b} \frac{1}{2}{||w||^2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ s.t. 　y_i(w.x_i+b) - 1\geq 0 　i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;（1）构建拉格朗日函数
&lt;/p&gt;
&lt;div class="math"&gt;$$L(w,b,\alpha)=\frac{1}{2}{||w||^2} - \sum_{i=1}^{N}\alpha_iy_i(w.x_i+b)+ \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;p&gt;
对每一个约束引进拉格朗日乘子&lt;span class="math"&gt;\(\alpha_i \geq 0\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;（2）根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题:
&lt;/p&gt;
&lt;div class="math"&gt;$$原始问题：min_{w,b}max_{\alpha_i \geq 0}L(w, b, \alpha)$$&lt;/div&gt;
&lt;div class="math"&gt;$$对偶问题：max_{\alpha_i \geq 0}min_{w,b}L(w, b, \alpha)$$&lt;/div&gt;
&lt;p&gt;（3）求解对偶问题的解，需要先求&lt;span class="math"&gt;\(L(w, b, \alpha)\)&lt;/span&gt;对w,b的极小，再求对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;的极大&lt;/p&gt;
&lt;p&gt;（4）先求&lt;span class="math"&gt;\(min_{w,b}L(w, b, \alpha)\)&lt;/span&gt;
将拉格朗日函数分别对w,b求偏导数并令偏导数等于0. 将求得的结果代入拉格朗日函数中可以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{w,b}L(w,b,\alpha)=-\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)+ \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;p&gt;（5）求&lt;span class="math"&gt;\(min_{w,b}L(w, b, \alpha)\)&lt;/span&gt;对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;的极大，即是对偶问题
&lt;/p&gt;
&lt;div class="math"&gt;$$max_{\alpha}L(w,b,\alpha)=-\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)+ \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. \sum_{i=1}^{N}\alpha_iy_i = 0　\alpha_i \geq 0, i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;可以将上面的目标函数由极大转换为求极小，得到&lt;strong&gt;线性可分支持向量机的学习算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：线性可分训练数据集T&lt;/p&gt;
&lt;p&gt;输出：分离超平面和分类决策函数&lt;/p&gt;
&lt;p&gt;（1）构造并求解约束最优化问题
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{w,b}L(w,b,\alpha)=\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_ix_j)- \sum_{i=1}^{N}\alpha_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. \sum_{i=1}^{N}\alpha_iy_i = 0　\alpha_i \geq 0, i=1,2,...,N$$&lt;/div&gt;
&lt;p&gt;
求得最优解&lt;span class="math"&gt;\(\alpha^* = (\alpha_1^* ... \alpha_n^*)^T\)&lt;/span&gt;（必须满足&lt;span class="math"&gt;\(\alpha_i \geq 0\)&lt;/span&gt;，如果找不到则最小值在边界上）.&lt;/p&gt;
&lt;p&gt;（2）计算
&lt;/p&gt;
&lt;div class="math"&gt;$$w^* = \sum_{i=1}^{N}\alpha_i^*y_ix_i$$&lt;/div&gt;
&lt;p&gt;
并选择一个&lt;span class="math"&gt;\(\alpha_j^* &amp;gt; 0\)&lt;/span&gt;，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$b^* = y_j - \sum_{i=1}^{N}\alpha_i^*y_i(x_ix_j)$$&lt;/div&gt;
&lt;p&gt;（3）求得分离超平面
&lt;/p&gt;
&lt;div class="math"&gt;$$w^*.x+b^*=0$$&lt;/div&gt;
&lt;p&gt;
分类决策函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(w^*.x+b^*)$$&lt;/div&gt;
&lt;p&gt;其中，&lt;span class="math"&gt;\(w^*与b^*\)&lt;/span&gt;是由&lt;a href="http://www.wengweitao.com/la-ge-lang-ri-dui-ou-xing-lagrange-duality.html"&gt;KKT条件&lt;/a&gt;得到的。&lt;span class="math"&gt;\(w^*与b^*\)&lt;/span&gt;只依赖于训练数据中对应于&lt;span class="math"&gt;\(\alpha_i &amp;gt; 0\)&lt;/span&gt;的样本点，而其他样本点对它们没有影响，因此，我们也可以将&lt;span class="math"&gt;\(\alpha_i &amp;gt; 0\)&lt;/span&gt;的样本点称为&lt;strong&gt;支持向量&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第七章&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:convex"&gt;
&lt;p&gt;凸优化问题是指约束最优化问题
&lt;div class="math"&gt;$$min_x f(x)$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. g_i(x) \leq0　h_i(x) \leq 0　i=1,2,...,N$$&lt;/div&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:convex" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>逻辑斯谛回归与最大熵模型</title><link href="http://www.wengweitao.com/luo-ji-si-di-hui-gui-yu-zui-da-shang-mo-xing.html" rel="alternate"></link><updated>2014-08-01T09:53:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-08-01:luo-ji-si-di-hui-gui-yu-zui-da-shang-mo-xing.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;逻辑斯谛回归（logistic regression）是统计学习中的经典分类方法，可以用于二类分类也可以用于多类分类。最大熵模型由最大熵原理推导出来，最大熵原理是概率模型学习或估计的一个准则，最大熵原理认为在所有可能的概率模型的集合中，熵最大的模型是最好的模型，最大熵模型也可以用于二类分类和多类分类。逻辑斯谛回归模型与最大熵模型都属于对数线性模型&lt;sup id="fnref:log"&gt;&lt;a class="footnote-ref" href="#fn:log" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;逻辑斯谛回归模型&lt;/h2&gt;
&lt;h3&gt;逻辑斯谛分布&lt;/h3&gt;
&lt;p&gt;设X是连续随机变量，X服从逻辑斯谛分布是指X具有如下分布函数和密度函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$F(X)=P(X \leq x)=\frac{1}{1+e^{-(x-u)/\gamma}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$f(x)=F'(x)=\frac{e^{-(x-u)/\gamma}}{\gamma(1+e^{-(x-u)/\gamma})^2}$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(\mu\)&lt;/span&gt;为位置参数，&lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;为形状参数。
&lt;strong&gt;分布函数的图形是一条S形曲线&lt;/strong&gt;（sigmoid curve），是中心对称的，曲线在中心点附近增长的较快，在两端增长的较慢。&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;的值越小，曲线在中心附近增长的越快。&lt;/p&gt;
&lt;h3&gt;逻辑斯谛回归模型&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;二项逻辑斯谛回归模型&lt;/strong&gt;是如下的条件概率分布：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=1|x)=\frac{exp(w.x+b)}{1+exp(w.x+b)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(Y=0|x)=\frac{1}{1+exp(w.x+b)}$$&lt;/div&gt;
&lt;p&gt;
随机变量X取值为实数，输出Y是0或1，w称为权值向量，b称为偏置。二项逻辑斯谛回归模型是一种分类模型，由条件概率P(Y|X)表示，形式为参数化的逻辑斯谛分布。&lt;/p&gt;
&lt;p&gt;有时为了方便，将权值向量和输入向量加以扩展，将偏置放入权值向量中，输入向量也增加一个值为1的分量，这时逻辑斯谛回归模型可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=1|x)=\frac{exp(w.x)}{1+exp(w.x)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(Y=0|x)=\frac{1}{1+exp(w.x)}$$&lt;/div&gt;
&lt;p&gt;对逻辑斯谛回归而言，x输入分类为Y=1的对数几率&lt;sup id="fnref:odds"&gt;&lt;a class="footnote-ref" href="#fn:odds" rel="footnote"&gt;2&lt;/a&gt;&lt;/sup&gt;或logit函数是：
&lt;/p&gt;
&lt;div class="math"&gt;$$log\frac{P(Y=1|X)}{P(Y=0|X)}=w.x$$&lt;/div&gt;
&lt;p&gt;
也就是说再逻辑斯谛回归模型中，输出Y=1的对数几率是输入x的线性函数。或者&lt;strong&gt;说Y=1的对数几率是由输入x的线性函数&lt;span class="math"&gt;\(w.x\)&lt;/span&gt;表示的模型，即逻辑斯谛回归模型&lt;/strong&gt;。
线性函数越接近正无穷，概率值就越接近1；越接近负无穷，概率值就越接近0.这样的模型就是逻辑斯谛回归模型。&lt;/p&gt;
&lt;p&gt;而在使用线性回归进行二类分类的时候：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=1|x)= w_0 + w_1x_1 + ... + w_nx_n$$&lt;/div&gt;
&lt;p&gt;
存在两个问题：&lt;/p&gt;
&lt;p&gt;（1）等式两边的取值范围不同。左边为概率是[0, 1]，右边是无穷&lt;/p&gt;
&lt;p&gt;（2）实际很多问题中，概率P和输入并非简单的线性关系，在x很小或很大的时候，可能对于P的影响很小，而x在某些值附近可能对P的影响很大。&lt;/p&gt;
&lt;p&gt;逻辑斯谛回归模型对线性回归模型进行了修正，解决了以上的2个问题。&lt;/p&gt;
&lt;h3&gt;模型参数估计&lt;/h3&gt;
&lt;p&gt;我们通过监督学习的方法来估计模型参数。对于给定的训练数据集，可以运用极大似然法估计模型参数，从而得到逻辑斯谛回归模型。&lt;/p&gt;
&lt;p&gt;假设
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=1|X)=\pi(x)$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(Y=0|X)=1 - \pi(x)$$&lt;/div&gt;
&lt;p&gt;
似然函数可以表示为
&lt;/p&gt;
&lt;div class="math"&gt;$$\prod_{i=1}^{N}[\pi(x_i)]^{y_i}[1 - \pi(x_i)]^{1-y_i}$$&lt;/div&gt;
&lt;p&gt;
对数似然函数表示为&lt;span class="math"&gt;\(L(w)\)&lt;/span&gt;，&lt;strong&gt;对&lt;span class="math"&gt;\(L(w)\)&lt;/span&gt;求最大值，得到w的估计值&lt;/strong&gt;。得到了
w就可以学到逻辑斯谛回归模型。&lt;/p&gt;
&lt;p&gt;这样问题就变成了以对数似然函数为目标的最优化问题。逻辑斯谛回归学习中常用的方法是梯度下降&lt;sup id="fnref:gradient descent"&gt;&lt;a class="footnote-ref" href="#fn:gradient descent" rel="footnote"&gt;3&lt;/a&gt;&lt;/sup&gt;及拟牛顿法&lt;sup id="fnref:newton method"&gt;&lt;a class="footnote-ref" href="#fn:newton method" rel="footnote"&gt;4&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3&gt;多项逻辑斯谛回归&lt;/h3&gt;
&lt;p&gt;上面介绍的是二类分类，也可以将逻辑斯谛回归模型推广到多类分类。假设离散型随机变量Y的取值集合是{1,2,...,K}，那么多项逻辑斯谛回归模型是
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=k|x)=\frac{exp(w_k.x)}{1+\sum_{k=1}^{K-1}exp(w_k.x)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(w_k.x)}$$&lt;/div&gt;
&lt;hr /&gt;
&lt;h2&gt;最大熵模型&lt;/h2&gt;
&lt;h3&gt;最大熵原理&lt;/h3&gt;
&lt;p&gt;最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合。最大熵原理认为选择的概率模型首先必须满足已有的事实，即约束条件。所以最大熵原理可以表示为：&lt;strong&gt;在满足约束条件的模型集合中选取熵最大的模型。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在没有更多信息的情况下，可以按照满足约束条件下求等概率的方法估计概率分布（因为均匀分布，熵最大）。&lt;/p&gt;
&lt;h3&gt;最大熵模型的定义&lt;/h3&gt;
&lt;p&gt;将最大熵原理应用到分类就得到最大熵模型。&lt;/p&gt;
&lt;p&gt;给定训练数据集可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布&lt;span class="math"&gt;\(\widetilde{P}(X,Y)和\widetilde{P}(X)\)&lt;/span&gt;。用特征函数f(x,y)描述输入x和输出y之间的某一个事实，定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x,y)=\left\{\begin{matrix}
1,x与y满足某一事实\\ 
0,不满足某一事实
\end{matrix}\right.$$&lt;/div&gt;
&lt;p&gt;特征函数f(x,y)&lt;strong&gt;关于经验分布&lt;span class="math"&gt;\(\widetilde{P}(X,Y)\)&lt;/span&gt;的期望值&lt;/strong&gt;，用&lt;span class="math"&gt;\(E_\widetilde{P}(f)\)&lt;/span&gt;表示：
&lt;/p&gt;
&lt;div class="math"&gt;$$E_\widetilde{P}(f)=\sum_{x,y}\widetilde{P}(x,y)f(x,y)$$&lt;/div&gt;
&lt;p&gt;特征函数f(x,y)&lt;strong&gt;关于模型P(Y|X)与经验分布&lt;span class="math"&gt;\(\widetilde{P}(X)\)&lt;/span&gt;的期望值&lt;/strong&gt;，用&lt;span class="math"&gt;\(E_{P}(f)\)&lt;/span&gt;表示：
 &lt;/p&gt;
&lt;div class="math"&gt;$$E_p(f)=\sum_{x,y}\widetilde{P}(X)P(y|x)f(x,y)$$&lt;/div&gt;
&lt;p&gt;如果模型可以获取训练数据中的信息，那么可以假设以上两个期望值相等：
 &lt;/p&gt;
&lt;div class="math"&gt;$$E_\widetilde{P}(f)=E_p(f)$$&lt;/div&gt;
&lt;p&gt;
 或
 &lt;/p&gt;
&lt;div class="math"&gt;$$\sum_{x,y}\widetilde{P}(x,y)f(x,y)=\sum_{x,y}\widetilde{P}(X)P(y|x)f(x,y)$$&lt;/div&gt;
&lt;p&gt;
以上两式作为&lt;strong&gt;模型学习的约束条件&lt;/strong&gt;。假如有n个特征函数&lt;span class="math"&gt;\(f_i(x,y)， i=1,2,...,n\)&lt;/span&gt;那么就有&lt;em&gt;n个约束条件&lt;/em&gt;。&lt;/p&gt;
&lt;h3&gt;最大熵模型的学习&lt;/h3&gt;
&lt;p&gt;最大熵模型的学习可以形式化为约束最优化问题。&lt;/p&gt;
&lt;p&gt;给定训练数据集T已经特征函数&lt;span class="math"&gt;\(f_i(x,y)， i=1,2,...,n\)&lt;/span&gt;，最大熵模型的学习等价于约束最优化问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$max_{P \in C} H(P)=-\sum_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. E_\widetilde{P}(f)=E_p(f)， i=1,2,...,n　并且　\sum_yP(y|x)=1$$&lt;/div&gt;
&lt;p&gt;
按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题：
&lt;/p&gt;
&lt;div class="math"&gt;$$min_{P \in C} -H(P)=\sum_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. E_\widetilde{P}(f)-E_p(f)=0， i=1,2,...,n　并且　\sum_yP(y|x)=1$$&lt;/div&gt;
&lt;p&gt;
求解以上最优化问题，所得出的解，就是最大熵模型学习的解。&lt;/p&gt;
&lt;p&gt;这里，将约束最优化问题的原始问题转换为无约束最优化的对偶问题，通过求解对偶问题求解原始问题。&lt;/p&gt;
&lt;p&gt;（1）引入拉格朗日乘子，定义拉格朗日函数&lt;span class="math"&gt;\(L(P,w)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;（2）首先求解&lt;span class="math"&gt;\(L(P,w)\)&lt;/span&gt;关于P的极小化问题，固定拉格朗日乘子&lt;span class="math"&gt;\(w_0,w_1,...,w_n\)&lt;/span&gt;，对P求偏导数&lt;/p&gt;
&lt;p&gt;（3）另各偏导数等于0，解出各个P，得到&lt;span class="math"&gt;\(min_PL(P,w)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（4）求解&lt;span class="math"&gt;\(L(P_w,w)\)&lt;/span&gt;关于w的极大化问题&lt;/p&gt;
&lt;p&gt;（5）令&lt;span class="math"&gt;\(L(P_w,w)\)&lt;/span&gt;对w的各偏导数为0&lt;/p&gt;
&lt;p&gt;（6）得到最大熵模型&lt;/p&gt;
&lt;p&gt;可以求解得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$P_w(y|x)=\frac{1}{Z_w(x)}exp(\sum_{i=1}^{n}w_if_i(x,y))$$&lt;/div&gt;
&lt;p&gt;
其中
&lt;/p&gt;
&lt;div class="math"&gt;$$Z_w(x)=\sum_{y}exp(\sum_{i=1}^{n}w_if_i(x,y))$$&lt;/div&gt;
&lt;p&gt;
&lt;span class="math"&gt;\(Z_w(x)\)&lt;/span&gt;称为&lt;strong&gt;规范化因子&lt;/strong&gt;；&lt;span class="math"&gt;\(f_i(x,y)\)&lt;/span&gt;是特征函数；&lt;span class="math"&gt;\(w_i\)&lt;/span&gt;是特征的权值。有上式表示的模型&lt;span class="math"&gt;\(P_w=P_w(y|x)\)&lt;/span&gt;就是最大熵模型。最后，最大熵模型的学习归结为对偶函数的极大化。&lt;/p&gt;
&lt;h3&gt;极大似然函数&lt;/h3&gt;
&lt;p&gt;可以证明对偶函数的极大化等价于最大熵模型的极大似然估计。这样最大熵模型的学习问题就转化为具体求解对数似然函数极大化或对偶函数极大化问题。&lt;/p&gt;
&lt;p&gt;最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为&lt;strong&gt;对数线性模型(log linear model)&lt;/strong&gt;。模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;模型学习的最优化选择&lt;/h2&gt;
&lt;p&gt;最大熵模型与逻辑斯谛回归模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。这时的目标函数是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。牛顿法或拟牛顿法一般收敛速度更快。&lt;/p&gt;
&lt;h3&gt;改进的迭代尺度法&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;改进的迭代尺度法（improved iterative scaling, IIS）&lt;/strong&gt;是一种最大熵模型学习的最优化算法。IIS的想法是：假设最大熵模型的当前的参数向量是&lt;span class="math"&gt;\(w\)&lt;/span&gt;，我们希望找到一个新的参数向量&lt;span class="math"&gt;\(w=w+\delta\)&lt;/span&gt;，使得模型的对数似然函数值增大。如果能有一种参数向量的更新方法&lt;span class="math"&gt;\(w \rightarrow w+\delta\)&lt;/span&gt;，那么就可以重复使用这一方法，直到找到对数似然函数的最大值。&lt;/p&gt;
&lt;h3&gt;拟牛顿法&lt;/h3&gt;
&lt;p&gt;最大熵模型还可以使用牛顿法或拟牛顿法。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第六章&lt;/p&gt;
&lt;p&gt;http://blog.csdn.net/lilyth_lilyth/article/details/10032993&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:log"&gt;
&lt;p&gt;对数线性模型是对线性模型的一个变形，使用原始数据的对数建模。&amp;#160;&lt;a class="footnote-backref" href="#fnref:log" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:odds"&gt;
&lt;p&gt;一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。&amp;#160;&lt;a class="footnote-backref" href="#fnref:odds" rev="footnote" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:gradient descent"&gt;
&lt;p&gt;参考我之前的另一篇文章&lt;a href="http://www.wengweitao.com/ti-du-xia-jiang-fa.html"&gt;梯度下降法&lt;/a&gt; &amp;#160;&lt;a class="footnote-backref" href="#fnref:gradient descent" rev="footnote" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:newton method"&gt;
&lt;p&gt;参考我之前的另一篇文章&lt;a href="http://www.wengweitao.com/niu-dun-fa.html"&gt;牛顿法&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:newton method" rev="footnote" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>牛顿法</title><link href="http://www.wengweitao.com/niu-dun-fa.html" rel="alternate"></link><updated>2014-07-30T20:12:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-30:niu-dun-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;牛顿法是近似求解方程的方法，方法是使用函数的泰勒级数的前面几项来寻找方程的根。在统计学习中，牛顿法（Newton method）和拟牛顿法（quasi Newton method）也是&lt;strong&gt;求解无约束最优化问题&lt;/strong&gt;的常用方法。有&lt;strong&gt;收敛速度快&lt;/strong&gt;的优点。牛顿法是&lt;strong&gt;迭代方法&lt;/strong&gt;，每一步需要求解目标函数的海森矩阵（Hessian matrix）的逆矩阵，计算比较复杂。拟牛顿法通过正定矩阵近似海赛矩阵的逆矩阵或海赛矩阵，简化了这一过程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;牛顿法&lt;/h2&gt;
&lt;h3&gt;目标&lt;/h3&gt;
&lt;p&gt;考虑无约束最优化问题
&lt;/p&gt;
&lt;div class="math"&gt;$$minf(x)$$&lt;/div&gt;
&lt;p&gt;其中&lt;span class="math"&gt;\(x^* $为目标函数的极小点。求出f(x)的极小点 $x^*\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;基本概念&lt;/h3&gt;
&lt;h4&gt;泰勒级数&lt;/h4&gt;
&lt;p&gt;若函数f（x）在点的某一临域内具有直到（n+1）阶导数，则在该邻域内f（x）的n阶泰勒公式为
用无限项的级数连加式来表示一个函数，这些相加的项由函数在某一点的导数求得。在该点邻域内f（x）的n阶泰勒公式表示为&lt;img alt="泰勒级数" src="http://upload.wikimedia.org/math/0/2/8/02837e50dddb76c237328172e2040135.png" /&gt;。
对于二阶泰勒展开，二阶的项可以使用海森矩阵来表示。&lt;/p&gt;
&lt;h4&gt;海森矩阵&lt;/h4&gt;
&lt;p&gt;海森矩阵（Hessian matrix）是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵。可以表示为：
&lt;img alt="海森矩阵" src="http://upload.wikimedia.org/math/d/c/e/dce355e43fb1539863302f482da7e6f6.png" /&gt;&lt;/p&gt;
&lt;h3&gt;牛顿法算法&lt;/h3&gt;
&lt;p&gt;函数f(x)有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0.特别是当&lt;span class="math"&gt;\(H(x^{(k)})\)&lt;/span&gt;是正定矩阵时，函数f(x)的极值为极小值。
&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown f(x)=0$$&lt;/div&gt;
&lt;p&gt;
假设f(x)具有二阶连续偏导数，则将f(x)在第k次迭代值&lt;span class="math"&gt;\(x^{(k)}\)&lt;/span&gt;邻域内进行二阶展开。&lt;/p&gt;
&lt;p&gt;每次迭代从点&lt;span class="math"&gt;\(x^{(k)}\)&lt;/span&gt;开始，求目标函数的极小点，作为第k+1次迭代值&lt;span class="math"&gt;\(x^{(k+1)}\)&lt;/span&gt;。对将f(x)在第k次迭代值&lt;span class="math"&gt;\(x^{(k)}\)&lt;/span&gt;邻域内进行二阶展开，然后对x求导有：
&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown f(x)=g_k + H_k(x-x^{(k)})$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(g_k=\bigtriangledown f(x^{(k)})\)&lt;/span&gt;，&lt;span class="math"&gt;\(H_k\)&lt;/span&gt;是f(x)的海森矩阵在&lt;span class="math"&gt;\(x^{(k)}\)&lt;/span&gt;的值。
假设&lt;span class="math"&gt;\(x^{(k+1)}\)&lt;/span&gt;满足：
&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown f(x^{(k+1)})=0$$&lt;/div&gt;
&lt;p&gt;
由前面两个式子，可以得到：
&lt;/p&gt;
&lt;div class="math"&gt;$$g_k + H_k(x^{(k+1)}-x^{(k)})=0$$&lt;/div&gt;
&lt;p&gt;
因此可得
&lt;/p&gt;
&lt;div class="math"&gt;$$x^{(k+1)}=x^{(k)}+p_k  \; \; \; 其中H_kp_k=-g_k$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：目标函数f(x)，梯度&lt;span class="math"&gt;\(g(x)=\bigtriangledown f(x)\)&lt;/span&gt;矩阵,海森矩阵&lt;span class="math"&gt;\(H(x)\)&lt;/span&gt;，精度要求&lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;输出：f(x)的极小值点&lt;span class="math"&gt;\(x^*\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）取初始点&lt;span class="math"&gt;\(x^{(0)}\)&lt;/span&gt;，置k=0&lt;/p&gt;
&lt;p&gt;（2）计算&lt;span class="math"&gt;\(g_k=\bigtriangledown f(x^{(k)})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（3）若&lt;span class="math"&gt;\(\left \| g_k \right \| &amp;lt; \varepsilon\)&lt;/span&gt;，则停止计算，得近似解&lt;span class="math"&gt;\(x^*=x^{(k)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（4）计算&lt;span class="math"&gt;\(H_k=H(x^{(k)})\)&lt;/span&gt;，并求&lt;span class="math"&gt;\(p_k\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$p_k=-H_k^{-1}g_k$$&lt;/div&gt;
&lt;p&gt;（5）置&lt;span class="math"&gt;\(x^{(k+1)}=x^{(k)}+p_k\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（6）置k=k+1，转（2）&lt;/p&gt;
&lt;p&gt;步骤（4）中求&lt;span class="math"&gt;\(p_k=-H_k^{-1}g_k\)&lt;/span&gt;，先求&lt;span class="math"&gt;\(H_k^{-1}\)&lt;/span&gt;计算比较复杂，所以有其他改进算法。&lt;/p&gt;
&lt;h2&gt;拟牛顿法思路&lt;/h2&gt;
&lt;p&gt;考虑用一个n阶矩阵&lt;span class="math"&gt;\(G_k=G(x^{(k)})\)&lt;/span&gt;来代替&lt;span class="math"&gt;\(H_k^{-1}=H^{-1}(x^{(k)})\)&lt;/span&gt;。这就是拟牛顿法的基本想法。&lt;/p&gt;
&lt;p&gt;将&lt;span class="math"&gt;\(x=x^{(k+1)}\)&lt;/span&gt;代入下式：
&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown f(x)=g_k + H_k(x-x^{(k)})$$&lt;/div&gt;
&lt;p&gt;
得到
&lt;/p&gt;
&lt;div class="math"&gt;$$g_{k+1} - g_k = H_k(x^{(k+1)}-x^{(k)})$$&lt;/div&gt;
&lt;p&gt;
记$y_k=g_{k+1} - g_k, \delta_k = x^{(k+1)}-x^{(k)} $，则
&lt;/p&gt;
&lt;div class="math"&gt;$$y_k = H_k\delta_k$$&lt;/div&gt;
&lt;p&gt;
或
&lt;/p&gt;
&lt;div class="math"&gt;$$\delta_k = H_k^{-1}y_k$$&lt;/div&gt;
&lt;p&gt;
上式就称为拟牛顿条件。&lt;/p&gt;
&lt;div class="math"&gt;$$x=x^{(k)}+\lambda p_k = x^{(k)}-\lambda H_k^{-1}g_k$$&lt;/div&gt;
&lt;p&gt;
所以f(x)在&lt;span class="math"&gt;\(x^{(k)}\)&lt;/span&gt;的泰勒展开式可以近似写成：
&lt;/p&gt;
&lt;div class="math"&gt;$$ f(x) = f(x^{(k)})-\lambda g_k^TH_k^{-1}g_k$$&lt;/div&gt;
&lt;p&gt;
因为&lt;span class="math"&gt;\(H_k^{-1}\)&lt;/span&gt;是正定的，当&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;为一个充分小的正数时，总有&lt;span class="math"&gt;\(f(x)&amp;lt; f(x^{(k)})\)&lt;/span&gt;，也就是说&lt;span class="math"&gt;\(p_k\)&lt;/span&gt;是下降方向。&lt;/p&gt;
&lt;p&gt;拟牛顿法将&lt;span class="math"&gt;\(G_k\)&lt;/span&gt;作为&lt;span class="math"&gt;\(H_k^{-1}\)&lt;/span&gt;的近似，要求矩阵&lt;span class="math"&gt;\(G_k\)&lt;/span&gt;满足同样的条件。首先，&lt;span class="math"&gt;\(G_k\)&lt;/span&gt;是正定的，同时&lt;span class="math"&gt;\(G_k\)&lt;/span&gt;满足下面的拟牛顿条件：
&lt;/p&gt;
&lt;div class="math"&gt;$$\delta_k = G_{k+1}y_k$$&lt;/div&gt;
&lt;p&gt;
按照拟牛顿条件选择将&lt;span class="math"&gt;\(G_k\)&lt;/span&gt;作为&lt;span class="math"&gt;\(H_k^{-1}\)&lt;/span&gt;的近似或者将&lt;span class="math"&gt;\(B_k\)&lt;/span&gt;作为&lt;span class="math"&gt;\(H_k\)&lt;/span&gt;的近似的算法就称为&lt;strong&gt;拟牛顿法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;按照拟牛顿条件，在每次迭代中可以选择更新矩阵&lt;span class="math"&gt;\(G_{k+1}\)&lt;/span&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$G_{k+1} = G_{k} + \Delta G_{k} $$&lt;/div&gt;
&lt;p&gt;
这种选择有一定的灵活性，因此有多种具体实现方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DFP算法&lt;/strong&gt;：选择&lt;span class="math"&gt;\(G_{k+1}\)&lt;/span&gt;的方法是，假设每一步迭代中矩阵&lt;span class="math"&gt;\(G_{k+1}\)&lt;/span&gt;是由&lt;span class="math"&gt;\(G_{k}加上两个附加项构成\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BFGS算法&lt;/strong&gt;: 最流行的拟牛顿算法。BFGS算法与DFP算法类似，只是采用的B来近似H&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Broyden类算法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;BFGS的详细算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这时相应的拟牛顿条件是
&lt;/p&gt;
&lt;div class="math"&gt;$$B_{k+1}\delta_k = y_k$$&lt;/div&gt;
&lt;p&gt;
令
&lt;/p&gt;
&lt;div class="math"&gt;$$B_{k+1}=B_{k}+P_{k}+Q_{k}$$&lt;/div&gt;
&lt;p&gt;
找到适合条件的&lt;span class="math"&gt;\(P_{k}和Q_{k}\)&lt;/span&gt;可以求得&lt;span class="math"&gt;\(B_{k+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;输入：目标函数f(x)，梯度&lt;span class="math"&gt;\(g(x)=\bigtriangledown f(x)\)&lt;/span&gt;矩阵，精度要求&lt;span class="math"&gt;\(\varepsilon\)&lt;/span&gt; （不需要输入海森矩阵）&lt;/p&gt;
&lt;p&gt;输出：f(x)的极小值点&lt;span class="math"&gt;\(x^*\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）取初始点&lt;span class="math"&gt;\(x^{(0)}\)&lt;/span&gt;，取&lt;span class="math"&gt;\(B_0\)&lt;/span&gt;为正定对称矩阵，置k=0&lt;/p&gt;
&lt;p&gt;（2）计算&lt;span class="math"&gt;\(g_k=\bigtriangledown f(x^{(k)})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（3）若&lt;span class="math"&gt;\(|\left \| g_k \right \|| &amp;lt; \varepsilon\)&lt;/span&gt;，则停止计算，得近似解&lt;span class="math"&gt;\(x^*=x^{(k)}\)&lt;/span&gt; ，否则转向（4）&lt;/p&gt;
&lt;p&gt;（4）计算&lt;span class="math"&gt;\(p_k\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$p_k=-B_k^{-1}g_k$$&lt;/div&gt;
&lt;p&gt;（5）一维搜索：求&lt;span class="math"&gt;\(\lambda_k\)&lt;/span&gt;使得
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x^{(k)}+\lambda_k p_k) = min_{\lambda \geq 0}f(x^{(k)}+\lambda_k p_k)$$&lt;/div&gt;
&lt;p&gt;（6）置&lt;span class="math"&gt;\(x^{(k+1)}=x^{(k)}+\lambda_k p_k\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（7）计算&lt;span class="math"&gt;\(g_{k+1} =g(x^{(k+1)})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（8）若&lt;span class="math"&gt;\(|\left \| g_{k+1} \right \|| &amp;lt; \varepsilon\)&lt;/span&gt;，则停止计算，得近似解&lt;span class="math"&gt;\(x^*=x^{(k+1)}\)&lt;/span&gt; ，否则计算出&lt;span class="math"&gt;\(B_{k+1}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（9）置k=k+1，转（4）&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="基本概念"></category></entry><entry><title>拉格朗日对偶性（Lagrange Duality）</title><link href="http://www.wengweitao.com/la-ge-lang-ri-dui-ou-xing-lagrange-duality.html" rel="alternate"></link><updated>2014-07-30T09:56:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-30:la-ge-lang-ri-dui-ou-xing-lagrange-duality.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;在求解约束优化问题的时候，常常利用拉格朗日对偶性（Lagrange Duality）将原始问题转化为对偶问题，通过求解对偶问题而得到原始问题的解。该方法用在许多统计学习方法中，如最大熵模型和SVM等。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;原始问题&lt;/h2&gt;
&lt;h3&gt;原始最优化问题&lt;/h3&gt;
&lt;p&gt;考虑最优化问题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$min_x f(x)$$&lt;/div&gt;
&lt;div class="math"&gt;$$s.t. c_i(x) \leq 0  \:\:\:\:\: i=1,2,...,k$$&lt;/div&gt;
&lt;div class="math"&gt;$$h_j(x) = 0   \:\:\:\:\: j=1,2,...,l$$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;称此约束最优化问题为原始最优化问题或原始问题。&lt;/p&gt;
&lt;h3&gt;广义拉格朗日函数&lt;/h3&gt;
&lt;p&gt;为了解决原始问题，我们先引入&lt;strong&gt;广义拉格朗日（generalized Lagrangian）函数&lt;/strong&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{k}\alpha_i  c_i(x) + \sum_{j=1}^{l}\beta_j h_j(x)$$&lt;/div&gt;
&lt;p&gt;
这里的&lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;和&lt;span class="math"&gt;\(\beta_j\)&lt;/span&gt;称为&lt;strong&gt;拉格朗日乘子&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;极小极大问题&lt;/h3&gt;
&lt;p&gt;现在考虑&lt;span class="math"&gt;\(L(x, \alpha, \beta)\)&lt;/span&gt;的最大值：
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_p(x) = max_{\alpha, \beta: \alpha_i \geq 0}L(x, \alpha, \beta) $$&lt;/div&gt;
&lt;p&gt;
上式中的下标p代表“primal（原始问题）”。如果上式中的x不满足原始问题的约束条件（即存在某些i使得&lt;span class="math"&gt;\(c_i(x)&amp;gt;0\)&lt;/span&gt;或者&lt;span class="math"&gt;\(h_i(x) \neq 0\)&lt;/span&gt;），那么就有
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_p(x) =  f(x) + \sum_{i=1}^{k}\alpha_i  c_i(x) + \sum_{j=1}^{l}\beta_j h_j(x)= \infty$$&lt;/div&gt;
&lt;p&gt;
因为若&lt;span class="math"&gt;\(c_i(x)&amp;gt;0\)&lt;/span&gt;，可取&lt;span class="math"&gt;\(\alpha_i \rightarrow +\infty\)&lt;/span&gt;，或者&lt;span class="math"&gt;\(h_i(x) \neq 0\)&lt;/span&gt;，可令&lt;span class="math"&gt;\(\beta_j h_j(x) \rightarrow +\infty\)&lt;/span&gt;，而将其余的&lt;span class="math"&gt;\(\alpha_i ，\beta_j\)&lt;/span&gt;都取0.&lt;/p&gt;
&lt;p&gt;相反的，若满足原始优化问题的约束条件，那么有&lt;span class="math"&gt;\(\theta_p(x) = f(x)\)&lt;/span&gt;。所以有
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_p(x) = \left\{\begin{matrix}
f(x)   \:\:\:\:\: x满足约束条件\\
\infty  \:\:\:\:\: 其他
\end{matrix}\right.$$&lt;/div&gt;
&lt;p&gt;
因此，如果我们考虑极小化问题
&lt;/p&gt;
&lt;div class="math"&gt;$$min_x\theta_p(x) = min_xmax_{\alpha, \beta: \alpha_i \geq 0}L(x, \alpha, \beta) $$&lt;/div&gt;
&lt;p&gt;
我们可以看到这与原始最优化问题是等价的，称为&lt;strong&gt;广义拉格朗日函数的极小极大问题&lt;/strong&gt;。这样就把原始最优化问题转化为求解拉格朗日函数的极小极大问题。我们定义原始最优化问题的最优值为
&lt;/p&gt;
&lt;div class="math"&gt;$$p^* = min_x\theta_p(x)$$&lt;/div&gt;
&lt;p&gt;
称为&lt;strong&gt;原始问题的值&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;对偶问题&lt;/h2&gt;
&lt;p&gt;定义
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_D(x) = min_xL(x, \alpha, \beta) $$&lt;/div&gt;
&lt;p&gt;
上式中的下标D代表“dual（对偶问题）”。与前面定义的原始问题不同的是，原始问题是通过参数&lt;span class="math"&gt;\(\alpha, \beta\)&lt;/span&gt;进行优化（极大化），而这里定义的问题是通过&lt;span class="math"&gt;\(x\)&lt;/span&gt;进行优化（极小化）。&lt;/p&gt;
&lt;p&gt;对&lt;span class="math"&gt;\(\theta_D(x)\)&lt;/span&gt;极大化，就可以得到&lt;strong&gt;对偶优化问题&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;
&lt;div class="math"&gt;$$max_{\alpha, \beta: \alpha_i \geq 0}\theta_D(\alpha, \beta) = max_{\alpha, \beta: \alpha_i \geq 0}min_xL(x, \alpha, \beta) $$&lt;/div&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;称为&lt;strong&gt;广义拉格朗日函数的极大极小问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;对偶问题与原始问题除了交换式子中max和min的顺序外，是一样的。同样，我也定义&lt;strong&gt;对偶优化问题的最优值&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$d^* = max_{\alpha, \beta: \alpha_i \geq 0}\theta_D(\alpha, \beta)$$&lt;/div&gt;
&lt;p&gt;
称为&lt;strong&gt;对偶问题的值&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;原始问题和对偶问题的关系&lt;/h2&gt;
&lt;p&gt;容易得到（max min 小于等于 min max）：
&lt;/p&gt;
&lt;div class="math"&gt;$$d^* = max_{\alpha, \beta: \alpha_i \geq 0}min_xL(x, \alpha, \beta) \leq min_xmax_{\alpha, \beta: \alpha_i \geq 0}L(x, \alpha, \beta)= p^*$$&lt;/div&gt;
&lt;p&gt;在某些条件下，原始问题和对偶问题的最优值相等即&lt;span class="math"&gt;\(d^* = p^*\)&lt;/span&gt;，这时可以&lt;strong&gt;用对偶问题替代原始问题&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定理1：&lt;/strong&gt;考虑原始问题和对偶问题。假设&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(c_i(x)\)&lt;/span&gt;是凸函数（convex），&lt;span class="math"&gt;\(h_j(x)\)&lt;/span&gt;是仿射函数（affine，与线性函数类似只是允许增加一个截距b）；并且存在x，对所有的i有&lt;span class="math"&gt;\(c_i(x) &amp;lt; 0\)&lt;/span&gt;. 则存在&lt;span class="math"&gt;\(x^* , \alpha^* ,\beta^*\)&lt;/span&gt;，使得在&lt;span class="math"&gt;\(x^*\)&lt;/span&gt;是原始问题的解，&lt;span class="math"&gt;\(\alpha^* ,\beta^*\)&lt;/span&gt;是对偶问题的解，并且&lt;/p&gt;
&lt;div class="math"&gt;$$d^* = p^* = L(x^*, \alpha^*, \beta^*)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;定理2：&lt;/strong&gt;考虑原始问题和对偶问题。假设&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;和&lt;span class="math"&gt;\(c_i(x)\)&lt;/span&gt;是凸函数（convex），&lt;span class="math"&gt;\(h_j(x)\)&lt;/span&gt;是仿射函数（affine）；并且存在x，对所有的i有&lt;span class="math"&gt;\(c_i(x) &amp;lt; 0\)&lt;/span&gt;. 则&lt;span class="math"&gt;\(x^*\)&lt;/span&gt;是原始问题的解，并且&lt;span class="math"&gt;\(\alpha^*, \beta^*\)&lt;/span&gt;是对偶问题的解的充分必要条件是&lt;span class="math"&gt;\(x^* , \alpha^* ,\beta^*\)&lt;/span&gt; 满足下面的&lt;strong&gt;Karush-Kuhn-Tucker(KKT)条件&lt;/strong&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$\bigtriangledown_x L(x^*, \alpha^*, \beta^*)=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigtriangledown_\alpha L(x^*, \alpha^*, \beta^*)=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigtriangledown_\beta L(x^*, \alpha^*, \beta^*)=0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha^* c_i(x^*) = 0  \:\:\:\:\: i=1,2,...,k$$&lt;/div&gt;
&lt;div class="math"&gt;$$c_i(x^*) \leq 0  \:\:\:\:\: i=1,2,...,k$$&lt;/div&gt;
&lt;div class="math"&gt;$$\alpha^* \geq 0  \:\:\:\:\: i=1,2,...,k$$&lt;/div&gt;
&lt;div class="math"&gt;$$h_j(x^*) = 0   \:\:\:\:\: j=1,2,...,l$$&lt;/div&gt;
&lt;p&gt;其中，上面的第4个式子，称为KKT的对偶互补条件，由此条件可得，若&lt;span class="math"&gt;\(\alpha^*&amp;gt;0\)&lt;/span&gt;那么&lt;span class="math"&gt;\(c_i(x^*) = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="基础概念"></category></entry><entry><title>决策树</title><link href="http://www.wengweitao.com/jue-ce-shu.html" rel="alternate"></link><updated>2014-07-29T10:00:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-29:jue-ce-shu.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;决策树（decision tree）是一种基本的分类与回归方法。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型可读性好，分类速度快。决策树的学习通常包含3个部分：特征选择、决策树生成和决策树的修剪。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。常用的算法有ID3、C4.5和CART。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;决策树模型与学习&lt;/h2&gt;
&lt;h3&gt;决策树模型&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;决策树&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。&lt;strong&gt;内部结点&lt;/strong&gt;表示一个特征或者属性，&lt;strong&gt;叶节点&lt;/strong&gt;表示一个类。&lt;/p&gt;
&lt;p&gt;用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归的对实例进行测试并分配，直至到达叶节点，最后将实例分配到叶节点的类中。&lt;/p&gt;
&lt;h3&gt;决策树与if-then规则&lt;/h3&gt;
&lt;p&gt;互斥且完备&lt;/p&gt;
&lt;h3&gt;决策树与条件概率分布&lt;/h3&gt;
&lt;p&gt;决策树还可以表示为&lt;strong&gt;给定特征条件下类的条件概率分布&lt;/strong&gt;。各叶节点上的条件概率往往偏向某一个类，决策树分类时将该结点的实例强行分到条件概率大的那一类去。&lt;/p&gt;
&lt;h3&gt;决策树学习&lt;/h3&gt;
&lt;p&gt;学习的目标：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。&lt;/p&gt;
&lt;p&gt;决策树模型可以看成是由训练数据集估计条件概率模型。&lt;/p&gt;
&lt;p&gt;决策树的学习的策略是以损失函数为目标函数的最小化。决策树的损失函数通常是正则化的极大似然函数。&lt;strong&gt;学习的问题就变成了在损失函数意义下选择最优决策树的问题。&lt;/strong&gt;因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树的学习算法通常采用启发式方法，近似求解这一优化问题。这样得到的决策树是次最优（sub-optimal）的。&lt;/p&gt;
&lt;p&gt;决策树的学习算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。&lt;/p&gt;
&lt;p&gt;（1）开始构建根结点，将所有的训练数据都放入根结点；&lt;/p&gt;
&lt;p&gt;（2）选择一个最优特征，按照这一特征将训练数据分割成子集，使得各个子集有一个在当前条件下最好的分类；&lt;/p&gt;
&lt;p&gt;（3）如果这些子集已经基本被正确分类，那么就把这些子集分到所对应的叶节点中去；&lt;/p&gt;
&lt;p&gt;（4）如果还有子集未能基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割&lt;/p&gt;
&lt;p&gt;（5）如此递归下去，直到全部基本正确分类，最后每一个子集都被分配到叶节点上，即都有了明确的分类，这就生成了一棵决策树。&lt;/p&gt;
&lt;p&gt;以上生成的决策树对训练数据有很好的分类能力，但可能发生过拟合的情况。我们需要对生成的决策树进行自下而上的剪枝（去掉过于细分的叶结点，使其回退到父节点或者更高的结点，使树变得更简单），使其具有更好的泛化能力。&lt;/p&gt;
&lt;p&gt;如果特征数量过多，可以在决策树学习开始的时候，对特征进行选择，留下对训练数据有足够充分分类能力的特征。&lt;/p&gt;
&lt;p&gt;可以看出决策树的学习算法包含特征选择、决策树生成和决策树剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;特征选择&lt;/h2&gt;
&lt;h3&gt;特征选择问题&lt;/h3&gt;
&lt;p&gt;特征的选择在于选取对训练数据具有分类能力的特征，这样可以提高决策树学习的效率。&lt;strong&gt;通常特征选择的准则是信息增益或信息增益比&lt;/strong&gt;。特征选择是决定用哪个特征来划分特征空间。如果一个特征具有较强的分类能力，那么我们就应该选择这个特征（或者说一个特征能使划分后的子集在当前条件下有最好的分类）。信息增益（information gain）就能够很好的表示这一准则。&lt;/p&gt;
&lt;h3&gt;信息增益&lt;/h3&gt;
&lt;p&gt;先给出熵和条件熵的定义。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;熵（entropy）&lt;/strong&gt;表示随机变量不确定性的度量。假设X是一个取有限个值的离散变量，X的熵定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$H(X)=-\sum_{i=1}^{n}p_ilogp_i$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(p_i\)&lt;/span&gt;为X取每一个可能值的概率，上式中的对数以2为底或者以e为底，这时熵的单位分别称作比特（bit）或纳特（nat）。可以看出X的熵只依赖于X的分布，而与X的取值无关，所以可以将X的熵记为
&lt;/p&gt;
&lt;div class="math"&gt;$$H(p)=-\sum_{i=1}^{n}p_ilogp_i$$&lt;/div&gt;
&lt;p&gt;
熵越大，随机变量的不确定性就越大。从定义可以验证：
&lt;/p&gt;
&lt;div class="math"&gt;$$0 \leq H(p) \leq logn$$&lt;/div&gt;
&lt;p&gt;
即熵达到最大值（所有可能的事件等概率时不确定性最高）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;条件熵H(Y|X)&lt;/strong&gt;表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望
&lt;/p&gt;
&lt;div class="math"&gt;$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$&lt;/div&gt;
&lt;p&gt;
这里&lt;span class="math"&gt;\(p_i=P(X=x_i), i=1,2,...,n\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;当熵和条件熵中的概率由数据统计得到时，所对应的熵与条件熵分别称为&lt;strong&gt;经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信息增益（information gain）&lt;/strong&gt;表示得知特征X的信息而使得类Y的信息不确定性的&lt;strong&gt;减少&lt;/strong&gt;长度。&lt;/p&gt;
&lt;p&gt;特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差：
&lt;/p&gt;
&lt;div class="math"&gt;$$g(D|A)=H(D)-H(D|A)$$&lt;/div&gt;
&lt;p&gt;
一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。
信息增益大的特征具有更强的分类能力。&lt;/p&gt;
&lt;p&gt;根据学习增益准则的特征选择方法：对训练数据集D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;信息增益的算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：训练数据集D和特征A；&lt;/p&gt;
&lt;p&gt;输出：特征A对训练数据集D的信息增益g(D,A)&lt;/p&gt;
&lt;p&gt;（1）计算数据集D的经验熵
&lt;/p&gt;
&lt;div class="math"&gt;$$H(D)=-\sum_{k=1}^{K}\frac{C_k}{|D|}log_2\frac{C_k}{|D|}$$&lt;/div&gt;
&lt;p&gt;
（2）计算特征A对数据集D的经验条件熵
&lt;/p&gt;
&lt;div class="math"&gt;$$H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}log_2\frac{|D_{ik}|}{|D_i|}$$&lt;/div&gt;
&lt;p&gt;
（3）计算信息增益
&lt;/p&gt;
&lt;div class="math"&gt;$$g(D|A)=H(D)-H(D|A)$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(|D|\)&lt;/span&gt;表示样本容量，设有K个类&lt;span class="math"&gt;\(C_k，|C_k|\)&lt;/span&gt;属于类&lt;span class="math"&gt;\(C_k\)&lt;/span&gt;的样本数量。特征A将D划分为n个子集&lt;span class="math"&gt;\(D_1,D_2,...,D_n，|D_i|\)&lt;/span&gt;为&lt;span class="math"&gt;\(D_i\)&lt;/span&gt;的样本数。子集&lt;span class="math"&gt;\(D_i\)&lt;/span&gt;中属于类&lt;span class="math"&gt;\(C_k\)&lt;/span&gt;的样本的集合为&lt;span class="math"&gt;\(D_{ik}\)&lt;/span&gt;。&lt;/p&gt;
&lt;h3&gt;信息增益比&lt;/h3&gt;
&lt;p&gt;以信息增益作为划分训练数据集的特征，存在&lt;strong&gt;偏向于选择取值较多的特征&lt;/strong&gt;的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。&lt;/p&gt;
&lt;p&gt;特征A对训练数据集D的信息增益比&lt;span class="math"&gt;\(g_R(D,A)\)&lt;/span&gt;定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵&lt;span class="math"&gt;\(H_A(D)\)&lt;/span&gt;之比：
&lt;/p&gt;
&lt;div class="math"&gt;$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(H_A(D)=-\sum_{i=1}^{n}\frac{|D_{i}|}{|D|}log_2\frac{|D_{i}|}{|D|}\)&lt;/span&gt;，n为特征A的取值个数，&lt;span class="math"&gt;\(D_i\)&lt;/span&gt;表示特征A将D分成的子集。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;决策树的生成&lt;/h2&gt;
&lt;h3&gt;ID3算法&lt;/h3&gt;
&lt;p&gt;ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体的方法是：&lt;/p&gt;
&lt;p&gt;（1）从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点&lt;/p&gt;
&lt;p&gt;（2）对子节点递归的调用以上方法，构建决策树&lt;/p&gt;
&lt;p&gt;（3）直到所有特征的信息增益均很小或没有特征选择为止。&lt;/p&gt;
&lt;p&gt;ID3算法相当于用极大似然估计法进行概率模型的选择。&lt;/p&gt;
&lt;h3&gt;C4.5的生成算法&lt;/h3&gt;
&lt;p&gt;C4.5算法与ID3算法类似，在生成的过程中，用信息增益比来选择特征。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;决策树的剪枝&lt;/h2&gt;
&lt;p&gt;决策树生成算法产生的决策树，会出现过拟合的现象。因为在学习的过程中过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法就是考虑决策树的复杂度，对已生成的决策树进行简化。对已生成的决策树进行简化的过程称为&lt;strong&gt;剪枝（pruning）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。决策树的损失函数可以定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|$$&lt;/div&gt;
&lt;p&gt;
其中，树T的叶节点数为|T|，叶节点t有&lt;span class="math"&gt;\(|N_t|\)&lt;/span&gt;个样本点，其中属于k类的数目为&lt;span class="math"&gt;\(|N_{tk}|\)&lt;/span&gt;个，&lt;span class="math"&gt;\(H_t(T)\)&lt;/span&gt;为叶节点t的经验熵
&lt;/p&gt;
&lt;div class="math"&gt;$$H_t(T)=-\sum_{k=1}^{K}\frac{|N_{tk}|}{|N_t|}log_2\frac{|N_{tk}|}{|N_t|}$$&lt;/div&gt;
&lt;p&gt;
决策树的损失函数可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(T)=C(T)+\alpha|T|$$&lt;/div&gt;
&lt;p&gt;
这时C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型的复杂度，参数α控制二者直接的影响。较大的α促使选择较简答的模型，较小反之。&lt;/p&gt;
&lt;p&gt;剪枝，就是当&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;确定时，选择损失函数最小的模型，即损失函数最小的子树。利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;树的剪枝算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：生成算法产生的整棵树T，参数&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;输出：修剪后的子树&lt;span class="math"&gt;\(T_\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）计算每个结点的经验熵&lt;/p&gt;
&lt;p&gt;（2）递归地从叶节点向上回缩。设叶节点回到到其父节点之前与之后的整体树分别为&lt;span class="math"&gt;\(T_B\)&lt;/span&gt;和&lt;span class="math"&gt;\(T_A\)&lt;/span&gt;，如果其对应的损失函数有：
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(T_A) \leq C_\alpha(T_B)$$&lt;/div&gt;
&lt;p&gt;
则进行剪枝，即将父节点变为新的叶结点。&lt;/p&gt;
&lt;p&gt;（3）返回（2）直至不能继续，得到损失函数最小的子树。&lt;/p&gt;
&lt;p&gt;上式只需考虑两棵树的损失函数的差，其计算可以在局部进行，所以决策树的剪枝算法可以由一种动态规划算法实现。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;CART算法&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;分类与回归树（classification and regression tree, CART）&lt;/strong&gt;是应用广泛的决策树学习方法。既可以用于分类也可以用于回归。CART假设决策树是&lt;strong&gt;二叉树&lt;/strong&gt;，内部结点的特征取值为“是”或“否”，左分支是取值为“是”的分支，右分支是取值为“否”的的分支。这样的决策树等于递归地二分每个特征，将输入空间划分为有限个单元，并在这些单元上确定预测的概率分布。&lt;/p&gt;
&lt;p&gt;CART算法由以下两步组成：&lt;/p&gt;
&lt;p&gt;（1）决策树的生成&lt;/p&gt;
&lt;p&gt;（2）决策树的剪枝&lt;/p&gt;
&lt;h3&gt;CART生成&lt;/h3&gt;
&lt;p&gt;决策树的生成就是递归地构建二叉决策树的过程，对回归树用平方误差最小化准则，对分类树&lt;strong&gt;用基尼指数（Gini index）最小化准则，进行特征选择&lt;/strong&gt;，生成二叉树。&lt;/p&gt;
&lt;h4&gt;回归树的生成&lt;/h4&gt;
&lt;p&gt;遍历所有输入变量，找到最优的切分变量j和切分点（可以用平方误差来表示回归树对于训练数据的预测误差，使预测误差最小），构成一个对（j,s）。依次将输入空间划分为两个区域，接着对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的&lt;strong&gt;回归树通常称为最小二乘回归树（least squares regression tree）&lt;/strong&gt;。&lt;/p&gt;
&lt;h6&gt;分类树的生成&lt;/h6&gt;
&lt;p&gt;分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。&lt;/p&gt;
&lt;p&gt;在分类问题中，假设有K个类，样本点属于第k类的概率为&lt;span class="math"&gt;\(p_k\)&lt;/span&gt;，则概率分布的基尼指数定义为
&lt;/p&gt;
&lt;div class="math"&gt;$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^{K}p_k^2$$&lt;/div&gt;
&lt;p&gt;
对于给定的样本集合D，其基尼指数为
&lt;/p&gt;
&lt;div class="math"&gt;$$Gini(p)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2$$&lt;/div&gt;
&lt;p&gt;
假设样本集合D根据特征A是否取某一可能值a被分割成两个部分&lt;span class="math"&gt;\(D_1\)&lt;/span&gt;和&lt;span class="math"&gt;\(D_2\)&lt;/span&gt;，则在特征A的条件下，集合D的基尼系数定义为
&lt;/p&gt;
&lt;div class="math"&gt;$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$&lt;/div&gt;
&lt;p&gt;
&lt;strong&gt;基尼指数Gini(D)表示集合D的不确定性&lt;/strong&gt;，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性也就越大（与熵类似）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CART生成算法：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：训练数据集D，停止计算的条件（如结点中样本个数小于预定阈值，或样本集的基尼指数小于预定阈值）&lt;/p&gt;
&lt;p&gt;输出：CART决策树&lt;/p&gt;
&lt;p&gt;从根结点开始，递归地对每个结点进行以下操作：&lt;/p&gt;
&lt;p&gt;（1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数，对每一个特征A，对其可能的每一个取值a，根据样本点对A=a的测试为“是”或“否”将D分割成&lt;span class="math"&gt;\(D_1\)&lt;/span&gt;和&lt;span class="math"&gt;\(D_2\)&lt;/span&gt;两部分，计算A=a时的基尼指数。&lt;/p&gt;
&lt;p&gt;（2）在所有的特征A以及它们所有可能的切分点a中，&lt;strong&gt;选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点&lt;/strong&gt;。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。&lt;/p&gt;
&lt;h3&gt;CART剪枝&lt;/h3&gt;
&lt;p&gt;CART剪枝算法由两步组成：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（1）剪枝，形成一个子树序列&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;从生成算法产生的决策树&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;低端开始不断剪枝，直到&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;的根结点，形成一个子树序列&lt;span class="math"&gt;\({T_0,T_1,...,T_n}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;在剪枝的过程中，计算子树的损失函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(T)=C(T)+\alpha|T|$$&lt;/div&gt;
&lt;p&gt;
其中，T为任意子树，C(T)为对训练数据的预测误差（如基尼指数），|T|为子树的叶结点个数。&lt;/p&gt;
&lt;p&gt;从整体树&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;开始剪枝，对&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;的任意内部结点t，以t为单结点树的损失函数是（叶结点个数为0）
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(t)=C(t)+\alpha$$&lt;/div&gt;
&lt;p&gt;
以t为根结点的子树&lt;span class="math"&gt;\(T_t\)&lt;/span&gt;的损失函数是
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(T_t)=C(T_t)+\alpha|T_t|$$&lt;/div&gt;
&lt;p&gt;
当&lt;span class="math"&gt;\(\alpha=0及\alpha\)&lt;/span&gt;充分小时，有不等式
&lt;/p&gt;
&lt;div class="math"&gt;$$C_\alpha(T_t) \leq C_\alpha(t)$$&lt;/div&gt;
&lt;p&gt;
当&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;增大时，上面&lt;span class="math"&gt;\(C_\alpha(T_t) = C_\alpha(t)\)&lt;/span&gt;，当&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;再增大时，上面的不等式就反向了。只要&lt;span class="math"&gt;\(\alpha = \frac{C(t)-C(T_t)}{|T_t|-1}\)&lt;/span&gt;，&lt;span class="math"&gt;\(T_t与t\)&lt;/span&gt;有相同的损失函数值，而t的结点少，因此t比&lt;span class="math"&gt;\(T_t\)&lt;/span&gt;更可取，对&lt;span class="math"&gt;\(T_t\)&lt;/span&gt;进行剪枝。&lt;/p&gt;
&lt;p&gt;为此对&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;中每一内部结点t，计算
&lt;/p&gt;
&lt;div class="math"&gt;$$g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}$$&lt;/div&gt;
&lt;p&gt;
上式的分子可以表示误差的增加，分母部分表示剪枝后树叶数量的减小。它表示剪枝后整体损失函数减少的程度。在&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;中减去g(t)最小的&lt;span class="math"&gt;\(T_t\)&lt;/span&gt;，将得到子树作为&lt;span class="math"&gt;\(T_1\)&lt;/span&gt;，同时将最小的g(t)设为&lt;span class="math"&gt;\(\alpha_1\)&lt;/span&gt;。&lt;span class="math"&gt;\(T_1\)&lt;/span&gt;为区间&lt;span class="math"&gt;\([\alpha_1,\alpha_2)\)&lt;/span&gt;的最优子树。&lt;/p&gt;
&lt;p&gt;如此剪枝下去，直至得到根结点。在这一过程中，不断增加&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;的值，产生新的区间。&lt;/p&gt;
&lt;p&gt;（1）对于所有的子树t，我们想用合适的叶子节点来代替t，然后计算增加的误差E与t的叶子节点的比值（即g(t)）。我们选择比值最小的那个子树t，用合适的叶子节点代替之。&lt;/p&gt;
&lt;p&gt;（2）重复迭代以上步骤，每次都替换掉一棵子树。我们会得到从完全增长的树&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;到只有根节点一个决策结点的树&lt;span class="math"&gt;\(T_n\)&lt;/span&gt;的一系列决策树：&lt;span class="math"&gt;\(T_0,T_1,...,T_n\)&lt;/span&gt;。然后我们用独立的验证集(我们可以从可用数据集中抽取三分之一作为验证集，剩下的三分之二作为训练集)来验证各个决策树的分类准确性。选取准确性最高的决策树为最终的CART决策树。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;（2）在剪枝得到的子树序列&lt;span class="math"&gt;\(T_0,T_1,...,T_n\)&lt;/span&gt;中通过交叉验证选取最优子树&lt;span class="math"&gt;\(T_\alpha\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;测试子树序列&lt;span class="math"&gt;\(T_0,T_1,...,T_n\)&lt;/span&gt;中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中每棵子树&lt;span class="math"&gt;\(T_0,T_1,...,T_n\)&lt;/span&gt;都对应于一个参数&lt;span class="math"&gt;\(\alpha_0,\alpha_1,...,\alpha_n\)&lt;/span&gt;。所以当最优子树&lt;span class="math"&gt;\(T_k\)&lt;/span&gt;确定时，对应的&lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt;也确定了，即得到最优决策树&lt;span class="math"&gt;\(T_\alpha\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;如果一个结点是叶节点那么其g(t)为无穷大，如果不是叶节点那么按照以上的方法计算g(t)。每次从当前的树中选择一个g(t)最小的（因为该结点增加的误差率是最小的），然后删去该结点或结点集合，得到一棵新的树，然后递归上面的过程。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CART剪枝算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：CART算法生成的决策树&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;输出：最优决策树&lt;span class="math"&gt;\(T_\alpha\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（1）设k=0, T=&lt;span class="math"&gt;\(T_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（2）$\alpha = +\infty $&lt;/p&gt;
&lt;p&gt;（3）自下而上对各内部结点计算&lt;span class="math"&gt;\(C(T_t)\)&lt;/span&gt;，计算出g(t)，并且将各结点计算出的最小的g(t)保存在&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;中&lt;/p&gt;
&lt;p&gt;（4）自上而下访问内部结点，如果g(t)=&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;，剪去g(t)最小的T，并对叶节点t以多数表决法决定其类，得到树T&lt;/p&gt;
&lt;p&gt;（5）k=k+1,在&lt;span class="math"&gt;\(\alpha_k=\alpha, T_k=T\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;（6）T不是由根结点单独一个结点构成的树，就&lt;strong&gt;返回（4）（（2）？）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;（7）采用交叉验证在剪枝得到的子树序列&lt;span class="math"&gt;\(T_0,T_1,...,T_n\)&lt;/span&gt;中选取最优子树&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第五章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>朴素贝叶斯法</title><link href="http://www.wengweitao.com/po-su-bei-xie-si-fa.html" rel="alternate"></link><updated>2014-07-28T14:06:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-28:po-su-bei-xie-si-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;朴素贝叶斯法（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，利用特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;朴素贝叶斯法的学习与分类&lt;/h2&gt;
&lt;h3&gt;基本方法&lt;/h3&gt;
&lt;p&gt;朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y).x为输入特征向量，y为输出的类标记。
先求先验概率
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=c_k), k = 1,2,...,K$$&lt;/div&gt;
&lt;p&gt;
然后求条件概率分布
&lt;/p&gt;
&lt;div class="math"&gt;$$P(X=x|Y=c_k)$$&lt;/div&gt;
&lt;p&gt;
于是学习到联合概率分布P(X,Y)。&lt;/p&gt;
&lt;p&gt;条件概率分布&lt;span class="math"&gt;\(P(X=x|Y=c_k)\)&lt;/span&gt;有指级数量的次数，其估计实际是不可行的。假设&lt;span class="math"&gt;\(x^{(j)}\)&lt;/span&gt;的可能有&lt;span class="math"&gt;\(S_j\)&lt;/span&gt;个，j=1,2,...,n，Y的取值有K个，那么参数的个数为&lt;span class="math"&gt;\(K\prod S_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;朴素贝叶斯法是典型的生成方法。之所以叫做朴素贝叶斯法是因为该方法&lt;strong&gt;对条件概率的分布作了条件独立性的假设&lt;/strong&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_k)=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$&lt;/div&gt;
&lt;p&gt;朴素贝叶斯分类时，对给定的输入x，通过学习到的模型计算后验概率分布&lt;span class="math"&gt;\(P(Y=c_k|X=x)\)&lt;/span&gt;，将后验概率最大的类作为x的类输出。&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y|X)=\frac{P(X,Y)}{P(X)}=\frac{P(X|Y)P(Y)}{\sum P(Y)P(X|Y)}$$&lt;/div&gt;
&lt;p&gt;
这是朴实贝叶斯分类器的基本形式。因为分母对于所有的&lt;span class="math"&gt;\(c_k\)&lt;/span&gt;都是相同的，所以朴素贝叶斯的分类器可以表示为
&lt;/p&gt;
&lt;div class="math"&gt;$$y=argmaxP(Y=c_k)\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_k)$$&lt;/div&gt;
&lt;p&gt;
将n维的输入向量x代入，求出y值最大的那个分类&lt;span class="math"&gt;\(c_k\)&lt;/span&gt;，就是输入x的类。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;什么是先验概率和后验概率？
先验概率P(h)：就是在事情尚未发生之前，我们对该事件发生概率的估计，是根据以往经验分析得到的概率；
后验概率P(h|D)：给定D时，h发生的概率。是表示在某事件已经发生的条件下，求该事件由某个元素引起的可能性大小。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;后验概率最大化的含义&lt;/h3&gt;
&lt;p&gt;朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望经验风险最小化。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;朴素贝叶斯法的参数估计&lt;/h2&gt;
&lt;h3&gt;极大似然估计&lt;/h3&gt;
&lt;p&gt;在朴素贝叶斯法中，学习意味着估计&lt;span class="math"&gt;\(P(Y=c_k)\)&lt;/span&gt;和&lt;span class="math"&gt;\(P(X^{(j)}=x^{(j)}|Y=c_k)\)&lt;/span&gt;。可以运用极大似然估计法估计相应的概率
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}=a_{jl}|y_i=c_k)}{N}$$&lt;/div&gt;
&lt;h3&gt;贝叶斯估计&lt;/h3&gt;
&lt;p&gt;用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y=c_k)=\frac{\sum_{i=1}^{N}I(y_i=c_k)+\lambda}{N+K\lambda}$$&lt;/div&gt;
&lt;div class="math"&gt;$$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^{N}I(x^{(j)}=a_{jl}|y_i=c_k)+\lambda}{N+S_j\lambda}$$&lt;/div&gt;
&lt;p&gt;
等价于在随机变量各个取值的频数上赋予一个正数&lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;。常取&lt;span class="math"&gt;\(\lambda=1\)&lt;/span&gt;成为拉普拉斯平滑。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第四章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>k近邻法</title><link href="http://www.wengweitao.com/kjin-lin-fa.html" rel="alternate"></link><updated>2014-07-27T16:18:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-27:kjin-lin-fa.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;k近邻法（k-nearest neighbor, k-NN）是一种基于分类与回归的方法。分类时，对新的待分类的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻算法不具有显示的学习过程。k值的选择、距离度量及分类决策规则是k近邻法的3个基本要素。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;k近邻算法&lt;/h2&gt;
&lt;p&gt;k近邻算法简单、直观：对新的待分类的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。&lt;/p&gt;
&lt;h2&gt;k近邻模型&lt;/h2&gt;
&lt;p&gt;k近邻法的使用的模型实际上对应于对特征空间的划分。模型由三个要素——距离度量、k值的选择和分类决策规则决定。&lt;/p&gt;
&lt;h3&gt;模型&lt;/h3&gt;
&lt;p&gt;当训练集、距离度量、k值以及分类决策规程确定后，对于任何一个新的输入实例，它所属的类唯一确定。k近邻法的模型对应特征空间的一个划分，将特征空间划分为一些子空间，确定子空间里的每一个点所属的类。&lt;/p&gt;
&lt;p&gt;在特征空间中，对每个训练实例点&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;，距离该点比其他点更近的所有点组成一个区域，叫做单元（cell）。最近邻将实例&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;的类作为类&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;作为其单元中所有点的类标记（class label）。&lt;/p&gt;
&lt;h3&gt;距离度量&lt;/h3&gt;
&lt;p&gt;特征空间中两个实例点的距离是两个实例点相似程度的反映。&lt;/p&gt;
&lt;p&gt;设特征空间是n维实数向量空间，对于&lt;span class="math"&gt;\(x_i,x_j\)&lt;/span&gt;的&lt;span class="math"&gt;\(L_p\)&lt;/span&gt;距离定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$L_p(x_i,x_j)=(\sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$$&lt;/div&gt;
&lt;p&gt;
当p=2时，是欧氏距离；&lt;/p&gt;
&lt;p&gt;当p=1时，是曼哈顿距离；&lt;/p&gt;
&lt;p&gt;当p=&lt;span class="math"&gt;\(\infty\)&lt;/span&gt;时，它是各个坐标距离的最大值，即
&lt;/p&gt;
&lt;div class="math"&gt;$$L(x_i,x_j)=max|x_i^{(l)}-x_j^{(l)}|$$&lt;/div&gt;
&lt;h3&gt;k值的选择&lt;/h3&gt;
&lt;p&gt;k值的选择会对k近邻法的结果产生重大的影响。如果选择较小的k值，那么就相当于用较小的领域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例较近的训练实例才会对预测结果有作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果对近邻的实例点非常敏感，如果近邻点切好是噪音，那么预测就会出错。也就是说，k值较小整体模型越复杂，容易发生拟合。k值较大则相反。&lt;/p&gt;
&lt;p&gt;实际中通常k值取一个较小的数，采用交叉验证法来选取最优的k值。&lt;/p&gt;
&lt;h3&gt;分类决策规则&lt;/h3&gt;
&lt;p&gt;由输入实例的k个近邻中的多数类来决定输入实例的类。其实多数表决的功能就等价于经验风险最小化。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;k近邻法的实现：kd树&lt;/h2&gt;
&lt;p&gt;实现k近邻法时，主要考虑的问题是如果对训练数据进行快速k近邻搜索。最简单的实现方法就是线性扫描，计算新的输入实例与每一个训练实例的距离。当训练集很大，计算非常耗时，不可行。&lt;/p&gt;
&lt;p&gt;为了提高k近邻搜索效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。&lt;/p&gt;
&lt;h3&gt;构造kd树&lt;/h3&gt;
&lt;p&gt;kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分（partition）。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每一个结点对应于一个k维超矩形区域。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;构造平衡kd树的算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：k维空间数据集&lt;span class="math"&gt;\(T={x_1,x_2,...,x_N}\)&lt;/span&gt;，其中&lt;span class="math"&gt;\(x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(k)})^T\)&lt;/span&gt;，i=1,2,...,N；&lt;/p&gt;
&lt;p&gt;输出：kd树&lt;/p&gt;
&lt;p&gt;（1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。
选择&lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt;为坐标轴，以T中所有实例的&lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt;坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴&lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt;垂直的超平面实现。由根结点生成深度为1的左、右结点，左子结点区域的&lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt;坐标对应的值小于切分点的值；右子结点区域的&lt;span class="math"&gt;\(x^{(1)}\)&lt;/span&gt;坐标对应的值大于切分点的值。将落在切分超平面上的实例点保存在根结点。&lt;/p&gt;
&lt;p&gt;（2）重复：对于深度为j的结点，选择&lt;span class="math"&gt;\(x^{(l)}\)&lt;/span&gt;为切分的坐标轴，&lt;span class="math"&gt;\(l=j \mod k + 1\)&lt;/span&gt;（如对于2维空间，（0,1）、（1,2）、（2,1）、（3,2）...），以该结点区域中所有实例的&lt;span class="math"&gt;\(x^{(l)}\)&lt;/span&gt;坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。&lt;/p&gt;
&lt;p&gt;（3）直到两个区域没有实例存在时停止（所有实例点都在超平面上）。&lt;/p&gt;
&lt;h3&gt;搜索kd树&lt;/h3&gt;
&lt;p&gt;kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;用kd树的最邻近搜索的算法&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;输入：已构造的kd树；目标点x；&lt;/p&gt;
&lt;p&gt;输出：x的最近邻&lt;/p&gt;
&lt;p&gt;（1）在kd树中找到包含目标点x的叶结点&lt;/p&gt;
&lt;p&gt;（2）以此叶节点为“当前最近点”&lt;/p&gt;
&lt;p&gt;（3）递归向上回退&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;（&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="err"&gt;）如果该结点保存的实例点比当前最几点距离目标点更近，则以该实例点为“当前”最近点&lt;/span&gt;

&lt;span class="err"&gt;（&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="err"&gt;）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。即以目标点为球心，以目标点与当前最近点间的距离为半径的球体是否与另一子结点对应的区域相交。如果相交，可能在另一个子结点对应的区域中存在更近的点。移动到另一个子结点，接着递归地进行最邻近搜索。如果不相交，则向上回退。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;（4）当回退到根结点，搜索结束，此时的当前最近结点即为最邻近点&lt;/p&gt;
&lt;p&gt;如果实例点是均匀分布的，那么kd树的搜索的平均计算复杂度是O(logN)，N为训练实例数。kd树适用于训练实例远大于空间维数时的k近邻搜索。当空间维数接近训练实例的时候，几乎接近线性搜索，效率迅速下降。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第三章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>感知机</title><link href="http://www.wengweitao.com/gan-zhi-ji.html" rel="alternate"></link><updated>2014-07-26T20:12:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-26:gan-zhi-ji.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此导入了基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机是神经网络与支持向量机的基础。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;感知机模型&lt;/h2&gt;
&lt;p&gt;由输入空间到输出空间的如下函数：
&lt;/p&gt;
&lt;div class="math"&gt;$$f(x)=sign(w.x+b)$$&lt;/div&gt;
&lt;p&gt;
称为&lt;strong&gt;感知机&lt;/strong&gt;。其中w和b称为感知机模型参数，&lt;strong&gt;w叫做权值&lt;/strong&gt;（weight）或权值向量，b叫做偏置（bias）。感知机（perceptron）是&lt;strong&gt;二类分类的线性分类模型&lt;/strong&gt;，属于判别模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知机的几何解释&lt;/strong&gt;：线性方程
&lt;/p&gt;
&lt;div class="math"&gt;$$w.x+b=0$$&lt;/div&gt;
&lt;p&gt;
对应于特征空间&lt;span class="math"&gt;\(R^n\)&lt;/span&gt;中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分为两个部分，位于两部分的点分别被分为正、负两类。因此，超平面S称为&lt;strong&gt;分离超平面（separating hyperplanes）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;感知机学习，就是通过训练数据集，求得感知机模型，即求的模型参数w，b。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;感知机学习策略&lt;/h2&gt;
&lt;h3&gt;数据集的线性可分性&lt;/h3&gt;
&lt;p&gt;如果存在某个超平面S
&lt;/p&gt;
&lt;div class="math"&gt;$$w.x+b=0$$&lt;/div&gt;
&lt;p&gt;
能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有&lt;span class="math"&gt;\(y_i=+1\)&lt;/span&gt;的实例i，有&lt;span class="math"&gt;\(w.x_i+b&amp;gt;0\)&lt;/span&gt;，对所有&lt;span class="math"&gt;\(y_i=-1\)&lt;/span&gt;的实例i，有&lt;span class="math"&gt;\(w.x_i+b&amp;lt;0\)&lt;/span&gt;，则称数据集T为&lt;strong&gt;线性可分数据集&lt;/strong&gt;；否则为线性不可分。&lt;/p&gt;
&lt;h3&gt;感知机学习策略&lt;/h3&gt;
&lt;p&gt;假设数据集线性可分，感知机的学习目标是求得一个能够将训练集正实例点和负实例点完全正确分开的超平面。为了找到这个超平面，即确定感知机模型参数w，b，需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。&lt;/p&gt;
&lt;p&gt;损失函数的一个自然选择是误分类点的总数，但是损失函数不是w，b的连续可导函数，不易优化。损失函数的另一个选择是计算误分类点到超平面的总距离。
输入空间中任一点&lt;span class="math"&gt;\(x_0\)&lt;/span&gt;到超平面S的距离为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{1}{\left \| w \right \|}|w.x_0+b|$$&lt;/div&gt;
&lt;p&gt;
对于误分类的数据点&lt;span class="math"&gt;\(-y_i(w.x_1+b)&amp;gt;0\)&lt;/span&gt;那么所有误分类点到超平面的距离为：
&lt;/p&gt;
&lt;div class="math"&gt;$$-\frac{1}{\left \| w \right \|}\sum y_i(w.x_1+b)$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;感知机&lt;span class="math"&gt;\(sign(w.x+b)\)&lt;/span&gt;学习的损失函数定义为&lt;/strong&gt;：
&lt;/p&gt;
&lt;div class="math"&gt;$$L(w,b)=-\sum y_i(w.x_1+b)$$&lt;/div&gt;
&lt;p&gt;
一个特定样本的损失函数，在误分类的时候该函数是w和b的线性函数，而正确分类的时候是0，因此损失函数时w和b的连续可导函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;感知机学习策略&lt;/strong&gt;就是在假设空间中选取使感知机的损失函数最小的模型参数w和b，即感知机模型。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;感知机学习算法&lt;/h2&gt;
&lt;p&gt;感知机学习算法转化为求解感知机损失函数的最优化问题，最优化的方法是随机梯度下降法。&lt;/p&gt;
&lt;h3&gt;感知机学习算法的原始形式&lt;/h3&gt;
&lt;p&gt;首先，任意选取一个超平面&lt;span class="math"&gt;\(w_0,b_0\)&lt;/span&gt;，然后用梯度下降法不断地极小化目标函数，极小化的过程不是一次使M中所有误分类点的梯度下降，而是&lt;strong&gt;一次随机选取一个误分类点使其梯度下降&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设误分类点集合M是固定的，那么损失函数L(w,b)的梯度由：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial L(w,b)}{\partial w}=-\sum_{x_i\in M}y_ix_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$\frac{\partial L(w,b)}{\partial b}=-\sum_{x_i\in M}y_i$$&lt;/div&gt;
&lt;p&gt;
给出。
随机选取一个误分类点&lt;span class="math"&gt;\((x_i,y_i)\)&lt;/span&gt;，对w,b进行更新：
&lt;/p&gt;
&lt;div class="math"&gt;$$w \leftarrow  w + \eta y_i x_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b \leftarrow  b + \eta y_i $$&lt;/div&gt;
&lt;p&gt;
其中&lt;span class="math"&gt;\(\eta\)&lt;/span&gt;是步长，又称为学习速率。这样通过迭代可以期待损失函数L(w,b)不断减小，直到0. &lt;/p&gt;
&lt;p&gt;这种学习算法直观上解释：当一个实例类被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分离超平面向该分类点的一侧移动，以减少该误分类点与超平面的距离，直至超平面越过该误分类点使其被正确分类。&lt;/p&gt;
&lt;p&gt;感知机学习算法的原始形式：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;输入：训练数据集T、学习率α
输出：w,b；感知机模型f(x)=sign(w.x + b)
(1)选取初值w0,b0
(2)在训练集中选取数据(xi,yi)
(3)如果yi(w.xi + b) &lt;span class="err"&gt;&amp;lt;&lt;/span&gt;= 0，使用随机梯度下降法更新w和b
(4)转至(2)，直至训练集中没有误分类点（重复的将误分类的点一直更新）
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;当采用不同的初值或者选取不同的误分类点的顺序时，解可能不同。&lt;/p&gt;
&lt;p&gt;这种算法是感知机学习的基本算法，对应于后面的对偶形式，称为&lt;strong&gt;原始形式&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;算法的收敛性&lt;/h3&gt;
&lt;p&gt;可以证明，对于&lt;strong&gt;线性可分数据集感知机学习算法原始形式收敛&lt;/strong&gt;，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分割超平面及感知机模型。
可以证明：
&lt;/p&gt;
&lt;div class="math"&gt;$$k \leq (\frac{R}{\gamma})^2$$&lt;/div&gt;
&lt;p&gt;
其中k是误分类的次数，&lt;span class="math"&gt;\(R=max\left \| x_i \right \|\)&lt;/span&gt;，&lt;span class="math"&gt;\(\gamma &amp;gt; 0\)&lt;/span&gt;。
即误分类的次数k是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。感知机学习算法存在许多解，这些解依赖于初值的选择和迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件（这就是SVM的思想）。&lt;/p&gt;
&lt;h3&gt;感知机学习算法的对偶形式&lt;/h3&gt;
&lt;p&gt;感知机学习算法的原始形式和对偶形式与SVM中的原始形式和对偶形式相对应。&lt;/p&gt;
&lt;p&gt;对偶形式的基本想法是：把w和b表示成实例&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;与标记&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;的线性组合的形式，通过求解其系数而求得w和b。
误分类点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;对w,b进行逐步更新：
&lt;/p&gt;
&lt;div class="math"&gt;$$w \leftarrow  w + \eta y_i x_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b \leftarrow  b + \eta y_i $$&lt;/div&gt;
&lt;p&gt;
假设误分类点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;修改了n次w，b，则w,b关于误分类点&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;的增量分别是&lt;span class="math"&gt;\(\alpha_i y_i x_i\)&lt;/span&gt;和&lt;span class="math"&gt;\(\alpha_i y_i\)&lt;/span&gt;，这里&lt;span class="math"&gt;\(\alpha_i=n_i \eta\)&lt;/span&gt;。假设初始值&lt;span class="math"&gt;\(w_0,b_0\)&lt;/span&gt;均为0，那么最后可以学习到w,b的表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$w=\sum_{i=1}^{N}\alpha_i y_i x_i$$&lt;/div&gt;
&lt;div class="math"&gt;$$b=\sum_{i=1}^{N}\alpha_i y_i$$&lt;/div&gt;
&lt;p&gt;感知机学习算法的对偶形式：&lt;/p&gt;
&lt;p&gt;输入：训练数据集T、学习率η&lt;/p&gt;
&lt;p&gt;输出：α,b；感知机模型&lt;span class="math"&gt;\(f(x)=sign(\sum_{j=1}^{N}\alpha_j y_j x_j + b)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(1)选取初值&lt;span class="math"&gt;\(\alpha_0,b_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(2)在训练集中选取数据(xi,yi)&lt;/p&gt;
&lt;p&gt;(3)如果&lt;span class="math"&gt;\(yi(\sum_{j=1}^{N}\alpha_j y_j x_j + b) \leq 0\)&lt;/span&gt;，更新&lt;span class="math"&gt;\(\alpha_i\)&lt;/span&gt;和b&lt;/p&gt;
&lt;div class="math"&gt;$$\alpha_i = \alpha_i + \eta$$&lt;/div&gt;
&lt;div class="math"&gt;$$b = b + \eta y_i$$&lt;/div&gt;
&lt;p&gt;(4)转至(2)，直至训练集中没有误分类点（重复的将误分类的点一直更新）&lt;/p&gt;
&lt;p&gt;为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵：
&lt;/p&gt;
&lt;div class="math"&gt;$$G=[x_i.x_j]_{N \times N}$$&lt;/div&gt;
&lt;p&gt;
如&lt;span class="math"&gt;\(G_{1,1}\)&lt;/span&gt;为向量&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;与&lt;span class="math"&gt;\(x_i\)&lt;/span&gt;的相乘结果。&lt;/p&gt;
&lt;p&gt;与原始形式一样，感知机学习算法的对偶形式是收敛的，存在多个解。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第二章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry><entry><title>梯度下降法</title><link href="http://www.wengweitao.com/ti-du-xia-jiang-fa.html" rel="alternate"></link><updated>2014-07-26T16:18:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-26:ti-du-xia-jiang-fa.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;梯度下降法（Gradient Descent）&lt;/strong&gt;是一种常见的最优化算法，用于求解函数的最大值或者最小值。&lt;/p&gt;
&lt;h2&gt;梯度的概念&lt;/h2&gt;
&lt;p&gt;一个函数&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;对它的一个变量&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的梯度定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial J(\theta))}{\partial \theta} = \lim_{\delta \theta\rightarrow 0}\frac{J(\theta + \delta \theta)-J(\theta ))}{\delta \theta}$$&lt;/div&gt;
&lt;p&gt;
某一点上的梯度指向标量场增长最快的方向，梯度的长度就是最大的变化率。&lt;/p&gt;
&lt;h2&gt;梯度下降&lt;/h2&gt;
&lt;p&gt;在高数中，我们求解一个函数的最小值时，最常用的方法就是求出它的导数为0的那个点，进而判断这个点是否能够取最小值。但是，在实际很多情况，我们很难求解出使函数的导数为0的方程，这个时候就可以使用梯度下降。我们知道对于一个函数沿着梯度的那个方向是下降是最快的。例如为了选取一个&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;使&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;最小，我们可以先随机选择&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;一个初始值，然后不断的修改&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;以减小&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;，直到&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;的值不再改变。对于梯度下降法，可以表示为：
&lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{j} = \theta_j - \alpha \frac{\partial J(\theta))}{\partial \theta_j} $$&lt;/div&gt;
&lt;p&gt;
即不断地向梯度的那个方向（减小最快的方向）更新&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;，最终使得&lt;span class="math"&gt;\(J(\theta)\)&lt;/span&gt;最小。其中&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;称为学习速率（learning rate），取值太小会导致迭代过慢，取值太大可能错过最值点。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;举一个具体的例子，假如你在一座山的山顶准备下山，往哪一个方向走下山最快呢？下山最快的方向是最陡的那个方向，每一步你都应该朝最陡的那个方向走，直到到达山底，学习速率就表示你每一步迈的步伐有多大。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;为什么从函数的梯度方向下降可以得到函数的最小值&lt;/h2&gt;
&lt;p&gt;梯度下降法，基于这样的观察：如果实值函数F(x)在点a 处可微且有定义，那么函数 F(x)在a点沿着梯度相反的方向&lt;span class="math"&gt;\(-\bigtriangledown F(a)\)&lt;/span&gt;下降最快。&lt;/p&gt;
&lt;p&gt;因而，如果
&lt;/p&gt;
&lt;div class="math"&gt;$$b=a-\alpha\bigtriangledown F(a)$$&lt;/div&gt;
&lt;p&gt;
那么&lt;span class="math"&gt;\(F(b) \leq F(a)\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;考虑如下序列&lt;span class="math"&gt;\(x_0,x_1,x_2,...\)&lt;/span&gt;使得
&lt;/p&gt;
&lt;div class="math"&gt;$$x_{n+1}=x_n-\alpha\bigtriangledown F(x_n)$$&lt;/div&gt;
&lt;p&gt;
因此可以得到：
&lt;span class="math"&gt;\(F(x_0) \geq F(x_1) \geq F(x_2) \geq ...\)&lt;/span&gt;
如果顺利的话序列最终可以收敛到期望的极值。&lt;/p&gt;
&lt;p&gt;&lt;img alt="梯度下降描述" src="http://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Gradient_descent.png/350px-Gradient_descent.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：梯度下降得到的结果可能是局部最优值。如果&lt;span class="math"&gt;\(F(x)\)&lt;/span&gt;是凸函数，则可以保证梯度下降得到的是全局最优值。&lt;/p&gt;
&lt;h2&gt;批梯度下降法&lt;/h2&gt;
&lt;p&gt;批梯度下降法（batch gradient descent）的算法描述如下：&lt;/p&gt;
&lt;p&gt;对每一个j重复以下过程直到收敛 {
    &lt;/p&gt;
&lt;div class="math"&gt;$$\theta_{j} = \theta_j - \alpha \sum_{i=1}^{m}\frac{\partial J(\theta , z))}{\partial \theta_j} $$&lt;/div&gt;
&lt;p&gt;
}&lt;/p&gt;
&lt;p&gt;其中假设训练样本数有m个，每个样本用&lt;span class="math"&gt;\(z_i\)&lt;/span&gt;可以表示为&lt;span class="math"&gt;\((x_i, y_i)\)&lt;/span&gt;。可以看出，使用批梯度下降法每一次更新&lt;span class="math"&gt;\(\theta_j\)&lt;/span&gt;都需要遍历训练样本中的所有样本。&lt;/p&gt;
&lt;h2&gt;随机梯度下降法&lt;/h2&gt;
&lt;p&gt;除了批梯度下降法之外，还有一种算法称为——&lt;strong&gt;随机梯度下降法（stochastic gradient descent，SGD）&lt;/strong&gt;。算法描述如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Loop {
for i=1 to m, {
&lt;div class="math"&gt;$$\theta_{j} = \theta_j - \alpha\frac{\partial J(\theta , z_i))}{\partial \theta_j} $$&lt;/div&gt;
}
}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;每一次更新只使用了一个训练样本，但更新了m次。&lt;/p&gt;
&lt;p&gt;批梯度下降法每更新一步都需要遍历所有的样本数据，如果样本数m很大的话，就会非常的耗时；而对于随机梯度下降，每一遇到一个样本数据，就可以马上更新。通常情况下，使用随机梯度下降法的速度会比批梯度下降法快很多。因此，当样本数很大的时候，我们通常都选择使用随机梯度下降法。&lt;/p&gt;
&lt;h2&gt;关于梯度下降法中学习速率的取值问题&lt;/h2&gt;
&lt;p&gt;前面提到&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;称为学习速率（learning rate），取值太小会导致迭代过慢，取值太大可能错过最值点。所以对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;还是非常重要的。最常见的对&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt;的选择策略为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不变策略：&lt;span class="math"&gt;\(\alpha_k=\alpha_0\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;1/k策略：&lt;span class="math"&gt;\(\alpha_k=\alpha_0\frac{\tau}{\tau + k}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体的取值方法，还是需要根据实际的数据进行实验得出。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;http://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95&lt;/li&gt;
&lt;li&gt;http://www.iro.umontreal.ca/~bengioy/ift6266/H12/html/gradient_en.html&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="基本概念"></category></entry><entry><title>统计学习方法概论</title><link href="http://www.wengweitao.com/tong-ji-xue-xi-fang-fa-gai-lun.html" rel="alternate"></link><updated>2014-07-25T20:12:00+08:00</updated><author><name>wwt</name></author><id>tag:www.wengweitao.com,2014-07-25:tong-ji-xue-xi-fang-fa-gai-lun.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文主要介绍统计学习方法的一些基本概念。首先叙述统计学习的定义、研究对象与方法；然后叙述什么是监督学习；接着提出统计学习方法的三要素：模型、测量和算法；然后又介绍了模型选择的方法，包括：正则化与交叉验证；也介绍了学习方法的泛化能力；接着介绍了监督学习中的两种模型：生成模型和判别模型；最后介绍了监督学习方法的应用：分类问题、标注问题与回归问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;统计学习&lt;/h2&gt;
&lt;h3&gt;统计学习的特点&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;统计学习（statistical learning）&lt;/strong&gt;是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;研究对象：数据&lt;/li&gt;
&lt;li&gt;目的：对数据进行预测和分析&lt;/li&gt;
&lt;li&gt;以方法为中心，统计学习方法构建模型并运用模型进行预测与分析&lt;/li&gt;
&lt;li&gt;是概率论、统计学等多个学科的交叉&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;统计学习的对象&lt;/h3&gt;
&lt;p&gt;统计学习的对象是数据（data），并且假设同类数据具有一定的统计规律性，这是统计学习的前提。只有具有一定的统计规律性才能使用概率论的方法进行描述。&lt;/p&gt;
&lt;h3&gt;统计学习的目的&lt;/h3&gt;
&lt;p&gt;考虑学习什么样的模型和如何学习模型，以使模型能够对数据进行准确的预测与分析，同时也要尽可能考虑学习的效率。&lt;/p&gt;
&lt;h3&gt;统计学习的方法&lt;/h3&gt;
&lt;p&gt;统计学习由：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;监督学习（supervised learning）&lt;/li&gt;
&lt;li&gt;非监督学习（unsupervised learning）&lt;/li&gt;
&lt;li&gt;半监督学习（semi-supervised learning）&lt;/li&gt;
&lt;li&gt;强化学习（reinforcement learning）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;等几种学习方法组成。这里主要讨论监督学习。这种情况下，统计学习方法可以概括如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;从给定的、有限的、用于学习的&lt;strong&gt;训练数据（traning data）&lt;/strong&gt;集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为&lt;strong&gt;假设空间（hypoth space）&lt;/strong&gt;；应用某个&lt;strong&gt;评价准则（evaluation criterion）&lt;/strong&gt;，从假设空间中选取一个&lt;strong&gt;最优的模型&lt;/strong&gt;，使得它对已知训练数据及未知&lt;strong&gt;测试数据（test data）&lt;/strong&gt;在给定的评价准则下有最优的预测；最优模型的选取由算法实现；利用最优模型对新数据进行预测或分析。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这样统计学习方法包括：模型的假设空间、模型选择的准则以及模型学习的算法。&lt;strong&gt;这也就是统计学习方法的三要素：模型、策略、算法。&lt;/strong&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;监督学习&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;监督学习（supervised learning）&lt;/strong&gt;的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。
分类和回归都属于监督学习：必须知道预测什么，即目标变量的分类信息。&lt;/p&gt;
&lt;p&gt;而与此相对的是&lt;strong&gt;无监督学习（unsupervised learning）&lt;/strong&gt;，数据没有类别信息，也不会给定目标值。&lt;/p&gt;
&lt;h3&gt;基本概念&lt;/h3&gt;
&lt;h4&gt;输入空间、特征空间与输出空间&lt;/h4&gt;
&lt;p&gt;将输入与输出所有可能取值的集合分别称为&lt;strong&gt;输入空间（input space）&lt;/strong&gt;与&lt;strong&gt;输出空间（output space）&lt;/strong&gt;。
每个具体的输入是一个&lt;strong&gt;实例（instance）&lt;/strong&gt;，通常由&lt;strong&gt;特征向量（feature vector）&lt;/strong&gt;表示。这时，所有特征向量存在的空间称为&lt;strong&gt;特征空间（feature space）&lt;/strong&gt;。特征空间的每一维对应一个特征。&lt;/p&gt;
&lt;p&gt;输入变量X和输出变量Y有不同的类型，根据不同的类型，对预测任务给予不同的名称：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;回归问题&lt;/strong&gt;：输入变量与输出变量均为连续变量的预测（房间面积预测房价）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分类问题&lt;/strong&gt;：输出变量为有限个离散变量的预测问题（新闻分类）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;标注问题&lt;/strong&gt;：输入变量与输出变量均为变量序列的预测问题（词性标注）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;联合概率分布&lt;/h4&gt;
&lt;p&gt;假定我们知道Y的一些情况，包括它和X一起出现的概率，在数学上称作&lt;em&gt;联合概率分布(Joint Probabilily)&lt;/em&gt;。输入变量X和输出变量Y遵循联合概率分布P(X,Y)。P(X,Y)表示&lt;strong&gt;分布函数&lt;/strong&gt;，或&lt;strong&gt;分布密度函数&lt;/strong&gt;。X和Y具有联合概率分布的假设是监督学习关于数据的基本假设。&lt;/p&gt;
&lt;h4&gt;假设空间&lt;/h4&gt;
&lt;p&gt;由输入到输出的映射是由模型来表示的，学习的目的就是为了找到最好的一个模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是&lt;strong&gt;假设空间（hypoth space）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;监督学习的模型可以是概率模型也可以是&lt;strong&gt;非概率模型&lt;/strong&gt;，由条件概率分布 &lt;span class="math"&gt;\(P(Y|X)\)&lt;/span&gt; 或决策函数（decision function） &lt;span class="math"&gt;\(Y=f(x)\)&lt;/span&gt; 表示。&lt;/p&gt;
&lt;h3&gt;问题的形式化&lt;/h3&gt;
&lt;p&gt;监督学习分为学习和预测两个过程。学习的过程需要训练数据集，而训练数据集往往是人工给出的，所以称为监督学习。&lt;/p&gt;
&lt;p&gt;如果一个模型有很好的预测能力，训练样本输出&lt;span class="math"&gt;\(y_i\)&lt;/span&gt;和模型输出&lt;span class="math"&gt;\(f(x_i)\)&lt;/span&gt;之间的差就应该足够小。&lt;/p&gt;
&lt;h2&gt;统计学习3要素&lt;/h2&gt;
&lt;p&gt;方法 = 模型 + 策略 + 算法&lt;/p&gt;
&lt;h3&gt;模型&lt;/h3&gt;
&lt;p&gt;模型就是所要学习的条件概率分布或决策函数。模型的假设空间包含所有可能的条件概率分布或决策函数。
假设空间通常是由一个&lt;strong&gt;参数向量&lt;span class="math"&gt;\(\theta\)&lt;/span&gt;决定&lt;/strong&gt;的族。&lt;/p&gt;
&lt;h3&gt;策略&lt;/h3&gt;
&lt;p&gt;有了模型的假设空间，接着需要考虑按照什么样的准则学习或者选择最优的模型。
需要引入损失函数与风险函数的概念。&lt;/p&gt;
&lt;h4&gt;损失函数和风险函数&lt;/h4&gt;
&lt;p&gt;用&lt;strong&gt;损失函数（loss function）或代价函数（cost function）&lt;/strong&gt;来度量预测错误的程度，记作&lt;span class="math"&gt;\(L(Y,f(X))\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;常用的损失函数有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;0-1损失函数
&lt;div class="math"&gt;$$L(Y, f(X))=\left\{\begin{matrix}
0, Y \neq f(X)\\ 
1, Y =  f(X)
\end{matrix}\right.$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;平方损失函数（quadratic loss function）
&lt;div class="math"&gt;$$L(Y, f(X))=(Y - f(X))^2$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;绝对损失函数
&lt;div class="math"&gt;$$L(Y, f(X))=|Y - f(X)|$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对数损失函数或者对数似然损失函数
&lt;div class="math"&gt;$$L(Y, P(Y|X))=-log(P(Y|X))$$&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;输出(X,Y)是随机变量，遵循联合概率分布，所以&lt;strong&gt;损失函数的期望&lt;/strong&gt;是：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{exp}(f)=E_p[L(Y, f(X))]=\int L(y,f(x))P(x,y)dxdy$$&lt;/div&gt;
&lt;p&gt;这是模型&lt;span class="math"&gt;\(f(X)\)&lt;/span&gt;在联合概率分布P(X, Y)的平均意义下的损失，称为&lt;strong&gt;风险函数（risk function）或期望损失（expected loss）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;学习的目标是选择期望风险最小的模型，由于联合分布P(X,Y)是未知的（已知那么可以直接同P(X)求出P(Y|X)了），所以风险函数无法直接计算。&lt;/p&gt;
&lt;p&gt;模型f(X)关于一个给定训练数据集的平均损失称为&lt;strong&gt;经验风险（empirical risk）或者经验损失（empirical loss）&lt;/strong&gt;，记作：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{emp}(f)=\frac{1}{N}\sum L(y,f(x))$$&lt;/div&gt;
&lt;p&gt;
根据大数定律，当N趋于无穷时，经验风险趋于期望风险。所以很自然的想到用经验风险估计期望风险。但是实际训练样本数目有限，用经验风险评估期望风险常常并不理想，要对经验风险进行一定的矫正，这就关系到监督学习中的两个基本策略：经验风险最小化和结构化风险最小化。&lt;/p&gt;
&lt;h4&gt;经验风险最小化和结构化风险最小化&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;经验风险最小化（empirical risk minimization， ERM）&lt;/strong&gt;的策略认为，经验风险最小的模型是最优的模型。
当样本容量足够大时，经营风险最小化能保证有很好的学习效果，比如极大似然估计就是一个例子。&lt;/p&gt;
&lt;p&gt;当样本容量很小时，经营风险最小化可能产生&lt;strong&gt;过拟合（over-fitting）&lt;/strong&gt;的现象。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结构风险最小化（structural risk minimization， SRM）&lt;/strong&gt;是为了防止过拟合而提出的策略，&lt;strong&gt;结构风险最小化等价于正则化（regularization）&lt;/strong&gt;。结构风险在经验风险上加上表示模型复杂度的正则化项或罚项。结构风险的定义是：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{srm}(f)=\frac{1}{N}\sum L(y_i,f(x_i))+\lambda J(f)$$&lt;/div&gt;
&lt;p&gt;
J(f)为模型的复杂度，模型f越复杂，复杂度J(f)就越大；反之越小。也就是说复杂度表示了对复杂模型的惩罚。$\lambda \geq 0 $用以权衡经验风险和模型复杂度。&lt;/p&gt;
&lt;p&gt;贝叶斯估计中的最大后验概率估计（maximum posterior probability estimation, MAP）就是结构化风险最小化的例子。&lt;/p&gt;
&lt;p&gt;这样，&lt;strong&gt;监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;算法&lt;/h3&gt;
&lt;p&gt;最后需要考虑用什么样的计算方法求解最优模型。&lt;/p&gt;
&lt;p&gt;如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;模型的假设空间（模型）、模型选择的准则（策略）以及模型学习的算法（算法），这3个确定了，统计学习的方法也就确定了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2&gt;模型评估与模型选择&lt;/h2&gt;
&lt;h3&gt;训练误差与测试误差&lt;/h3&gt;
&lt;p&gt;前者是基于训练数据集的平均损失，后者是根据测试数据集的平均损失。测试误差反映了学习方法对未知的测试数据集的预测能力，越小就代表预测能力越强。通常将学习方法对未知数据的预测能力称为&lt;strong&gt;泛化能力（generalization ability）&lt;/strong&gt;。&lt;/p&gt;
&lt;h3&gt;过拟合与模型选择&lt;/h3&gt;
&lt;p&gt;对于有多个模型，如果一味追求提高对训练数据的预测能力，所选模型的复杂度往往会比真模型更高，这种现象就称为&lt;strong&gt;过拟合（over-fitting）&lt;/strong&gt;。过拟合是指学习时选择的模型包含的参数过多，以致于出现这一模型对已知数据预测很好，但对未知数据预测得很差的现象。&lt;/p&gt;
&lt;p&gt;模型选择时，不仅要考虑对已知数据的预测能力，还要考虑对未知数据的预测能力。当训练数据拟合效果较好，模型也比较简单，是一个较好的选择。&lt;/p&gt;
&lt;p&gt;在多项式函数拟合中可以看到，随着多项式次数（模型复杂度）的增加，训练误差会减小，直至趋向于0，但测试误差却并不如此，会先减小，达到最小值，然后增大。
下面介绍两种常用的&lt;strong&gt;模型选择方法&lt;/strong&gt;：正则化与交叉验证。&lt;/p&gt;
&lt;h2&gt;正则化与交叉验证&lt;/h2&gt;
&lt;h3&gt;正则化&lt;/h3&gt;
&lt;p&gt;模型选择的典型方法是正则化（regularization），正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项。正则化的作用就是选择经验风险与模型复杂度同时较小的模型。&lt;/p&gt;
&lt;p&gt;正则化符合奥卡姆剃刀（Occam's razor）原理：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;若无必要，勿增实体。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在所有可能的选择汇总，能够很好的解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度看，正则化对应于模型的先验概率。复杂的模型有较小的先验概率，简单的模型有较大的先验概率。&lt;/p&gt;
&lt;h3&gt;交叉验证&lt;/h3&gt;
&lt;p&gt;如果样本充足，可以将数据集分为3个部分：训练集、验证集和测试集。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。
但是，在实际中，数据样本是不充足的，为了选择好的模型，可以采用交叉验证（cross validation）的方法。交叉验证的基本思想是重复地使用数据，将给定的数据集进行切分，将切分的数据集组合为训练集与测试集，反复进行训练、测试以及模型选择。&lt;/p&gt;
&lt;p&gt;1.简单交叉验证&lt;/p&gt;
&lt;p&gt;随机地将数据集分为两部分，一部分为训练集，一部分为测试集（如70%和30%的比例切分）。选出测试误差最小的模型。&lt;/p&gt;
&lt;p&gt;2.S折交叉验证&lt;/p&gt;
&lt;p&gt;应用最多的是S折交叉验证（S-fold cross validation）。随机地将数据集切分成S个互不相交的大小相同的子集；然后利用S-1个自己训练模型，利用余下的一个子集测试模型；重复这一过程S次，每次选择不同的一份子集作为测试集；最后选出S次评测中平均测试误差最小的模型。&lt;/p&gt;
&lt;p&gt;3.留一交叉验证&lt;/p&gt;
&lt;p&gt;S折交叉验证当S=N的特例，往往在数据缺乏的情况下使用。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;泛化能力&lt;/h2&gt;
&lt;h3&gt;泛化误差&lt;/h3&gt;
&lt;p&gt;泛化能力是指该方法学习到的模型对未知数据的预测能力，通常是通过测试误差来评价学习方法的泛化能力。
如果学到的模型是f，那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）：
&lt;/p&gt;
&lt;div class="math"&gt;$$ R_{exp}(f)=E_p[L(Y, f(X))]=\int L(y,f(x))P(x,y)dxdy$$&lt;/div&gt;
&lt;p&gt;
事实上，泛化误差就是所学习到的模型的期望风险。&lt;/p&gt;
&lt;h3&gt;泛化误差的上界&lt;/h3&gt;
&lt;p&gt;学习方法的泛化能力往往都是通过研究泛化误差的上界进行的。泛化误差上界通常具有如下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;它是样本容量的函数，当样本容易增加，泛化上界趋向于0；&lt;/li&gt;
&lt;li&gt;它是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于二类分类问题，当假设空间是有限个，对任意一个函数f，至少以概率&lt;span class="math"&gt;\(1-\sigma\)&lt;/span&gt;以下不等式成立：
&lt;/p&gt;
&lt;div class="math"&gt;$$R(f) \leq R’(f) + \epsilon (d, N, \sigma)$$&lt;/div&gt;
&lt;p&gt;
左端是泛化误差，右端即为泛化误差的上界。在泛化误差的上界中，第1项是训练误差，训练误差越小，泛化误差越小。第2项是N的单调递减函数，当N趋于无穷时趋于0；同时也是&lt;span class="math"&gt;\(\sqrt{logd}\)&lt;/span&gt;阶的函数（d是函数个数），假设空间中函数个数越多，其值越大。
可以证明以上不等式，需要用到Hoeffding不等式。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;生成模型与判别模型&lt;/h2&gt;
&lt;p&gt;监督学习方法可以分为&lt;strong&gt;生成方法（generative approach）和判别方法（discriminative approach）&lt;/strong&gt;。所学到的模型分别称为生成模型和判别模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生成方法&lt;/strong&gt;由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：
&lt;/p&gt;
&lt;div class="math"&gt;$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$&lt;/div&gt;
&lt;p&gt;
这样的方法称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。
典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;判别方法&lt;/strong&gt;由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测模型，即判别模型。判别方法关心的是对给定的输入X，应该预测什么样的输出Y。
典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;生成方法的特点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以还原出P(X,Y)，而判别模型不能&lt;/li&gt;
&lt;li&gt;学习收敛速度更快，即样本容量增加，学到的模型能够更快收敛于真实模型&lt;/li&gt;
&lt;li&gt;当存在隐变量时，仍然可以用生成学习方法学习，此时判别方法就不能用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;判别方法的特点：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接学习的是P(Y|X)或决策函数f(X)，直接面对预测，学习的效率更高&lt;/li&gt;
&lt;li&gt;由于是直接学习P(Y|X)或决策函数f(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2&gt;分类问题&lt;/h2&gt;
&lt;p&gt;当&lt;strong&gt;输出变量Y取有限个离散值&lt;/strong&gt;时，预测问题便成为分类问题。&lt;/p&gt;
&lt;p&gt;评价分类器性能的指标一般是分类&lt;strong&gt;准确率（accuracy）&lt;/strong&gt;，其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。&lt;/p&gt;
&lt;p&gt;对于二类分类问题常用的评价指标是&lt;strong&gt;精确率（precision）&lt;/strong&gt;与&lt;strong&gt;召回率（recall）&lt;/strong&gt;。通常以关注的类为正类，其他类为负类。预测的4种情况出现的总数分别记作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP：将正类预测为正类数&lt;/li&gt;
&lt;li&gt;FN：将正类预测为负类数&lt;/li&gt;
&lt;li&gt;FP：将负类预测为正类数&lt;/li&gt;
&lt;li&gt;TN：将负类预测为负类数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;精确率定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$P=\frac{TP}{TP+FP}$$&lt;/div&gt;
&lt;p&gt;
即预测结果为正类中的确是正类的概率。&lt;/p&gt;
&lt;p&gt;召回率定义为：
&lt;/p&gt;
&lt;div class="math"&gt;$$R=\frac{TP}{TP+FN}$$&lt;/div&gt;
&lt;p&gt;
即预测结果中与正类的概率占数据集中总的正类的概率。&lt;/p&gt;
&lt;p&gt;此外，还有&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;值，是精确率和召回率的调和均值，即：
&lt;/p&gt;
&lt;div class="math"&gt;$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$&lt;/div&gt;
&lt;p&gt;
精确率和召回率都高时，&lt;span class="math"&gt;\(F_1\)&lt;/span&gt;值也会高。&lt;/p&gt;
&lt;p&gt;许多统计学习方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络等。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;标注问题&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;标注（taggging）&lt;/strong&gt;也是一个监督学习问题，可以认为是分类问题的一个推广，标注问题是对更复杂的结构预测问题的简单形式。标注问题的输入是一个观察序列，输出是一个标记序列或状态序列。注意，可能的标记个数是有限的，但其组合缩成的标记序列的个数是依序列长度呈指数级增长。&lt;/p&gt;
&lt;p&gt;标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。&lt;/p&gt;
&lt;p&gt;标注问题在信息抽取、自然语言处理等领域被广泛应用，如自然语言处理中的词性标注。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;回归问题&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;回归（regression）&lt;/strong&gt;是监督学习的另一个重要问题。回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系。回归模型是表示从输入变量到输出变量之间的映射函数。回归问题的学习等价于函数拟合。&lt;/p&gt;
&lt;p&gt;回归问题按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间的关系的类型，分为线性回归和非线性回归。&lt;/p&gt;
&lt;p&gt;回归学习最常用的损失函数时平方损失函数，在此情况下，回归问题可以用著名的最小二乘法（least square）求解。&lt;/p&gt;
&lt;p&gt;回归问题可以用于预测股票，如将影响股票价格的各种因素作为自变量（输入特征），而将股价作为因变量（输出的值），将过去的数据作为训练数据，就可以学习一个回归模型，对未来的股价进行预测。&lt;/p&gt;
&lt;hr /&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://book.douban.com/subject/10590856/"&gt;统计学习方法&lt;/a&gt;第一章&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="读书笔记"></category></entry></feed>